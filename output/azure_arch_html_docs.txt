[
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n2 minutes\n\n\nMany organizations have an aging or under-engineered data platform strategy. There's a significant trend of moving existing systems to the cloud, building new AI-ready applications quickly with the cloud, and offloading some on-premises costs. Organizations are seeking to design a plan for how to move their data workloads to the cloud. Administrators want to understand how to set up their organizations for success.\nMeet Tailwind Traders\n\n\nTailwind Traders is a fictitious home improvement retailer. The company operates retail hardware stores across the globe and online. It currently manages an on-premises datacenter that hosts the company's retail website. The datacenter also stores all of the data and streaming video for its applications. The on-premises SQL Server provides storage for customer data, order history, and product catalogs. SQL Server also serves as data storage for the internal-only training portal website. The company wants to effectively manage their database needs by migrating their infrastructure to the cloud.\nAs you work through this lesson, suppose you're the CTO for Tailwind Traders. You're tasked with finding a cost efficient database solution that provides low latency and high availability. In this module, you explore different storage solutions that solve different types of problems. You review storage options offered by Azure SQL Database, Azure SQL Managed Instance, and SQL Server in an Azure virtual machine. You discover how to work with data in Azure SQL Edge and Azure Cosmos DB. You also learn how to design your solution with data protection.\nLearning objectives\nIn this module, you learn how to:\n\nDesign for Azure SQL Database.\n\nDesign for Azure SQL Managed Instance.\n\nDesign for SQL Server on Azure Virtual Machines.\n\nRecommend a solution for database scalability.\n\nRecommend a solution for database availability.\n\nDesign protection for data at rest, data in transmission, and data in use.\n\nDesign for Azure SQL Edge.\n\nDesign for Azure Cosmos DB and Cosmos DB Table API.\n\n\nSkills measured\nThe content in this module helps you prepare for Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions. The module concepts are covered in:\nDesign data storage solutions\n\nDesign a data storage solution for relational data\n\nRecommend a solution for storing relational data.\n\nRecommend a database service tier and compute tier.\n\nRecommend a solution for database scalability.\n\nRecommend a solution for data protection.\n\n\nPrerequisites\n\nConceptual knowledge of SQL Server.\n\nWorking experience with database solutions.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/1-introduction",
      "load_timestamp": "2025-06-01T22:19:22.599120",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure SQL Database - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure SQL Database\n\n\nCompleted\n\n\n9 minutes\n\n\nRelational data is a type of structured data that has a shared schema. The data is stored in database tables with rows, columns, and keys, and used for application storage like e-commerce websites. As the CTO for Tailwind Traders, you're responsible for designing databases for Azure to support existing structured data on-premises. You're looking for a storage solution to support existing data and future relational data workloads initiated by the company.\nWithin the umbrella of the Azure SQL platform, there are many AI-ready deployment options and choices to consider. This video reviews five options that give you the flexibility to get and pay for exactly what you need in your storage solution.\n\nAzure SQL Database\nIn this unit, we look at Azure SQL Database and SQL Database elastic pools. In the following units, we examine SQL Server on Azure Virtual Machines, along with Azure SQL Managed Instance and instance pools.\nAzure SQL Database is a PaaS deployment option of Azure SQL that abstracts both the OS and the SQL Server instance. An Azure SQL database is a fully managed service. You don't have to deal with complex database tasks like configuring and managing high availability, tuning, and backups. The service automatically upgrades each SQL database to run the most recent version of SQL Server. You get the latest SQL Server capabilities without having to perform manual updates.\nThings to know about Azure SQL Database\nReview the following characteristics of the SQL Database deployment option:\n\nIt's a highly scalable, intelligent, relational database service built for the cloud with the industry's highest availability.\n\nSQL Database is the only deployment option that supports scenarios that require large databases (currently up to 100 TB) or autoscaling for unpredictable workloads (serverless).\n\nYou can create a SQL Database elastic database pool, where all databases in the pool share the same set of compute and storage resources. Each database can use the resources it needs, within the limits you set, depending on current load.\n\nThere are two primary pricing options for SQL Database: DTU and vCore. A serverless option is also available for a single database.\n\n\nvCore: A vCore is a virtual core. You choose the number of virtual cores and have greater control over your compute costs. This option supports the Azure Hybrid Benefit for SQL Server and reserved capacity (pay in advance).\n\nDTU: A DTU (Database Transaction Unit) is a combined measure of compute, storage, and I/O resources. The DTU option is an easy, preconfigured purchase option.\n\nServerless: A compute tier for single databases in SQL Database. The serverless model automatically scales compute, based on workload demand, and bills only for the amount of compute used.\n\n\nBusiness scenario\nLet's explore a business scenario for Azure SQL Database. AccuWeather has been analyzing and predicting the weather for more than 55 years. The company chose the Azure platform for its big data, machine learning, and AI capabilities. AccuWeather wants to focus on building new models and applications, not on managing databases. The company chose SQL Database to use with other services, like Azure Data Factory and Azure Machine Learning to quickly and easily deploy new internal applications to make sales and customer predictions.\n\n\nThings to consider when using Azure SQL Database\nConsider how Azure SQL Database can be included in your relational data storage plan for Tailwind Traders:\n\nConsider vCore pricing. (Microsoft recommended) Select compute and storage resources independently for multiple SQL databases or an elastic database pool. Use Azure Hybrid Benefit for SQL Server or reserved capacity (pay in advance) to save money. You control the compute and storage resources that you create and pay for.\n\nConsider DTU pricing. Choose this easy, preconfigured purchase plan for a bundled measure of compute, storage, and I/O resources to support multiple SQL databases. This option isn't available for Azure SQL Managed Instance.\n\nConsider serverless option. Use the serverless compute tier for a single SQL database. You're billed only for the amount of compute used.\n\nConsider elastic database pools. Buy a set of compute and storage resources to share among all SQL databases in an elastic pool. For more information, see SQL elastic pools.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/2-design-for-azure-sql-database",
      "title": "Design for Azure SQL Database - Training | Microsoft Learn",
      "description": "Design for Azure SQL Database",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/2-design-for-azure-sql-database",
      "load_timestamp": "2025-06-01T22:19:22.885352",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure SQL Managed Instance - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure SQL Managed Instance\n\n\nCompleted\n\n\n3 minutes\n\n\nAzure SQL Managed Instance is a PaaS deployment option of Azure SQL. As with Azure SQL Database, Azure SQL Managed Instance is a fully managed service. It provides an instance of SQL Server, but removes much of the overhead of managing a virtual machine. You can deploy a single managed instance or a managed instance pool.\n\n\nThings to know about Azure SQL Managed Instance\nReview the following characteristics of the SQL Managed Instance deployment option.\n\nYou can use SQL Managed Instance to do lift-and-shift migrations to Azure without having to redesign your applications.\n\nAzure SQL Managed Instance is ideal for customers interested in instance-scoped features, such as SQL Server Agent, Common language runtime (CLR), Database Mail, Distributed transactions, and Machine Learning Services.\n\nSQL Managed Instance uses vCores mode. You can define the maximum CPU cores and maximum storage allocated to your managed instance. All databases within the managed instance share the resources allocated to the instance.\n\nMost of the features available in SQL Server are available in SQL Managed Instance. Review this comparison of SQL Database and SQL Managed Instance.\n\n\nBusiness scenario\nLet's explore a business scenario for Azure SQL Managed Instance. Komatsu is a manufacturing company that produces and sells heavy equipment for construction. The company had multiple mainframe applications for different types of data. Komatsu wants to consolidate these applications to get an overall view. Additionally, Komatsu wants a way to reduce overhead. Because the company uses a large surface area of SQL Server features, the IT department wants to move to Azure SQL Managed Instance. They were able to move about 1.5 TB of data smoothly and gained many benefits. With the deployment to Azure SQL Managed Instance, the company gains automatic patching and version updates, automated backups, high availability, and reduced management overhead.\n\n\nThings to consider when using Azure SQL Managed Instance\nConsider how Azure SQL Managed Instance can be included in your relational data storage plan for Tailwind Traders:\n\nConsider instance-scoped features. Use instance-scoped features of Azure SQL Managed Instance like Service Broker, CLR, SQL Server Agent, and Linked servers. Migrate your relational and structured data to Azure without rearchitecting your applications.\n\nConsider instance scalability. Add scalability for your instance by enabling vCores mode. You can define the maximum CPU cores and storage for your instances, so all databases in the instance share the same resources.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/3-design-for-azure-sql-managed-instance",
      "title": "Design for Azure SQL Managed Instance - Training | Microsoft Learn",
      "description": "Design for Azure SQL Managed Instance",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/3-design-for-azure-sql-managed-instance",
      "load_timestamp": "2025-06-01T22:19:23.287347",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for SQL Server on Azure Virtual Machines - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for SQL Server on Azure Virtual Machines\n\n\nCompleted\n\n\n4 minutes\n\n\nSQL Server on Azure Virtual Machines is a version of SQL Server that runs on an Azure virtual machine (VM). This service lets you use full versions of SQL Server in the cloud without having to manage your on-premises machines. Azure VMs come in many sizes and can be run in diverse geographic regions. Each SQL Server VM can be created to meet specific version and operating system requirements, which make them a good option for handling different SQL Server workloads.\nThings to know about SQL Server on Azure Virtual Machines\nReview the following characteristics of the SQL Server on Azure Virtual Machines deployment option:\n\nWhen you run SQL Server on Azure Virtual Machines, you have access to the full capabilities of SQL Server.\n\nAll of your SQL Server skills should directly transfer during the migration, and Azure can help automate backups and security patches.\n\nUnlike the Azure SQL Database and Azure SQL Managed Instance deployment options, you're responsible for version update operations for the OS and SQL Server.\n\n\nBusiness scenario\nLet's explore a business scenario for SQL Server on Azure Virtual Machines. AllScripts is a leading manufacturer of healthcare software. The company serves physician practices, hospitals, health plans, and the pharmaceutical industry. To transform its applications frequently, and host them securely and reliably, AllScripts wants to quickly move its data to Azure. In just three weeks, the company used Azure Site Recovery to migrate dozens of acquired applications running on approximately 1,000 VMs to Azure.\n\n\nThings to consider when using SQL Server on Azure Virtual Machines\nConsider how SQL Server on Azure Virtual Machines can be included in your relational data storage plan for Tailwind Traders:\n\nConsider server access. Access your SQL Server and operating system server by implementing SQL Server on your virtual machines. Expansive support is provided for SQL Server and operating system versions.\n\nConsider automated management. Use the automated management features of SQL Server for your virtual machines.\n\nConsider Azure Hybrid Benefit. Exercise the Azure Hybrid Benefit for existing on-premises Windows Server and SQL Server licenses.\n\n\nCompare Azure SQL deployment options\nYou review the different Azure SQL deployment options. Compare the solution features and recommended usage scenarios, and think about which options support the Tailwind Traders organization.\n\n\nCompare\nSQL Database\nSQL Managed Instance\nSQL Server on Azure Virtual Machines\n\n\nScenarios\nBest for modern cloud applications, hyperscale, or serverless configurations\nBest for most lift-and-shift migrations to the cloud, instance-scoped features\nBest for fast migrations, and applications that require OS-level access\n\n\nFeatures\nSingle database  - Hyperscale storage (for databases up to 100 TB)  - Serverless compute  - Fully managed service  Elastic pool  - Resource sharing between multiple databases for price optimization  - Simplified performance management for multiple databases  - Fully managed service\nSingle instance  - SQL Server surface area (vast majority)  - Native virtual networks  - Fully managed service  Instance pool  - Pre-provision compute resources for migration  - Cost-efficient migration  - Host smaller instances (2vCore)  - Fully managed service\nAzure Virtual Machines  - SQL Server access  - OS-level server access  - Expansive version support for SQL Server  - Expansive OS version support  - File stream, Microsoft Distributed Transaction Coordinator (DTC), and Simple Recovery model  - SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), and SQL Server Analysis Services (SSAS)\n\n\nTip\nLearn more  in the Introduction to the Azure SQL family of products module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/4-design-for-sql-server-azure",
      "title": "Design for SQL Server on Azure Virtual Machines - Training | Microsoft Learn",
      "description": "Design for SQL Server on Azure Virtual Machines",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/4-design-for-sql-server-azure",
      "load_timestamp": "2025-06-01T22:19:23.701581",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Recommend a solution for database scalability - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nRecommend a solution for database scalability\n\n\nCompleted\n\n\n9 minutes\n\n\nAs the CTO for Tailwind Traders, you're interested in a low latency and high availability database solution to store relational data. After reviewing the five deployment options, you decide to implement Azure SQL Database for its fully managed service features. You're hoping to gain the high availability SLA offered in the Business Critical service tier when using availability zones.\nYour next decision is how to support scalability for a relational database. You need a dynamically scalable solution that can handle an expanding number of requests over time without a negative effect on availability or performance.\n\nAzure SQL Database and dynamic scalability\nAzure SQL Database supports dynamic scalability. You can easily change resources allocated to your databases, such as CPU power, memory, I/O throughput, and storage with minimal downtime. Use the Azure portal to scale an Azure SQL Database without changing the existing infrastructure or purchasing new hardware.\nThings to know about dynamic scalability\nReview the following characteristics of dynamic scalability for an Azure SQL database:\n\nChoose DTU or vCore models, and define the maximum amount of resources to assign to each database with a single database implementation.\n\nUse elastic database pools, and purchase resources for the group, and set minimum and maximum resource limits for the databases within the pool.\n\nImplement vertical or horizontal scaling:\n\nVertical: Increase or decrease the compute size of an individual database, also called scaling up. Implement vertical scaling by using SQL Database elastic database pools. When you have low average utilization, but infrequent, high utilization spikes, you can allocate enough capacity in the pool to manage the spikes for the group.\n\nHorizontal: Add or remove databases to adjust capacity or overall performance, also called scaling out. Apply horizontal scaling by using sharding to partition data or read scale-out provisioning.\n\n\nBusiness scenario\nLet's explore a business scenario for using vertical scaling. A small business experiences rapid growth globally. The company needs to maintain and scale separate SQL databases for each location. The rates of growth and database load vary significantly. Resource requirements are unpredictable. The ideal dynamic scaling solution is to use SQL elastic database pools with vertical scaling. You can scale, manage performance, and manage costs for a set of SQL databases. For more information, see SQL elastic database pools.\n\n\nThings to consider when choosing scalability solutions\nReview the following scaling scenarios, and think about which database scaling strategy can work for Tailwind Traders.\n\n\nScenario\nScaling solution\n\n\nManage and scale multiple Azure SQL databases that have varying and unpredictable resource requirements\nElastic database pools and vertical scaling. Use elastic database pools to ensure databases get the performance resources they need when they need it. Elastic pools provide a simple resource allocation mechanism within a predictable budget. There's no per-database charge for elastic pools. You're billed for each hour a pool exists at the highest eDTU or vCores, regardless of usage or whether the pool was active for less than an hour.\n\n\nDifferent sections of a database reside in different geographic locations for compliance reasons\nHorizontal scaling and sharding. Use sharding to split your data into several databases and scale them independently. The shard map manager is a special database that maintains global mapping information about all shards (databases) in a shard set. The metadata allows an application to connect to the correct database based on the value of the sharding key.\n\n\nDependency support for commercial BI or data integration tools, where multiple databases contribute rows into a single overall result for use in Excel, Power BI, or Tableau\nElastic database tools and elastic query. Use the Elastic database tools elastic query feature to access data spread across multiple databases. Elastic query is available on the Standard tier. Querying can be done in T-SQL that spans multiple databases in Azure SQL Database. Run cross-database queries to access remote tables, and to connect tools (Excel, Power BI, Tableau, and so on) and query across data tiers. You can scale out queries to large data tiers and visualize the results in business intelligence reports.\n\n\nTip\nContinue your learning in the Scale multiple Azure SQL Databases with SQL elastic pools training module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/5-recommend-database-scalability",
      "title": "Recommend a solution for database scalability - Training | Microsoft Learn",
      "description": "Recommend a solution for database scalability",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/5-recommend-database-scalability",
      "load_timestamp": "2025-06-01T22:19:23.870792",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Recommend a solution for database availability - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nRecommend a solution for database availability\n\n\nCompleted\n\n\n6 minutes\n\n\nAzure SQL provides varying levels of database availability options and capabilities according to service tiers. The service tier determines the underlying architecture of the database or managed instance that you deploy.\n\nThings to know about General Purpose availability\nSQL databases and managed instances in the General Purpose (or Standard) service tier have the same availability architecture.\n\n\nThe image illustrates the availability architecture for the vCore General Purpose (or DTU Standard) tier:\n\nThe application connects to the server name, which connects to a gateway GW that points the application to the server to connect to. The application is running on a VM.\n\nThe General Purpose tier uses remote storage. The primary replica uses locally attached SSD for the temporary database, tempdb.\n\nThe data and log files are stored in Azure Premium Storage, which is locally redundant storage. Multiple copies are stored in one zone of a region.\n\nThe backup files are stored in Azure Standard Storage, which is RA-GRS by default. It's globally redundant storage with copies in multiple regions.\n\n\nAll of Azure SQL is built on Azure Service Fabric, which serves as the Azure backbone. If Azure Service Fabric determines that a failover needs to occur, the failover is similar to that of a failover cluster instance (FCI). The service fabric locates a node with spare capacity and spins up a new SQL Server instance. The database files are attached, recovery is run, and gateways are updated to point applications to the new node. No virtual network or listener or updates are required. These features are built in.\nThings to know about Business Critical availability\nIn the Business Critical (or Premium) tier, you can generally achieve the highest performance and availability of all Azure SQL service tiers. This tier is meant for mission-critical applications that need low latency and minimal downtime.\n\n\nThe image illustrates the availability architecture for the vCore Business Critical (or DTU Premium) tier:\n\nDatabase availability in the Business Critical tier is like deploying an Always On availability group behind the scenes.\n\nUnlike the General Purpose tier, the data and log files all run on direct-attached SSD, which significantly reduces network latency.\n\nIn this tier, there are three secondary replicas. One secondary replica can be used as a read-only endpoint (at no extra charge). A transaction can complete a commit when at least one secondary replica completes the change.\n\n\nThings to know about Hyperscale availability\nThe Hyperscale service tier is available only in Azure SQL Database. This service tier has a unique architecture because it uses a tiered layer of caches and page servers to expand the ability to quickly access database pages without having to access the data file directly.\n\n\nThe image illustrates the availability architecture for the vCore Hyperscale tier:\n\nThe Hyperscale tier architecture uses paired page servers. You can scale horizontally to put all the data in caching layers.\n\nThe Hyperscale architecture supports databases as large as 100 TB.\n\nThis tier uses snapshots, which allow for nearly instantaneous database backups, regardless of database size.\n\nDatabase restores take minutes rather than hours or days.\n\nYou can scale up or down in constant time to accommodate your workloads.\n\n\nThings to consider when choosing database availability\nThe following table compares support for database availability across the vCore service tiers. For the DTU model, you can equate the Basic and Standard tiers to the vCore General Purpose tier, and the Premium tier to the vCore Business Critical tier. The DTU model doesn't offer a Hyperscale tier. As you compare the support options, think about which service tier meets the database availability requirements for Tailwind Traders.\n\n\nSQL Database/SQL Managed Instance vCore tiers\nSQL Database DTU tiers\nDatabase availability support\n\n\nGeneral Purpose\nStandard or Basic\nProvides balanced compute and storage options for business workloads\n\n\nBusiness Critical\nPremium\nMeets low latency requirements and enables highest resilience to failures for business applications\n\n\nHyperscale\nNo applicable tier\nOffers highly scalable storage and meets read-scale requirements for business workloads",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/6-recommend-database-availability",
      "title": "Recommend a solution for database availability - Training | Microsoft Learn",
      "description": "Recommend a solution for database availability",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/6-recommend-database-availability",
      "load_timestamp": "2025-06-01T22:19:24.014751",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design security for data at rest, data in motion, and data in use - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign security for data at rest, data in motion, and data in use\n\n\nCompleted\n\n\n10 minutes\n\n\nMany organizations use Azure SQL Database for large customer databases that store phone numbers, addresses, orders, and credit card information.\nThey need a security solution to prevent unauthorized data access to their cloud hosted databases. Classifying stored data by sensitivity and business scenario helps organizations determine the risks associated with their data.\n\nThere are three basic tenets of good information security: data discovery, classification, and protection. In this unit, we review different data states and encryption methods to apply these tenets in a strong security solution.\nData encryption for structured data\nData exists in three basic states: data at rest, data in motion, and data in process.\n\nData at rest is data on a storage device that isn't being moved or used. Data at rest includes archived email messages stored in your Outlook inbox, or files on your laptop that you aren't using. Data encryption at rest is a mandatory step toward data privacy, compliance, and data sovereignty.\n\nData in motion (also called data in transit) is data that's being moved from one device to another within a private network or public network like the internet. Data in motion can also be data that's being read (used) but not changed. Data in motion includes email messages in transit, browsing internet websites, or using company applications like an organization chart.\n\nData in process is data that's open and being changed. Data in process includes writing an email message, saving your work files, or ordering from a website.\n\n\nThere are different encryption methods for each of data state. The following table summarizes the methods.\n\n\nData state\nEncryption method\nEncryption level\n\n\nData at rest\nTransparent data encryption (TDE)\nAlways encrypted.\n\n\nData in motion\nSecure Socket Layers and Transport Layer Security (SSL/TLS)\nAlways encrypted.\n\n\nData in process\nDynamic data masking\nSpecific data is unencrypted. Remaining data is encrypted.\n\n\nThings to know about data encryption\n\nTransparent data encryption (TDE) protects Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics against the threat of malicious offline activity by encrypting data at rest. TDE performs real-time encryption and decryption of the database, associated backups, and transaction log files at rest without requiring changes to the application. TDE is enabled by default to all newly deployed Azure SQL Databases.\n\nAzure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics enforce Secure Socket Layers and Transport Layer Security (SSL/TLS) encryption always for all connections. This level of encryption ensures all data is encrypted \"in transit\" between the client and server. Transport Layer Security (TLS) is used by all drivers that Microsoft supplies or supports for connecting to databases in Azure SQL Database or Azure SQL Managed Instance.\n\nData-in-use employs a policy-based security feature called dynamic data masking. This feature hides the sensitive data in the result set of a query over designated database fields, while the data in the database remains unchanged. Dynamic data masking helps prevent unauthorized access to sensitive data by enabling customers to designate how much of the sensitive data to reveal with minimal consequence on the application layer.\n\n\nTip\nExpand your learning with the Plan and implement security for Azure SQL Database and Azure SQL Managed Instance learning module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/7-design-security-for-data-at-rest-data-transmission-data-use",
      "title": "Design security for data at rest, data in motion, and data in use - Training | Microsoft Learn",
      "description": "Design security for data at rest, data in motion, and data in use",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/7-design-security-for-data-at-rest-data-transmission-data-use",
      "load_timestamp": "2025-06-01T22:19:24.176131",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure SQL Edge - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure SQL Edge\n\n\nCompleted\n\n\n6 minutes\n\n\nAzure SQL Edge is an optimized relational database engine geared for IoT and IoT Edge deployments. Azure SQL Edge is built on the same engine as SQL Server and Azure SQL. Developers with SQL Server skills can reuse their code to build edge-specific solutions on Azure SQL Edge. Azure SQL Edge provides capabilities to stream, process, and analyze relational and nonrelational data.\nThings to know about Azure SQL Edge\nLet's review the characteristics of Azure SQL Edge that make it useful to include in a relational data storage solution.\n\nAzure SQL Edge is a containerized Linux application. The startup-memory footprint is less than 500 MB.\n\nYou can design and build apps that run on many IoT devices. Capture continuous data streams in real time, or integrate data in a comprehensive organizational data solution. The following diagram shows how SQL Edge captures and stores streaming data.\n\n\nAccess a built-in streaming engine to help derive insights from data streams.\n\nPerform transformation, Windowed aggregation, Simple anomaly detection, and classification of incoming data streams.\n\nUse time-series storage for time-indexed data, which can be aggregated and stored in the cloud for future analysis.\n\n\nAzure SQL Edge interacts with components at the network edge including edge gateways, IoT devices, and edge servers.\n\n\nAzure SQL Edge is available in two editions that have identical feature sets. The editions offer different usage rights and the amount of memory and cores accessible on the host system.\n\n\nAzure SQL Edge Developer\nAzure SQL Edge\n\n\nEach Azure SQL Edge Developer container is limited to up to four cores and 32-GB memory.\nEach Azure SQL Edge container is limited to up to eight cores and 64-GB memory.\n\n\nDevelopment only\nProduction\n\n\nDeployment security\nSecurity is a primary concern when deploying IoT apps to the edge. Because Azure SQL Edge is based on SQL Server technology, one of the most secure database platforms available, it has the same security features of SQL Server Enterprise. The same security policies and practices are extended from cloud to the edge.\nEnsuring protection for Azure SQL Edge deployments involves four steps:\n\nPlatform and system security. This security step includes the physical docker host, the operating system on the host, and the networking systems that connect the physical device to applications and clients.\n\nAuthentication and authorization. SQL authentication refers to the authentication of a user when connecting to Azure SQL Edge by using their username and password. Authorization refers to the permissions assigned to a user within a database in Azure SQL Edge.\n\nDatabase object security. Database objects or securables are the server, database, and other objects in the database. Encryption enhances security. Data protection with Transparent Data Encryption (TDE) enables compliance with many security regulations. \"Always Encrypted\" provides separation between users who own the data and administrators who manage the data.\n\nApplication security. Azure SQL Edge security best practices include writing secure client applications.\n\n\nDeployment options\nAzure SQL Edge as two deployment options:\n\nConnected deployment: For connected deployment, Azure SQL Edge is available on the Microsoft Azure Marketplace and can be deployed as a module for Azure IoT Edge.\n\nDisconnected deployment: Disconnected deployment is accomplished through Azure SQL Edge container images. The images can be pulled from docker hub and deployed either as a standalone docker container or on a Kubernetes cluster.\n\n\nTip\nIf you're interested in learning more, watch the What is Azure SQL Edge video from Channel 9.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/8-design-for-azure-sql-edge",
      "title": "Design for Azure SQL Edge - Training | Microsoft Learn",
      "description": "Design for Azure SQL Edge",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/8-design-for-azure-sql-edge",
      "load_timestamp": "2025-06-01T22:19:24.351470",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Cosmos DB and Table Storage - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Cosmos DB and Table Storage\n\n\nCompleted\n\n\n7 minutes\n\n\nThe final option we consider is relational table data storage using Azure Cosmos DB.\nAzure Cosmos DB is a fully managed NoSQL database service for modern app development. As a fully managed service, Azure Cosmos DB takes database administration off your hands with automatic management, updates, and patching. It also handles capacity management with cost-effective serverless and automatic scaling options that respond to application needs to match capacity with demand.\n\nThings to know about Azure Cosmos DB\nReview the following characteristics of Azure Cosmos DB and how it can help with relational data storage.\n\nAzure Cosmos DB has single-digit millisecond response times and guaranteed speed at any scale.\n\nAzure Cosmos DB offers multiple database APIs, which include NoSQL, MongoDB, PostgreSQL, Cassandra, Gremlin, and Table. These APIs allow your applications to treat Azure Cosmos DB as if it were various other databases technologies, without the overhead of management, and scaling approaches.\n\n\nApplications that are written for Azure Table Storage can migrate to the Azure Cosmos DB Table API with few code changes.\n\nAzure Cosmos DB Table API and Table Storage share the same table data model and expose the same create, delete, update, and query operations through their SDKs.\n\n\nThings to consider when choosing the Azure Cosmos DB Table API\nIf you currently use Azure Table Storage, you gain many benefits by moving to the Azure Cosmos DB Table API. As you review these benefits, consider how Azure Cosmos DB can be included in your relational data storage plan for Tailwind Traders:\n\n\nFeature\nAzure Table Storage\nAzure Cosmos DB Table API\n\n\nLatency\nFast, but no upper bounds on latency.\nSingle-digit millisecond latency for reads and writes, backed with < 10-ms latency reads and < 15-ms latency writes at the 99th percentile, at any scale, anywhere in the world.\n\n\nThroughput\nVariable throughput model. Tables have a scalability limit of 20,000 operations.\nHighly scalable with dedicated reserved throughput per table. Accounts have no upper limit on throughput and support > 10 million operations/s per table (in provisioned throughput mode).\n\n\nGlobal distribution\nSingle region with one optional readable secondary read region for high availability.\nTurnkey global distribution from one to 30+ regions.\n\n\nIndexing\nOnly primary index on PartitionKey and RowKey. No secondary indexes.\nAutomatic and complete indexing on all properties, no index management.\n\n\nQuery\nQuery execution uses index for primary key, and scans otherwise.\nQueries can take advantage of automatic indexing on properties for fast query times.\n\n\nConsistency\nStrong within primary region.\nFive well-defined consistency levels to trade off availability, latency, throughput, and consistency.\n\n\nPricing\nConsumption-based pricing model.\nAvailable in both consumption-based and provisioned capacity pricing models.\n\n\nSLAs\n99.99% availability\n99.99% availability SLA for all single region accounts and all multi-region accounts with relaxed consistency, and 99.999% read availability on all multi-region database accounts.\n\n\nTip\nThere's a lot more to learn in the Introduction to Azure Cosmos DB for NoSQL training module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/9-design-for-azure-cosmos",
      "title": "Design for Azure Cosmos DB and Table Storage - Training | Microsoft Learn",
      "description": "Design for Azure Cosmos DB and Table Storage",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/9-design-for-azure-cosmos",
      "load_timestamp": "2025-06-01T22:19:24.580544",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n3 minutes\n\n\nTailwind Traders has several workloads that are being migrated to Azure. You're reviewing all application database requirements to recommend the appropriate solution. Your design has to accommodate the following scenarios:\n\nInventory application. You plan to migrate the inventory application with a database to the cloud, but you use a non-Microsoft application that requires Windows authentication. You also need full control and customization, including OS access.\n\nHR application. You need to migrate the HR application and database to the cloud, and remove some of the management associated with SQL Server. However, your application uses intra-scoped features, CLR and Service Broker capabilities, in SQL Server.\n\nDatabase migration. There are a few large on-premises SQL server databases that need to be migrated to the cloud. These databases contain anywhere from 40 TB to 80 TB of data.\n\n\nAnswer the following questions\nChoose the best response for each question.\n\n\n1.\nWhich Azure SQL deployment option should be the easiest method to migrate the Inventory application?\n\n\nAzure SQL Managed Instance\n\n\nAzure SQL Database, single database\n\n\nSQL Server on Azure Virtual Machines\n\n\n2.\nWhat's the optimal Azure SQL deployment option to migrate the HR application?\n\n\nAzure SQL Managed Instance\n\n\nAzure SQL Database\n\n\nSQL Server in an Azure virtual machine\n\n\n3.\nWhich database service tier best supports the large on-premises database migration?\n\n\nGeneral Purpose\n\n\nHyperscale\n\n\nBusiness Critical\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/10-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/10-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:24.740574",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary and resources - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary and resources\n\n\nCompleted\n\n\n2 minutes\n\n\nIn this module, you learned how to design a cost efficient database solution that provides low latency, high availability, and scalability. You examined options for moving relational data workloads to the cloud. You explored scenarios for using Azure SQL Database, Azure SQL Managed Instance, and SQL Server on Azure Virtual Machines. You reviewed how to work with data in Azure SQL Edge, Azure Cosmos DB, and Azure Table Storage. You discovered how to protect data at rest, data in transmission, and data in use by using different security and encryption methods.\nLearn more with Copilot\nCopilot can assist you in designing Azure infrastructure solutions. Copilot can compare, recommend, explain, and research products and services where you need more information. Open a Microsoft Edge browser and choose Copilot (top right) or navigate to copilot.microsoft.com. Take a few minutes to try these prompts and extend your learning with Copilot.\n\nWhat are the different Azure products for storing relational data? Compare and contrast these products so I know which one to select for my project.\n\nProvide scalability options for Azure SQL. Provide examples for each option.\n\nCompare Azure SQL to Cosmos DB. Include examples and pros and cons of using the products.\n\nProvide scalability options for Cosmos DB. Are there any limits on scalability?\n\n\nLearn more with Azure documentation\n\nExplore SQL Server on Azure Virtual Machines.\n\nExplore Azure SQL Managed Instance.\n\nExplore Azure SQL Database.\n\nRead about Azure Cosmos DB.\n\nReview FAQ about the Table API in Azure Cosmos DB.\n\nDiscover Azure SQL Database serverless.\n\nBuild scalable cloud databases.\n\nScale out with Azure SQL Database.\n\nUse elastic pools to manage and scale multiple databases in Azure SQL Database.\n\n\nLearn more with self-paced training\n\nSecure data at rest on Azure.\n\nWork with Azure Cosmos DB.\n\nExplore Azure database and analytics services.\n\nExplore relational database services in Azure.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/11-summary-resources",
      "title": "Summary and resources - Training | Microsoft Learn",
      "description": "Summary and resources",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/11-summary-resources",
      "load_timestamp": "2025-06-01T22:19:24.936341",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n2 minutes\n\n\nData storage describes how different data is stored and managed in your organization. The type of data storage that you implement is based on the structure of your data and how your data is accessed.\nData can be highly organized like machine configurations and customer invoices. Other data is less structured, such as fax images and engineering white papers. Some data is used only by specific users like system administrators or file owners. And other data is used by all users, including internal employees and external partners.\nMeet Tailwind Traders\n\n\nTailwind Traders is a fictitious home improvement retailer. The company operates retail hardware stores across the globe and online. As you work through this lesson, suppose you're the CTO for Tailwind Traders. You're investigating storage solutions for unstructured data. You're interested in how storage accounts can be used with blob, file, and disk storage for unstructured data. You want to implement strong protection for your data storage. The company is seeking a data storage solution that balances features with strong performance and reasonable cost.\nLearning objectives\nIn this module, you learn how to:\n\nDesign for data storage.\n\nDesign for Azure storage accounts.\n\nDesign for Azure Blob Storage.\n\nDesign for data redundancy.\n\nDesign for Azure Files.\n\nDesign for Azure managed disks.\n\nDesign for storage security.\n\n\nSkills measured\nThe content in this module helps you prepare for Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions. The module concepts are covered in:\nDesign data storage solutions\n\nDesign data storage solutions for semi-structured and unstructured data\n\nRecommend a solution for storing semi-structured data.\n\nRecommend a solution for storing unstructured data.\n\nRecommend a data storage solution to balance features, performance, and costs.\n\nRecommend a data solution for protection and durability.\n\n\nPrerequisites\n\nConceptual knowledge of storage accounts, blobs, files, disks, and data protection.\n\nWorking experience with creating and securing storage systems.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/1-introduction",
      "load_timestamp": "2025-06-01T22:19:25.094929",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for data storage - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for data storage\n\n\nCompleted\n\n\n4 minutes\n\n\nThe first step in your design for Azure storage is to determine what types of data are required to support the Tailwind Traders organization. In general, data can be classified in three ways: structured, semi-structured, and unstructured. Most organizations need to provide storage options for all data types.\nThings to know about types of data\nThe following table describes three data types. Consider how these different types are used in your organization.\n\n\nStructured\nSemi-structured\nUnstructured\n\n\nStructured data is stored in a relational format that has a shared schema. Structured data is often contained in a database table with rows, columns, and keys.\nSemi-structured data is less organized. The data fields don't fit neatly into tables, rows, and columns. Semi-structured data contains tags that clarify how the data is organized. The data is defined by using a serialization language.\nUnstructured data is the least organized. This data is a mix of information without a clear relationship. The format of unstructured data is referred to as nonrelational.\n\n\n- Relational databases, such as medical records, phone books, and financial accounts  - Application data for an e-commerce website\n- Hypertext Markup Language (HTML) files  - JavaScript Object Notation (JSON) files  - Extensible Markup Language (XML) files\n- Media files like photos, videos, and audio  - Office files, such as Word documents and PowerPoint slides  - Text files like PDF, TXT, and RTF\n\n\nNote\nIn this module, we review storage options for unstructured nonrelational data. You can discover how to work with structured and relational data in the module, Design a data storage solution for relational data.\n\nThings to consider when choosing data storage\nNonrelational data in Azure can be stored in several different data objects. We look at scenarios that implement four storage objects. As you review these options for Tailwind Traders, think about what types of nonrelational data are of most interest to your organization. Consider the storage objects that you might need to implement.\n\n\nConsider Azure Blob Storage. Store vast amounts of unstructured data by using Azure Blob Storage. Blob stands for Binary Large Object. Blob Storage is often used for images and multimedia files.\n\nConsider Azure Files. Provide fully managed file shares in the cloud with Azure Files. This storage data is accessible via the industry standard Server Message Block (SMB) protocol, Network File System (NFS) protocol, and the Azure Files REST API.\n\nConsider Azure managed disks. Support Azure virtual machines by using Azure managed disks. These disks are block-level storage volumes managed by Azure. Managed disks perform like physical disks in an on-premises server, but in a virtual environment.\n\nConsider Azure Queue Storage. Use Azure Queue Storage to store large numbers of messages. Queue Storage is commonly used to create a backlog of work to process asynchronously.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/2-design-for-data-storage",
      "title": "Design for data storage - Training | Microsoft Learn",
      "description": "Design for data storage",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/2-design-for-data-storage",
      "load_timestamp": "2025-06-01T22:19:25.371299",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure storage accounts - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure storage accounts\n\n\nCompleted\n\n\n6 minutes\n\n\nAfter you determine the data storage requirements for your organization, you need to create storage accounts for Tailwind Traders.\nAn Azure storage account groups together all of your Azure Storage services. The storage account provides a unique namespace that's accessible from anywhere (assuming you have the correct permissions) in the world over HTTPS. Data in your storage account is durable and highly available, secure, and massively scalable.\nA storage account represents a collection of settings like location, replication strategy, and subscription owner. Organizations often have multiple storage accounts so they can implement different sets of requirements. The following illustration shows two storage accounts in the Tailwind Traders organization that differ only in the data location (region). But, this one difference is enough to require separate storage accounts in the infrastructure.\n\n\nThings to know about storage account types\nAzure Storage offers several storage account options. Each storage account supports different features and has its own pricing model. Review the following options and think about what storage accounts are required to support Tailwind Traders applications.\n\n\nStorage account\nSupported services\nRecommended usage\n\n\nStandard general-purpose v2\nBlob Storage (including Data Lake Storage), Queue Storage, Table Storage, and Azure Files\nStandard storage account for most scenarios, including blobs, file shares, queues, tables, and disks (page blobs).\n\n\nPremium block blobs\nBlob Storage (including Data Lake Storage)\nPremium storage account for block blobs and append blobs. Recommended for applications with high transaction rates. Use Premium block blobs if you work with smaller objects or require consistently low storage latency. This storage is designed to scale with your applications.\n\n\nPremium file shares\nAzure Files\nPremium storage account for file shares only. Recommended for enterprise or high-performance scale applications. Use Premium file shares if you require support for both Server Message Block (SMB) and NFS file shares.\n\n\nPremium page blobs\nPage blobs only\nPremium high-performance storage account for page blobs only. Page blobs are ideal for storing index-based and sparse data structures, such as operating systems, data disks for virtual machines, and databases.\n\n\nThings to consider when choosing storage accounts\nYou review Azure storage account options and some scenarios for when to use different types of storage accounts. Take a few minutes to think about the storage accounts in the Tailwind Traders organization. If you're already using storage accounts, explore how well the configuration meets the business scenarios.\n\nConsider your storage locations. Locate data storage close to where it's most frequently used. Does Tailwind Traders have data that's specific to a location? You might need a storage account to best support each location.\n\nConsider compliance requirements. Examine regulatory guidelines for Tailwind Traders business scenarios. Are there guidelines for keeping data in a specific location? Does your company have internal requirements for auditing or storing data? You might require different storage accounts to meet the different requirements.\n\nConsider data storage costs. Factor in data storage costs into your plan for Tailwind Traders. A storage account by itself has no financial cost. But, the settings you choose for the account do influence the cost of services in the account. Geo-redundant storage costs more than locally redundant storage. Premium performance and the hot access tier increase the cost of blobs. Do you need to keep track of expenses or billing by department or project? Are you working with partners where storage costs need to be separated? By creating multiple storage accounts, you can better control the overall costs.\n\nConsider replication scenarios. Configure data storage to support different replication strategies. You could partition your data into critical and noncritical categories. You could place Tailwind Traders critical data into a storage account with geo-redundant storage. You could put Tailwind Traders noncritical data in a different storage account with locally redundant storage.\n\nConsider administrative overhead. Plan for administrative overhead in your Tailwind Traders storage design. Each storage account requires some time and attention from an administrator to create and maintain. Using multiple storage accounts increases the complexity for users who add data to your cloud storage. Users in this role need to understand the purpose of each storage account to ensure they add new data to the correct account.\n\nConsider data sensitivity. Protect sensitive and proprietary Tailwind Traders data in your data storage. You can enable virtual networks for proprietary data and not for public data. This scenario might require separate storage accounts.\n\nConsider data isolation. Segregate regulatory and compliance data, or local policies by using multiple storage accounts for Tailwind Traders. You can separate data in one application from data in another application to ensure data isolation.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/3-design-for-azure-storage-accounts",
      "title": "Design for Azure storage accounts - Training | Microsoft Learn",
      "description": "Design for Azure storage accounts",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/3-design-for-azure-storage-accounts",
      "load_timestamp": "2025-06-01T22:19:25.558191",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for data redundancy - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for data redundancy\n\n\nCompleted\n\n\n8 minutes\n\n\nAzure Storage always stores multiple copies of your Tailwind Traders data. This redundancy ensures the data is protected from planned and unplanned events. These events can include transient hardware failures, network or power outages, and massive natural disasters. Storage redundancy ensures your storage account meets its availability and durability targets. This video reviews the data redundancy options.\n\nThings to know about data redundancy\nReview the following characteristics of data redundancy in Azure Storage.\n\nRedundancy is achieved by replicating data to a primary region.\n\nWhen you create a storage account, you select the primary region for the account.\n\nThe primary region supports two replication options: locally redundant storage (LRS) and zone-redundant storage (ZRS).\n\nReplication can also be done for a secondary region. A secondary region is recommended for applications that require high durability.\n\nThe paired secondary region is determined based on the primary region and can't be changed.\n\nThe secondary region is usually in a geographic location that's distant to the primary region. The distance helps to protect against regional disasters.\n\nThe secondary region supports two replication options: geo-redundant storage (GRS) and geo-zone-redundant storage (GZRS).\n\n\nRedundancy in the primary region\nAzure Storage offers two options for how your data is replicated in the primary region: locally redundant storage and zone-redundant storage.\n\n\nLocally redundant storage is the lowest-cost redundancy option, and offers the least durability. LRS protects your data against server rack and drive failures. However, if the data center fails, all replicas of a storage account that use LRS might be lost or unrecoverable.\n\nZone-redundant storage replicates synchronously across three Azure availability zones in the primary region. With ZRS, your data is still accessible for both read and write operations even if a zone becomes unavailable.\n\n\nRedundancy in a secondary region\nFor applications that require high durability, you can choose to copy the data in your storage account to a secondary region. Azure Storage offers two options for copying your data to a secondary region: geo-redundant storage and geo-zone-redundant storage.\n\n\nThe primary difference between GRS and GZRS is how data is replicated in the primary region. Within the secondary region, data is always replicated synchronously with LRS.\n\nIf the primary region becomes unavailable, you can choose to fail over to the secondary region. After the failover completes, the secondary region becomes the primary region, and you can again read and write data.\n\nData is replicated to the secondary region asynchronously. A failure that affects the primary region might result in data loss if the primary region can't be recovered.\n\nWith GRS or GZRS, the data in the secondary region isn't available for read or write access unless there's a failover to the secondary region. For read access to the secondary region, configure your storage account to use read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).\n\n\nThings to consider when using data redundancy\nYou review the different options for implementing replication. Data redundancy is accomplished through a primary region and paired secondary region. As you plan the storage accounts and redundancy settings for Tailwind Traders, consider the following factor.\n\nConsider primary replication options. Explore different scenarios for how Tailwind Traders data can be replicated in the primary region. The redundancy options present tradeoffs between lower costs and higher availability. Some business centers can require more data redundancy. Specific departments or regions might work with data that's not sensitive or which doesn't require high durability. You can implement multiple storage accounts with different redundancy to control the overall costs across the organization.\n\nConsider locally redundant storage. Implement LRS for a low cost redundancy solution, but with limited durability. LRS is suited for Tailwind Traders apps that store data that can be easily reconstructed if data loss occurs. LRS is also a good choice for apps that are restricted to replicating data only within a location due to data governance requirements.\n\nConsider zone-redundant storage. Choose ZRS for excellent performance, low latency, and resiliency for your data if it becomes temporarily unavailable. Keep in mind that ZRS by itself might not protect your data against a regional disaster where multiple zones are permanently affected.\n\nConsider secondary regions. For applications requiring high durability, you can choose to additionally copy the data in your storage account to a secondary region that is hundreds of miles away from the primary region. If your storage account is copied to a secondary region, then your data is durable even if a complete regional outage or a disaster in which the primary region isn't recoverable.\n\nConsider read access requirements. Identify Tailwind Traders applications that require read access to the replicated data in the secondary region, if the primary region becomes unavailable for any reason. Configure your storage account with read access to the secondary region. Your applications can seamlessly shift to reading data from the secondary region if the primary region becomes unavailable.\n\n\nTip\nContinue your learning with the Describe Azure storage services training module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/4-design-for-data-redundancy",
      "title": "Design for data redundancy - Training | Microsoft Learn",
      "description": "Design for data redundancy",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/4-design-for-data-redundancy",
      "load_timestamp": "2025-06-01T22:19:26.402282",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Blob Storage - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Blob Storage\n\n\nCompleted\n\n\n9 minutes\n\n\nAzure Blob Storage is a cloud-based object storage service offered by Microsoft Azure. Blob storage is designed to store large amounts of unstructured data, such as text, images, documents, videos, and audio files. This video from the Developer course covers the basics of blob storage.\n\nThere are two main points to consider in an implementation plan for Azure Blob Storage. First, you need to identify which Azure blob access tier satisfies your organization's storage availability, latency, and cost requirements. The second consideration is to decide if you need access to immutable storage.\nThere are four access options: Hot, Cool, Cold, and Archive access tiers. All four options support availability and latency, but they have differing costs depending on the level of support. All options also support immutable storage, but the storage is implemented differently for the Hot, Cool, and Archive access tiers.\nThe four access options for Azure Blob Storage offer a range of features and support levels to help you optimize your storage costs.\n\n\nComparison\nHot access tier\nCool access tier\nCold access tier\nArchive access tier\n\n\nAvailability\n99.9%\n99%\n99%\n99%\n\n\nAvailability (RA-GRS reads)\n99.99%\n99.9%\n99.9%\n99.9%\n\n\nLatency (time to first byte)\nmilliseconds\nmilliseconds\nmilliseconds\nhours\n\n\nMinimum storage duration\nN/A\n30 days\n90 days\n180 days\n\n\nThings to know about Azure blob access tiers\nThink about your data sets, and which access options can satisfy the requirements for Tailwind Traders.\n\nHot tier. The Hot tier is optimized for frequent reads and writes of objects in the Azure storage account. A good usage case is data that is actively being processed. By default, new storage accounts are created in the Hot tier. This tier has the lowest access costs, but higher storage costs than the Cool and Archive tiers.\n\nCool tier. The Cool tier is optimized for storing large amounts of infrequently used data. This tier is intended for data that remains in the Cool tier for at least 30 days. A usage case for the Cool tier is short-term backup and disaster recovery datasets and older media content. This content shouldn't be viewed frequently, but it needs to be immediately available. Storing data in the Cool tier is more cost-effective. Accessing data in the Cool tier can be more expensive than accessing data in the Hot tier.\n\nCold tier. The Cold tier is also optimized for storing large amounts of infrequently used data. This tier is intended for data that can remain in the tier for at least 90 days.\n\nArchive tier. The Archive tier is an offline tier that's optimized for data that can tolerate several hours of retrieval latency. Data must remain in the Archive tier for at least 180 days or be subject to an early deletion charge. Data for the Archive tier includes secondary backups, original raw data, and legally required compliance information. This tier is the most cost-effective option for storing data. Accessing data is more expensive in the Archive tier than accessing data in the other tiers.\n\n\nThings to know about Azure Blob immutable storage\nImmutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data can't be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes. Policies are applied at the container level and audit logs are available.\n\n\nAzure Blob Storage supports two forms of immutability policies for implementing immutable storage:\n\nTime-based retention policies let users set policies to store data for a specified interval. When a time-based retention policy is in place, objects can be created and read, but not modified or deleted. After the retention period expires, objects can be deleted, but not overwritten. The Hot, Cool, and Archive access tiers support immutable storage by using time-retention policies.\n\nLegal hold policies store immutable data until the legal hold is explicitly cleared. When a legal hold is set, objects can be created and read, but not modified or deleted. Premium Blob Storage uses legal holds to support immutable storage.\n\n\nThings to consider when implementing Azure Blob Storage\nYou review the different access options for Azure Blob Storage, and how to use immutable storage. Take a few minutes to determine how you can configure Azure Blob Storage for Tailwind Traders.\n\nConsider Blob Storage availability. Determine the level of availability required for your data. Are there scenarios where offline data is sufficient? The Archive access tier is optimized for data that can remain offline for hours.\n\nConsider Blob Storage latency. Plan for the required time to access the first byte of data in different scenarios. Some work tasks require instant access to data, while others can accommodate some delay. Premium Blob Storage supports single-digit millisecond latency for data, while the Hot and Cool access tiers support latency in milliseconds.\n\nConsider Blob Storage costs. Weigh your options for total cost. Factor in data storage minimum durations, and potential charges for transactions and access. Premium Blob Storage and the Hot access tier have higher overall storage costs, but lower charges for access and transactions. The Cool and Archive access tiers offer lower storage costs, but tend to have higher charges for access and transactions.\n\nConsider immutable storage. Review your business scenarios to identify where you might need immutable storage. Consider the different types immutability policies and which form satisfies your organization's requirements.\n\n\nTip\nContinue your learning with the Configure Azure Blob Storage training module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage",
      "title": "Design for Azure Blob Storage - Training | Microsoft Learn",
      "description": "Design for Azure Blob Storage",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/5-design-for-azure-blob-storage",
      "load_timestamp": "2025-06-01T22:19:26.571867",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Files - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Files\n\n\nCompleted\n\n\n9 minutes\n\n\nAzure Files provides fully managed cloud-based file shares that are hosted on Azure. Shared files are accessible by using the industry standard Server Message Block (SMB) protocol, Network File System (NFS) protocol, and the Azure Files REST API. You can mount or connect to an Azure file share at the same time on all the main operating systems. This video compares files storage to blob storage.\n\nThings to know about Azure Files\nAzure Files can be used to add to or replace a company's existing on-premises network attached storage (NAS) devices or file servers. Here are some reasons why your organization might want to use Azure Files:\n\nDevelopers can store apps and configuration files in a file share and connect new VMs to the shared files. This action reduces the time to get new machines into production.\n\nWith file shares on Azure, a company doesn't need to buy and deploy expensive redundant hardware and manage software updates. The shares are cross-platform, and you can connect to them from Windows, Linux, or macOS.\n\nYour file share has all the resilence of the Azure platform, which makes files globally redundant. You also gain options to use the integrated snapshots feature, and set up automatic backups by using Recovery Services vaults.\n\nAll the data is encrypted in transit by using HTTPS and is stored encrypted when at rest.\n\n\nChoose your data access method\nTo move your company's shared files into Azure Files, you need to analyze your options and make an important decision. How are you going to access and update the files? You could replace your existing Server Message Block (SMB) file shares with their equivalent in Azure Files. Another option is to set up an instance of Azure File Sync. If you choose to use Azure File Sync, there's more flexibility on how files are secured and accessed.\nAzure file shares can be used in two ways. You can directly mount serverless Azure file shares (SMB) or cache Azure file shares on-premises by using Azure File Sync.\n\nDirect mount of Azure file shares: Because Azure Files provides SMB access, you can mount Azure file shares on-premises or in the cloud. Mounting uses the standard SMB client available in Windows, macOS, and Linux. Because Azure file shares are serverless, deploying for production scenarios doesn't require managing a file server or NAS device. Direct mounting means you don't have to apply software patches or swap out physical disks.\n\nCache Azure file shares on-premises with Azure File Sync: Azure File Sync lets you centralize your organization's file shares. Azure Files provides the flexibility, performance, and compatibility of an on-premises file server. Azure File Sync transforms an on-premises (or cloud) Windows Server into a quick cache of your Azure file share.\n\n\nChoose your performance level\nBecause Azure Files stores files in a storage account, you can choose your performance level. Performance metrics differ between standard and premium storage account levels. Premium accounts offer lower latency and higher IOPS and bandwidth.\nStandard performance accounts use HDD to store data. With HDD, the costs are lower but so is the performance. SSD arrays back the premium storage account's performance, which comes with higher costs. Currently, premium accounts can only use file storage accounts with ZRS storage in a limited number of regions.\nDetermine your storage tier\nAzure Files offers four tiers of storage. These tiers allow you to tailor your file shares to meet the performance and price requirements for your scenarios.\n\nPremium: File shares use solid-state drives (SSDs) and provide consistent high performance and low latency. Used for the most intensive IO workloads. Suitable workloads include databases, web site hosting, and development environments. Can be used with both Server Message Block (SMB) and Network File System (NFS) protocols.\n\nTransaction optimized: Used for transaction heavy workloads that don't need the latency offered by premium file shares. File shares are offered on the standard storage hardware backed by hard disk drives (HDDs).\n\nHot access tier: Storage optimized for general purpose file sharing scenarios such as team shares. Offered on standard storage hardware using HDDs.\n\nCool access tier: Cost-efficient storage optimized for online archive storage scenarios. Offered on storage hardware using HDDs.\n\n\nThings to consider when choosing your implementation\nYour decision about which technology to implement depends on your business use cases, the protocols required for your files, and your performance goals. We review considerations for using Azure Blob Storage and Azure Files. Another option is to use Azure NetApp Files, which is a fully managed, highly available, enterprise-grade NAS service. Azure NetApp Files can handle the most demanding, high-performance, low-latency workloads. You can migrate workloads that are deemed \"unmigratable.\"\nThe following table compares features and uses cases for these three implementation options. Consider how you might implement Azure Blob Storage or Azure NetApp Files instead of Azure Files storage for Tailwind Traders.\n\n\nComparison\nAzure Blob Storage\nAzure Files\nAzure NetApp Files\n\n\nDescription\nAzure Blob Storage is best suited for large scale read-heavy sequential access workloads where data is ingested once and modified later. Blob Storage offers the lowest total cost of ownership, if there's little or no maintenance.\nAzure Files is a highly available service best suited for random access workloads. For NFS shares, Azure Files provides full POSIX file system support and can easily be used from container platforms like Azure Container Instance (ACI) and Azure Kubernetes Service (AKS).\nAzure NetApp Files is a fully managed file service in the cloud, powered by NetApp, with advanced management capabilities. Azure NetApp Files is suited for workloads that require random access and provides broad protocol support and data protection capabilities.\n\n\nUse cases\nLarge scale analytical data, Throughput sensitive high-performance computing, Backup and archive, Autonomous driving, Media rendering, or Genomic sequencing\nShared files, Databases, Home directories, Traditional applications, ERP, CMS, NAS migrations that don't require advanced management, Custom applications that require scale-out file storage\nOn-premises enterprise NAS migration that requires rich management capabilities, Latency sensitive workloads like SAP HANA, Latency-sensitive, or IOPS intensive high performance compute, Workloads that require simultaneous multi-protocol access\n\n\nAvailable protocols\n- NFS 3.0  - REST  - Data Lake Storage Gen2\n- SMB  - NFS 4.1  - REST\n- NFS 3.0 and 4.1  - SMB\n\n\nPerformance (per volume)\nUp to 20,000 IOPS. Up to 15 GiB/s throughput.\nUp to 100,000 IOPS. Up to 10 GiB/s throughput\nUp to 460,000 IOPS. Up to 4.5 GiB/s throughput for regular volumes. Up to 10 GiB/s throughput for large volumes.\n\n\nTip\nLearn more about file based workloads in the Introduction to Azure NetApp Files training module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files",
      "title": "Design for Azure Files - Training | Microsoft Learn",
      "description": "Design for Azure Files",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/6-design-for-azure-files",
      "load_timestamp": "2025-06-01T22:19:26.817851",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure managed disks - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure managed disks\n\n\nCompleted\n\n\n5 minutes\n\n\nAzure offers many disk solutions. In this module, we examine how to work with data disks by using Azure managed disks.\nData disks are used by virtual machines to store data like database files, website static content, or custom application code. The number of data disks you can add depends on the virtual machine size. Each data disk has a maximum capacity of 32,767 GB.\n\nTip\nMicrosoft recommends always using Azure managed disks. You specify the disk size, the disk type, and provision the disk. Azure handles the remaining operations.\n\nThings to know about managed disks\nAzure offers several types of managed disks. The following table shows a comparison of four data disk types.\n\n\nComparison\nUltra-disk\nPremium SSD\nStandard SSD\nStandard HDD\n\n\nDisk type\nSSD\nSSD\nSSD\nHDD\n\n\nScenario\nIO-intensive workloads, such as SAP HANA, top tier databases like SQL Server and Oracle, and other transaction-heavy workloads\nProduction and performance sensitive workloads\nWeb servers, Lightly used enterprise applications, development, and testing\nBackup, Noncritical, Infrequent access\n\n\nChoose an encryption option\nThere are several encryption types available for your managed disks.\n\nAzure Disk Encryption (ADE) encrypts the VM's virtual hard disks (VHDs). If VHD is protected with ADE, the disk image is accessible only by the VM that owns the disk.\n\nServer-Side Encryption (SSE) is performed on the physical disks in the data center. If someone directly accesses the physical disk, the data is encrypted. When the data is accessed from the disk, it's decrypted and loaded into memory. This form of encryption is also referred to as encryption at rest or Azure Storage encryption.\n\nEncryption at host ensures that data stored on the VM host is encrypted at rest and flows encrypted to the Storage service. Disks with encryption at host enabled aren't encrypted with SSE. Instead, the server hosting your VM provides the encryption for your data, and that encrypted data flows into Azure Storage.\n\n\nThings to consider when using managed disks\nThink about what data disk types are needed for Tailwind Traders. Consider your scenarios, throughput, and IOPS.\n\nConsider your scenarios, throughput, and IOPS. Compare disk types and choose the data disks that satisfy your business scenarios, and throughput and IOPS requirements. For more information, see Select a disk type for Azure IaaS VMs - managed disks\n\nUltra-disk storage: Azure Ultra Disk storage provides the best performance. Choose this option when you need the fastest storage performance in addition to high throughput, high input/output operations per second (IOPS), and low latency. Ultra-disk storage might not be available in all regions.\n\nPremium SSD storage: Azure Premium SSD-managed disks provide high throughput and IOPS with low latency. These disks offer a slightly less performance compared to Ultra Disk Storage. Premium SSD storage is available in all regions.\n\nStandard SSD: Azure Standard SSD-managed disks are a cost-effective storage option for VMs that need consistent performance at lower speeds. Standard SSD disks aren't as fast as Premium SSD disks or Ultra Disk Storage. You can attach Standard SSD disks to any VM.\n\nStandard HDD: In Azure Standard HDD-managed disks, data is stored on conventional magnetic disk drives that have moving spindles. Disks are slower and the variation in speeds is higher compared to solid-state drives (SSDs). Like Standard SSD disks, you can use Standard HDD disks for any VM.\n\n\nConsider data caching. Improve performance with disk caching. Azure Virtual Machines disk caching optimizes read and write access to the virtual hard disk (VHD) files. The VHDs are attached to Azure Virtual Machines. For OS disks, the default cache setting is ReadWrite, and for data disks, the default is ReadOnly.\n\nWarning\nDisk caching isn't supported for disks 4 TiB and larger. When multiple disks are attached to your Virtual Machine, each disk smaller than 4 TiB supports caching. Changing the cache setting of an Azure disk, detaches and reattaches the target disk. When it's the OS disk, the VM is restarted.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/7-design-for-azure-disk-solutions",
      "title": "Design for Azure managed disks - Training | Microsoft Learn",
      "description": "Design for Azure managed disks",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/7-design-for-azure-disk-solutions",
      "load_timestamp": "2025-06-01T22:19:26.984404",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for storage security - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for storage security\n\n\nCompleted\n\n\n7 minutes\n\n\nAzure Storage provides a layered security model that lets you secure and control the level of access to your storage accounts. The model consists of several storage security options, including firewall policies, customer-managed keys, and endpoints. This video from the Developers course highlights storage security features.\n\nThings to know about storage security\nLet's take a look at some best practices for storage security. Think about options can be used for the Tailwind Traders infrastructure.\n\nAzure security baseline for Azure Storage grants limited access to Azure Storage resources. Azure security baseline provides a comprehensive list of ways to secure your Azure storage.\n\nShared access signatures (SAS) provide secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data.\n\nFirewall policies and rules limit access to your storage account. Requests can be limited to specific IP addresses or ranges, or to a list of subnets in an Azure virtual network. The Azure Storage firewall provides access control for the public endpoint of your storage account.\n\nSecure transfer enables an Azure storage account to accept requests from secure connections. When you require secure transfer, any requests originating from nonsecure connections are rejected. Microsoft recommends that you always require secure transfer for all your storage accounts.\n\nData in your storage account is automatically encrypted. Azure Storage encryption offers two ways to manage encryption keys at the storage account level:\n\nMicrosoft-managed keys: By default, Microsoft manages the keys used to encrypt your storage account.\n\nCustomer-managed keys: You can optionally choose to manage encryption keys for your storage account. These keys must be stored in Azure Key Vault.\n\n\nThings to consider when implementing storage security\nYou review some of the security options for Azure Storage. Take a few minutes to determine how you can configure security for Tailwind Traders.\n\nConsider Azure security baseline options. Review the comprehensive options provided by Azure security baseline provides to secure your Azure storage. Grant limited access to Azure Storage resources.\n\nConsider shared access signatures. Specify what Tailwind Traders resources clients can access. Define the access permissions for resources. Configure how long the SAS remains valid.\n\nConsider firewall policies and rules. Limit requests to IP addresses or subnets in an Azure virtual network. Use the Azure Storage firewall to block all access through the public endpoint when using private endpoints. Select trusted Azure platform services to access the storage account securely.\n\nConsider private endpoints. Add private endpoints to create a special network interface for an Azure service in your virtual network. When you implement a private endpoint for your storage account, it provides secure connectivity between clients on your virtual network and your storage.\n\n\nConsider secure transfer. (Microsoft recommended) Always require secure transfer for all your Azure storage accounts. In the Azure portal, choose Enable secure transfer for your storage accounts. The Secure transfer required property is enabled by default when an Azure storage account is created.\n\nConsider customer-managed keys. Manage encryption keys for your storage account by using customer-managed keys stored in Azure Key Vault. Customer-managed keys give you full control over access to your encryption keys and encrypted data.\n\n\nTip\nThere is a lot more to learn about storage security in the Plan and implement security for storage training module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/8-design-for-storage-security",
      "title": "Design for storage security - Training | Microsoft Learn",
      "description": "Design for storage security",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/8-design-for-storage-security",
      "load_timestamp": "2025-06-01T22:19:27.149679",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n4 minutes\n\n\nTailwind Traders wants to reduce storage costs by reducing duplicate content and migrating it to the cloud, when possible. The company is interested in a solution that centralizes maintenance while providing nation-wide access for customers. Customers should be able to browse and purchase items online even an entire Azure region goes offline. Here are some specific requirements:\n\nWarranty document retention. The Compliance and Legal teams require warranty documents be kept for three years.\n\nNew photos and videos. Each product has an accompanying photo or video to demonstrate the product.\n\nExternal vendor development. A vendor creates and develops some of the online e-commerce features. The developer needs access to the HTML files, but only during the development phase.\n\nProduct catalog updates. The product catalog is updated every few months. Older versions of the catalog aren't viewed frequently, but they must be available immediately, if accessed.\n\n\n1.\nWhat's the best way for Tailwind Traders to protect their warranty information?\n\n\nLegal hold policy\n\n\nTime-based retention policy\n\n\nPrivate endpoint for storage account\n\n\n2.\nWhich storage option should Tailwind Traders use for their photos and videos?\n\n\nBlob Storage\n\n\nAzure Files\n\n\nDisk storage\n\n\n3.\nHow can you provide your developers with access to the business e-commerce HTML files?\n\n\nEnable secure transfer\n\n\nEnable firewall policies and rules\n\n\nShared access signatures\n\n\n4.\nWhich access tier should be used for the older versions of the product catalog?\n\n\nHot access tier\n\n\nCold or Cool access tier\n\n\nArchive access tier\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/9-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/9-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:27.350428",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary and resources - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary and resources\n\n\nCompleted\n\n\n2 minutes\n\n\nIn this module, you learned how to design for unstructured nonrelational data. You compared the features of different types of Azure storage accounts and Storage access tiers. You explored scenarios for using Azure Blob Storage, Azure managed disks, Azure Files, and Azure NetApp Files. You reviewed how to implement support for data redundancy, and examined options for storage security, including customer-managed encryption keys, and secure transfer.\nLearn more with Copilot\nCopilot can assist you in designing Azure infrastructure solutions. Copilot can compare, recommend, explain, and research products and services where you need more information. Open a Microsoft Edge browser and choose Copilot (top right) or navigate to copilot.microsoft.com. Take a few minutes to try these prompts and extend your learning with Copilot.\n\nMy company has a large amount of unstructured data storage. How can this data be hosted in Azure?\n\nHow can I migrate data to Azure storage? How do I decide between the different migration options?\n\nHow do I select an Azure storage replication strategy? Provide example usage cases.\n\nHow can I minimize the cost of Azure storage? Specifically review cost saving options for Blob storage.\n\nList at least 10 best practices to ensure my Azure storage is secure. Group the suggestions by ease of implementation and explain.\n\n\nLearn more with Azure documentation\n\nRead an introduction to Azure Storage.\n\nReview Azure Storage options.\n\nExamine Azure disk storage options.\n\nConfigure Azure security baseline for Azure Storage.\n\nDiscover more about Azure Blob Storage.\n\nExplore more about Azure Files.\n\nRead more about Azure NetApp Files.\n\n\nLearn more with self-paced training\n\nChoose the right disk storage for your virtual machine workload.\n\nConfigure Azure Blob Storage.\n\nOptimize performance and costs by using Azure disk storage.\n\nGet an introduction to securing data at rest on Azure.\n\nChoose a data storage approach in Azure.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/10-summary-resources",
      "title": "Summary and resources - Training | Microsoft Learn",
      "description": "Summary and resources",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-non-relational-data/10-summary-resources",
      "load_timestamp": "2025-06-01T22:19:27.557475",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n1 minute\n\n\nImplementing High Availability and Disaster Recovery (HADR) solutions for a database platform involves more than just deploying a feature. It's crucial to understand the reasons behind your choices and the strategies you're implementing.\nFor example, if you need to set up HADR for a virtual machine, do you know how much time you have to bring everything back online if there's a problem? Are you aware of the available options in SQL Server and the reasons for choosing one over another?\nBy the end of this module, you'll have a solid foundation to implement HADR solutions in Azure.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/1-introduction",
      "load_timestamp": "2025-06-01T22:19:27.719664",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Describe recovery time objective and recovery point objective - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDescribe recovery time objective and recovery point objective\n\n\nCompleted\n\n\n10 minutes\n\n\nUnderstanding recovery time and recovery point objectives are crucial to your high availability and disaster recovery (HADR) plan as they're the foundation for any availability solution.\nRecovery Time Objective\nRecovery Time Objective (RTO) is the maximum amount of time available to bring resources online after an outage or problem. If that process takes longer than the RTO, there could be consequences such as financial penalties, work not able to be done, and so on. RTO can be specified for the whole solution, which includes all resources, and for individual components such as SQL Server instances and databases.\nRecovery Point Objective\nRecovery Point Objective (RPO) is the point in time to which a database should be recovered and equates to the maximum amount of data loss that the business is willing to accept. For example, suppose an IaaS VM containing SQL Server experiences an outage at 10:00 AM and the databases within the SQL Server instance have an RPO of 15 minutes. No matter what feature or technology is used to bring back that instance and its databases, the expectation is that there will be at most 15 minutes worth of data lost. That means the database can be restored to 9:45 AM or later to ensure minimal to no data loss meeting that stated RPO. There may be factors that determine if that RPO is achievable.\nDefining Recovery Time and Recovery Point Objectives\nRTOs (Recovery Time Objectives) and RPOs (Recovery Point Objectives) are driven by business requirements and many technological factors, including the skills of administrators. While businesses may desire no downtime or zero data loss, this is often unrealistic. Determining RTO and RPO should involve open discussions among all parties.\nUnderstanding the cost of downtime is crucial for defining RTO and RPO. For example, if downtime costs the business $10,000 per hour or results in fines, this helps in setting realistic objectives. The investment in a High Availability and Disaster Recovery (HADR) solution should be proportional to the cost of downtime. If a solution prevents hours or days of downtime, it justifies its cost.\nRTO should be defined at both the component level (for example, SQL Server) and the entire application architecture. The overall RTO is determined by the slowest component to recover. For instance, if SQL Server recovers in five minutes but application servers take 20 minutes, the overall RTO is 20 minutes.\nRPO focuses on data and influences the design of HADR solutions and administrative policies. Features used must support the defined RTO and RPO. For example, if transaction log backups are scheduled every 30 minutes but the RPO is 15 minutes, the database can only be recovered to the last backup, which could be 30 minutes old. Regularly testing backups ensures their reliability.\nThe choice of solutions, such as Always On Availability Groups (AG) or Failover Cluster Instances (FCI), affects RTO and RPO. Depending on configuration, IaaS or PaaS solutions may not automatically fail over, leading to longer downtime. If RTO and RPO are unrealistic, they must be adjusted. For example, if a backup takes three hours to copy but the RTO is two hours, the RTO is missed.\nRTOs and RPOs should be defined for both High Availability (HA) and Disaster Recovery (DR). HA events are localized and easier to recover from, such as an AG failing over within an Azure region, potentially taking seconds. DR involves bringing up a new data center, which can take hours or longer, hence separate RTOs and RPOs.\nAll RTOs and RPOs should be documented and periodically revised. Once documented, appropriate technologies and features can be considered for the architecture.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/2-describe-recovery-time-objective-recovery-point-objective",
      "title": "Describe recovery time objective and recovery point objective - Training | Microsoft Learn",
      "description": "Describe recovery time objective and recovery point objective",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/2-describe-recovery-time-objective-recovery-point-objective",
      "load_timestamp": "2025-06-01T22:19:27.896570",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Explore high availability and disaster recovery options - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nExplore high availability and disaster recovery options\n\n\nCompleted\n\n\n15 minutes\n\n\nTo envision a solution for virtual machines (VMs), you must first understand the availability options for IaaS-based deployments.\nInfrastructure-as-a-Service versus Platform-as-a-Service\nWhen it comes to availability, the choice of IaaS or PaaS makes a difference. With IaaS, you have a virtual machine, which means there's an operating system with an installation of SQL Server. The administrator or group responsible for SQL Server would have a choice of high availability and disaster recovery (HADR) solutions and a great deal of control over what how that solution was configured.\nWith PaaS-based deployments such as Azure SQL Database, the HADR solutions are built into the feature and often just need to be enabled. There are minimal options that can be configured.\nBecause of these differences, the choice of IaaS or PaaS may influence the final design of your HADR solution.\nSQL Server HADR Features for Azure Virtual Machine\nWhen using IaaS, you can use the features provided by SQL Server to increase availability. In some cases, they can be combined with Azure-level features to increase availability even further.\nThe following table shows the solutions available in SQL Server:\n\n\nFeature Name\nProtects\n\n\nAlways On Failover Cluster Instance (FCI)\nInstance\n\n\nAlways On Availability Group (AG)\nDatabase\n\n\nLog Shipping\nDatabase\n\n\nAn instance of SQL Server is the entire installation of SQL Server (binaries, all the objects inside the instance including things like logins, SQL Server Agent jobs, and databases). Instance-level protection means that the entire instance is accounted for in the availability feature.\nA database in SQL Server contains the data that end users and applications use. There are system databases that SQL Server relies on, and databases created for use by end users and applications. An instance of SQL Server always has its own system databases. Database-level protection means that anything that is in the database, or is captured in the transaction log for a user or application database, is accounted for as part of the availability feature. Anything that exists outside of the database or that isn't captured as part of the transaction log such as SQL Server Agent jobs and linked servers must be manually dealt with to ensure the destination server can function like the primary if there's a planned or unplanned failover event.\nBoth FCIs and AGs require an underlying cluster mechanism. For SQL Server deployments running on Windows Server, it's a Windows Server Failover Cluster (WSFC) and for Linux it's Pacemaker.\nAlways On Failover Cluster Instances\nAn FCI is configured when SQL Server is installed. A standalone instance of SQL Server can't be converted to an FCI. The FCI is assigned a unique name and an IP address that is different from the underlying servers, or nodes, participating in the cluster. The name and IP address must also be different from the underlying cluster mechanism. Applications and end users would use the unique name of the FCI for access. This abstraction enables applications to not have to know where the instance is running. One major difference between Azure-based FCIs versus on-premises FCIs, is that for Azure, an internal load balancer (ILB) is required. The ILB is used to help ensure applications and end users can connect to the FCI’s unique name.\nWhen an FCI fails over to another node of a cluster, whether it's initiated manually or happens due to a problem, the entire instance restarts on another node. That means the failover process is a full stop and start of SQL Server. Any applications or end users connected to the FCI is disconnected during failover and only applications that can handle and recover from this interruption can reconnect automatically.\nWhen the instance starts up on the other node, it undergoes the recovery process. The Failover Cluster Instance (FCI) is consistent up to the point of failure, ensuring no data loss. Any transactions that need to be rolled back is handled during recovery. Since this is instance-level protection, all necessary components (logins, SQL Server Agent jobs, etc.) are already in place, allowing business operations to resume as usual once the databases are ready.\nFCIs require one copy of a database, but that is also its single point of failure. To ensure another node can access the database, FCIs require some form of shared storage. For Windows Server-based architectures, this can be achieved via an Azure Premium File Share, iSCSI, Azure Shared Disk, Storage Spaces Direct (S2D), or a supported third-party solution like SIOS DataKeeper. FCIs using Standard Edition of SQL Server can have up to two nodes. FCIs also require the use of Active Directory Domain Services (AD DS) and Domain Name Services (DNS), so that means AD DS and DNS must be implemented somewhere in Azure for an FCI to work.\nIn Windows Server, FCIs can use storage replica to create a native disaster recovery solution for FCIs without having to use another feature such as log shipping or AGs.\nAlways On availability groups\nAvailability Groups (AGs) were introduced in SQL Server 2012 Enterprise Edition and are available in Standard Edition starting with SQL Server 2016. In Standard Edition, an AG can contain one database, while in Enterprise Edition, it can have multiple databases. Although AGs share some similarities with Failover Cluster Instances (FCIs), they're different in many ways.\nThe main difference is that AGs provide database-level protection. The primary replica in an AG contains the read/write databases, while the secondary replica receives transactions from the primary to stay synchronized. Data movement can be synchronous or asynchronous. In Standard Edition, an AG can have up to two replicas (one primary, one secondary), whereas Enterprise Edition supports up to nine replicas (one primary, eight secondary). Secondary replicas are initialized from a database backup or through automatic seeding, which streams the backup to the secondary replica.\nAGs use a listener for abstraction, which functions like the unique name assigned to an FCI and has its own name and IP address. Applications and end users can connect using the listener, but direct connections to the instance are also possible. In Enterprise Edition, secondary replicas can be configured for read-only access and used for tasks like database consistency checks (DBCCs) and backups.\nAGs offer quicker failover times compared to FCIs and don't require shared storage. Each replica has its own copy of the data, increasing the total number of database copies and overall storage costs. For example, if the primary replica's data footprint is 1 TB, each replica will also have 1 TB of data, requiring 5 TB of space for five replicas.\nObjects outside the database or not captured in the transaction log must be manually created on any new primary replica. Examples include SQL Server Agent jobs, instance-level logins, and linked servers. Using Windows authentication or contained databases with AGs can simplify access.\nOrganizations may face challenges implementing highly available architectures and might only need the high availability provided by the Azure platform or a PaaS solution like Azure SQL Managed Instance. Before exploring Azure platform solutions, it's essential to know about another SQL Server feature: log shipping.\nLog shipping\nLog shipping has been a fundamental feature of SQL Server since its early days. It's based on backup, copy, and restore, making it one of the simplest methods for achieving High Availability and Disaster Recovery (HADR). While primarily used for disaster recovery, log shipping can also enhance local availability.\nLike Availability Groups (AGs), log shipping provides database-level protection, meaning you must account for SQL Server Agent jobs, linked servers, instance-level logins, and other objects. Unlike AGs, log shipping doesn't offer native abstraction, so switching to another server requires tolerating a name change. This can be mitigated using methods like DNS aliases configured at the network layer.\nThe log shipping process is straightforward: take a full backup of the source database on the primary server, restore it in a loading state (STANDBY or NORECOVERY) on a secondary server, known as a warm standby. This new copy is called the secondary database. SQL Server then automatically backs up the primary database's transaction log, copies the backup to the standby server, and restores it onto the standby.\nIn addition to SQL Server HADR features, there are Azure features that can enhance IaaS availability.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/3-explore-high-availability-disaster-recovery-options",
      "title": "Explore high availability and disaster recovery options - Training | Microsoft Learn",
      "description": "Explore high availability and disaster recovery options",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/3-explore-high-availability-disaster-recovery-options",
      "load_timestamp": "2025-06-01T22:19:28.124665",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Describe Azure high availability and disaster recovery  features for Azure Virtual Machines - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDescribe Azure high availability and disaster recovery  features for Azure Virtual Machines\n\n\nCompleted\n\n\n7 minutes\n\n\nAzure provides three main options to enhance availability for IaaS deployments:\n\nAvailability Sets\n\nAvailability Zones\n\nAzure Site Recovery\n\n\nAll three of these options are external to the virtual machine (VM) and don't know what kind of workload is running inside of it.\nAvailability sets\nAvailability sets provide uptime against Azure-related maintenance and single points of failure in a single data center. This was one of the first availability features introduced into the Azure platform, and effectively it can be thought of as anti-affinity rules for your VMs. This means if you had two SQL Server VMs in an availability set or log shipping pair, they would be guaranteed to never run on the same physical server.\n\n\nAvailability sets in Azure are designed to enhance the reliability and availability of virtual machines by separating them into fault domains and update domains. Fault domains are groups of servers within a data center that share the same power source and network. Each data center can have up to three fault domains, labeled as FD 0, 1, and 2 in the image. Update domains, indicated by UD in the image, represent groups of virtual machines and their underlying physical hardware that can be rebooted simultaneously. By distributing virtual machines across different update domains, Azure ensures that updates and maintenance activities don't affect all instances at once, thereby maintaining service continuity.\nAvailability sets and zones don't protect against in-guest failures, such as an OS or RDBMS crash; which is why you need to implement other solutions such as AGs or FCIs to ensure you meet RTOs and RPOs. Both availability sets and zones are designed to limit the impact of environmental problems at the Azure level such as datacenter failure, physical hardware failure, network outages, and power interruptions.\nFor a multi-tier application, you should put each tier of the application into its own availability set. For example, if you were building a web application that has a SQL Server backend along with Active Directory Domain Services (AD DS), you would create an availability set for each tier (web, database, and AD DS).\nAvailability sets aren't the only way to separate IaaS VMs. Azure also provides Availability Zones, but the two can't be combined. You can pick one or the other.\nAvailability zones\nAvailability zones account for data center-level failure in Azure. Each Azure region consists of many data centers with low latency network connections between them. When you deploy VM resources in a region that supports Availability Zones, you have the option to deploy those resources into Zone 1, 2, or 3. A zone is a unique physical location, that is, a data center, within an Azure region.\nZone numbers are logical representations. For example, if two Azure subscribers both deploy a VM into Zone 1 in their own subscriptions, that doesn't mean those VMs exist in the same physical Azure data center. Additionally, because of the distance there can be some extra latency introduced into zonal deployments. You should test the latency between your VMs to ensure that the latency meets performance targets. In most cases round-trip latency will be less than 1 millisecond, which supports synchronous data movement in features like availability groups. You can also deploy Azure SQL Database into Availability Zones.\nAzure Site Recovery\nAzure Site Recovery provides enhanced availability for VMs at the Azure level and can work with VMs hosting SQL Server. Azure Site Recovery replicates a VM from one Azure region to another to create a disaster recovery solution for that VM. As noted earlier, this feature doesn't know that SQL Server is running in the VM and knows nothing about transactions. While Azure Site Recovery may meet RTO, it may not meet RPO since it isn't accounting for where data is inside SQL Server. Azure Site Recovery has a stated monthly RTO of two hours. While most database professionals may prefer to use a database-based method for disaster recovery, Azure Site Recovery works well if it meets your RTO and RPO needs.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/4-describe-azure-high-availability-disaster-recovery-features",
      "title": "Describe Azure high availability and disaster recovery  features for Azure Virtual Machines - Training | Microsoft Learn",
      "description": "Describe Azure high availability and disaster recovery  features for Azure Virtual Machines",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/4-describe-azure-high-availability-disaster-recovery-features",
      "load_timestamp": "2025-06-01T22:19:28.281394",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Describe high availability and disaster recovery for PaaS deployments - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDescribe high availability and disaster recovery for PaaS deployments\n\n\nCompleted\n\n\n5 minutes\n\n\nPaaS is different when it comes to availability; you can only configure the options that Azure provides.\nFor the SQL Server-based options of Azure SQL Database and Azure SQL Managed Instance, the options are active geo-replication (Azure SQL Database only) and auto-failover groups (Azure SQL Database or Azure SQL Managed Instance).\nAzure SQL Database has a service level agreement, which guarantees availability of 99.99, meaning nearly no downtime should be encountered. If a node-level problem happens such as hardware failure, a built-in failover mechanism kicks in. All transactional changes to the database are written synchronously to storage upon commit. If a node-level interruption occurs, the database server automatically creates a new node and attaches the data storage.\nFrom an application standpoint, you need to code the necessary retry logic because all connections are dropped as part of spinning up the new node and any in flight transactions are lost. This process is considered a best practice for any cloud application, as they should be designed to handle transient failures.\nAzure SQL Database and Azure SQL Managed Instance offer the option to create read replicas. These replicas can be used for activities such as reporting, which helps offload work from the primary database. Additionally, read replicas enhance availability by being located in different regions, ensuring that your data remains accessible even if one region experiences issue.\nDatabase availability\nIn Azure SQL Database and Azure SQL Managed Instance, you can't set a database state to OFFLINE or EMERGENCY. If you think about it, OFFLINE doesn't make sense, because you can't attach databases. Because you can't use EMERGENCY, you can't do emergency mode repair, but you shouldn't have to because Azure manages and maintains the service. Other capabilities, like RESTRICTED_USER and dedicated admin connection (DAC), are allowed in Azure SQL Database.\nAccelerated Database Recovery (ADR) is built into the engine. With ADR, the transaction log is aggressively truncated and a persisted version store (PVS) is used. This technology allows you to perform a transaction rollback instantly, solving a well-known problem with long-running transactions. It also allows Azure SQL to recover databases quickly.\nIn Azure SQL Database and Azure SQL Managed Instance, ADR greatly increases general database availability. It's a significant factor in the SLA. For these reasons, ADR is on by default and can't be turned off.\nDatabase consistency\nAs you learned in the beginning of this module, multiple copies of your data and backups exist both locally and across regions. Regularly, backup and restore integrity checks run. Detection for lost write and stale read is also in place. You can run DBCC CHECKDB (no repair), and CHECKSUM is on by default. In the back end, automatic page repair occurs when possible, and there's data integrity error alert monitoring. If there's no impact, repair without notification occurs. If there's an impact, proactive notification is provided.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/5-describe-high-availability-disaster-recovery-options",
      "title": "Describe high availability and disaster recovery for PaaS deployments - Training | Microsoft Learn",
      "description": "Describe high availability and disaster recovery for PaaS deployments",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/5-describe-high-availability-disaster-recovery-options",
      "load_timestamp": "2025-06-01T22:19:28.597844",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Explore high availability and disaster recovery solution for IaaS - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nExplore high availability and disaster recovery solution for IaaS\n\n\nCompleted\n\n\n11 minutes\n\n\nThere are many different combinations of features that could be deployed in Azure for IaaS. This section covers five common examples of SQL Server high availability and disaster recovery (HADR) architectures in Azure.\nSingle Region High Availability Example 1 – Always On availability groups\nIf you only need high availability and not disaster recovery, configuring an (availability group) AG is one of the most ubiquitous methods no matter where you're using SQL Server. The following image is an example of what one possible AG in a single region could look like.\n\n\nWhy is this architecture worth considering?\n\nThis architecture protects data by having more than one copy on different virtual machines (VMs).\n\nThis architecture allows you to meet recovery time objective (RTO) and recovery point objective (RPO) with minimal-to-no data loss if implemented properly.\n\nThis architecture provides an easy, standardized method for applications to access both primary and secondary replicas.\n\nThis architecture provides enhanced availability during patching scenarios.\n\nThis architecture needs no shared storage, so there's less complication than when using a failover cluster instance (FCI).\n\n\nSingle Region High Availability Example 2 – Always On Failover Cluster Instance\nUntil AGs were introduced, FCIs were the most popular way to implement SQL Server high availability. FCIs, however, were designed when physical deployments were dominant. In a virtualized world, FCIs don't provide many of the same protections in the way they would on physical hardware because it's rare for a VM to have a problem. FCIs were designed to protect against things like network card failure or disk failure, both of which would likely not happen in Azure.\nHaving said that, FCIs do have a place in Azure. They work, and as long as you have the right expectations about what is and isn't provided, an FCI is a perfectly acceptable solution. The following image shows a high-level view of what an FCI deployment looks like when using Storage Spaces Direct.\n\n\nWhy is this architecture worth considering?\n\nFCIs are still a popular availability solution.\n\nThe shared storage story is improving with feature like Azure Shared Disk.\n\nThis architecture meets most RTO and RPO for HA (although DR isn't handled).\n\nThis architecture provides an easy, standardized method for applications to access the clustered instance of SQL Server.\n\nThis architecture provides enhanced availability during patching scenarios.\n\n\nDisaster Recovery Example 1 – Multi-Region or Hybrid Always On availability group\nIf you're using AGs, one option is to configure the AG across multiple Azure regions or potentially as a hybrid architecture. This means that all nodes which contain the replicas participate in the same WSFC. This assumes good network connectivity, especially if this is a hybrid configuration. One of the biggest considerations would be the witness resource for the WSFC. This architecture would require AD DS and DNS to be available in every region and potentially on premises as well if this is a hybrid solution. The following image shows what a single AG configured over two locations looks like using Windows Server.\n\n\nWhy is this architecture worth considering?\n\nThis architecture is a proven solution; it's no different than having two data centers today in an AG topology.\n\nThis architecture works with Standard and Enterprise editions of SQL Server.\n\nAGs naturally provide redundancy with extra copies of data.\n\nThis architecture makes use of one feature that provides both HA and D/R\n\n\nDisaster Recovery Example 2 –Distributed availability group\nA distributed AG is an Enterprise Edition only feature introduced in SQL Server 2016. It's different than a traditional AG. Instead of having one underlying WSFC where all of nodes contain replicas participating in one AG as described in the previous example, a distributed AG is made up of multiple AGs. The primary replica containing the read/write database is known as the global primary. The primary of the second AG is known as a forwarder and keeps the secondary replica of that AG in sync. In essence, this is an AG of AGs.\nThis architecture makes it easier to deal with things like quorum since each cluster would maintain its own quorum, meaning it also has its own witness. A distributed AG would work whether you're using Azure for all resources, or if you're using a hybrid architecture.\nThe following image shows an example distributed AG configuration. There are two WSFCs. Imagine each is in a different Azure region or one is on premises and the other is in Azure. Each WSFC has an AG with two replicas. The global primary in AG 1 is keeping the secondary of replica of AG 1 synchronized as well as the forwarder, which also is the primary of AG 2. That replica keeps the secondary replica of AG 2 synchronized.\n\n\nWhy is this architecture worth considering?\n\nThis architecture separates out the WSFC as a single point of failure if all nodes lose communication\n\nIn this architecture, one primary isn't synchronizing all secondary replicas.\n\nThis architecture can provide failing back from one location to another.\n\n\nDisaster Recovery Example 3 – Log shipping\nLog shipping is one of the oldest HADR methods for configuring disaster recovery for SQL Server. As described, the unit of measurement is the transaction log backup. Unless the switch to a warm standby is planned to ensure no data loss, data loss will most likely occur. When it comes to disaster recovery, it's always best to assume some data loss even if minimal. The following image shows an example log shipping topology.\n\n\nWhy is this architecture worth considering?\n\nLog shipping is a tried-and-true feature that has been around for over 20 years\n\nLog shipping is easy to deploy and administer since it's based on backup and restore.\n\nLog shipping is tolerant of networks that aren't robust.\n\nLog shipping meets most RTO and RPO goals for DR.\n\nLog shipping is a good way to protect FCIs.\n\n\nDisaster Recovery Example 4 – Azure Site Recovery\nFor those who don't want to implement a SQL Server-based disaster solution, Azure Site Recovery is a potential option. However, most data professionals prefer a database-centric approach as it will generally have a lower RPO.\nThe following image shows where in the Azure portal you'd configure replication for Azure Site Recovery.\n\n\nWhy is this architecture worth considering?\n\nAzure Site Recovery works with more than just SQL Server.\n\nAzure Site Recovery may meet RTO and possibly RPO.\n\nAzure Site Recovery is provided as part of the Azure platform.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/6-explore-iaas-high-availability-disaster-recovery-solution",
      "title": "Explore high availability and disaster recovery solution for IaaS - Training | Microsoft Learn",
      "description": "Explore high availability and disaster recovery solution for IaaS",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/6-explore-iaas-high-availability-disaster-recovery-solution",
      "load_timestamp": "2025-06-01T22:19:28.892874",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Describe hybrid solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDescribe hybrid solutions\n\n\nCompleted\n\n\n4 minutes\n\n\nYou should now understand recovery time objectives (RTOs) and recovery point objectives (RPOs) as well as the different features both in SQL Server as well as Azure to increase availability, you can put all of that together and design a solution to meet your high availability and disaster recovery (HADR) requirements.\nWhile an architecture can be deployed in one or more Azure regions, many organizations will need or want to have solutions that span both on premises and Azure, or possibly Azure to another public cloud. This type of architecture is known as a hybrid solution.\nPaaS solutions by nature aren't designed to allow traditional hybrid solutions. HADR is provided by the Azure infrastructure. There are some exceptions. For example, SQL Server’s transactional replication feature can be configured from a publisher located on premises (or another cloud) to an Azure SQL Managed Instance subscriber, but not the other way.\nHybrid solutions are therefore IaaS-based since they rely on traditional infrastructure. Hybrid solutions are useful. Not only can they be used to help migrate to Azure, but the most common usage of a hybrid architecture is to create a robust disaster recovery solution for an on premises system. For example, a secondary replica for an AG can be added in Azure. That means any associated infrastructure must exist, such as AD DS and DNS.\nArguably the most important consideration for a hybrid HADR solution that extends to Azure is networking. Not having the right bandwidth could mean missing your RTO and RPO. Azure has a fast networking option called ExpressRoute. If ExpressRoute isn't something your company can or will implement, configure a secure site-to-site VPN so that the Azure VMs act as an extension of your on-premises infrastructure. Exposing IaaS VMs directly to the public internet is discouraged.\nAlthough not traditionally thought of as hybrid, Azure can also be used as the destination for a database backup and as cold, archival storage for backups.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/7-describe-hybrid-solutions",
      "title": "Describe hybrid solutions - Training | Microsoft Learn",
      "description": "Describe hybrid solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/7-describe-hybrid-solutions",
      "load_timestamp": "2025-06-01T22:19:29.059256",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n5 minutes\n\n\nCheck your knowledge\n\n\n1.\nWhat is RPO?\n\n\nThe number of nodes in a cluster\n\n\nThe point to which data needs to be recovered after a failure\n\n\nA partial database restores\n\n\n2.\nWhat is a hybrid solution?\n\n\nA solution that has resources both in Azure and on premises or in another cloud provider\n\n\nA solution that uses two different database engines, for example, MySQL and SQL Server\n\n\nA solution that spans two different versions of SQL Server\n\n\n3.\nWhat is available after failover with database-level protection in SQL Server?\n\n\nLogins, Databases, and SQL Server Agent Job\n\n\nDatabases and SQL Server Agent jobs\n\n\nWhatever is in the databases; anything outside must be dealt with manually\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/8-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/8-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:29.261291",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary\n\n\nCompleted\n\n\n1 minute\n\n\nDeploying the right HADR solution in Azure is more than just picking a feature. A solution must meet the requirements, and specifically, the RTO and RPO expected by the business. Depending on the path (IaaS or PaaS), there could be multiple options to choose from.\nNow that you've reviewed this module, you should be able to:\n\nExplain recovery time objective (RTO)\nExplain recovery point objective (RPO)\nExplain the available HADR options for both IaaS and PaaS\nDevise a HADR strategy",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/9-summary",
      "title": "Summary - Training | Microsoft Learn",
      "description": "Summary",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/describe-high-availability-disaster-recovery-strategies/9-summary",
      "load_timestamp": "2025-06-01T22:19:29.456187",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n2 minutes\n\n\nIn this module, we explore AI-ready compute solutions available in Azure. We look at Azure Virtual Machines, Azure Logic Apps, Azure App Service, Azure Functions, Azure Kubernetes Service, and other options. Some compute solutions support serverless scenarios, while other solutions work well with virtual machines and containers. We examine fast, scalable, flexible solution options to add compute power to your infrastructure.\nMeet Tailwind Traders\n\n\nTailwind Traders is a fictitious home improvement retailer. The company operates retail hardware stores across the globe and online.\nAs you work through this module, suppose you work for Tailwind Traders. The management team needs your input on several development projects that need to migrate to the cloud. There are also several new projects that should be optimized for the cloud. The departmental budgets are tight. It's important to select the right compute technology for each project. Ideally, you'd like to create and configure compute resources for each project, and pay only for the resources and services used.\nLearning objectives\nIn this module, you learn how to:\n\nChoose an Azure compute service.\n\nDesign for Azure virtual machines solutions.\n\nDesign for Azure Batch solutions.\n\nDesign for Azure Functions solutions.\n\nDesign for Azure Logic Apps solutions.\n\nDesign for Azure Container Instances solutions.\n\nDesign for Azure App Service solutions.\n\nDesign for Azure Kubernetes Service solutions.\n\n\nSkills measured\nThe content in the module helps you prepare for Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions. The module concepts are covered in:\nDesign infrastructure solutions\n\nDesign for compute solutions\n\nSpecify components of a compute solution based on workload requirements.\n\nRecommend a virtual machine-based solution.\n\nRecommend a container-based solution.\n\nRecommend a serverless-based solution.\n\nRecommend a compute solution for batch processing.\n\n\nPrerequisites\n\nConceptual knowledge of Azure compute solutions.\n\nWorking experience with virtual machines, containers, and app service.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/1-introduction",
      "load_timestamp": "2025-06-01T22:19:29.692453",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Choose an Azure compute service - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nChoose an Azure compute service\n\n\nCompleted\n\n\n5 minutes\n\n\nAzure offers several compute services. Compute refers to the hosting model for the computing resources that your applications run on. Azure provides a decision flowchart with high-level guidance for how to select the appropriate Azure compute service for your scenario.\n\nNote\nThe following diagram only shows the Azure services described in this module.\n\nThe output from this decision flowchart is a starting point for your planning. You need to do a detailed evaluation of the services to determine exactly which solution meets your requirements. As you work through this module, refer to this diagram to become familiar with the considerations and options.\n\n\nThings to know about Azure compute services\nLet's take a quick look at the Azure compute services we review in this module.\n\nAzure Virtual Machines: Deploy and manage virtual machines inside an Azure virtual network.\n\nAzure Batch: Apply this managed service to run large-scale parallel and high-performance computing (HPC) applications.\n\nAzure App Service: Host web apps, mobile app backends, RESTful APIs, or automated business processes with this managed service.\n\nAzure Functions: Use this managed service to run code in the cloud, without worrying about the infrastructure.\n\nAzure Logic Apps: Configure this cloud-based platform to create and run automated workflows similar to capabilities in Azure Functions.\n\nAzure Container Instances: Run containers in Azure in a fast and simple manner without creating virtual machines or relying on a higher-level service.\n\nAzure Kubernetes Service (AKS): Run containerized applications with this managed Kubernetes service.\n\n\nThings to consider when choosing Azure compute services\nAs you begin to compare Azure compute services to choose your infrastructure solution for Tailwind Traders, there are several implementation points to think about.\n\nArchitecture and infrastructure requirements.\nSupport for new workload scenarios, like HPC applications.\nRequired hosting options, including platform, infrastructure, and functions.\nSupport for migrations, such as cloud-optimized or lift and shift.\n\nWorkloads and architecture\nWhen you plan for new instances of Azure services and new workloads, consider the following scenarios.\n\nControl: Determine if you require full control over installed software and applications.\n\nWorkloads: Consider the workloads you need to support, such as HPC workloads or event-driven workloads.\n\nArchitecture: Think about what architecture best supports your infrastructure, including microservice, full-fledged orchestration, and serverless.\n\n\nMigrations\nAn important consideration for your compute service involves analyzing the migration capabilities.\n\nCloud optimized: To migrate to the cloud and refactor applications to access cloud-native features, consider compute services that are cloud-optimized.\n\nLift and shift: For lift and shift workload migrations, consider compute services that don't require application redesigns or code changes.\n\nContainerized: In your migration planning, consider whether your compute service needs to support containerized applications, or commercial off the shelf (COTS) apps.\n\n\nHosting\nThe hosting option of your compute solution determines the developer and cloud provider responsibilities. Azure offers three hosting options across the compute services.\n\n\nSoftware-as-a-Service (SaaS) delivers software applications over the internet, on a subscription basis. Users can access these applications via a web browser without needing to manage the underlying infrastructure or application software. Microsoft 365 is an example of a SaaS solution, providing cloud-based productivity tools like Word, Excel, Outlook, and Teams.\n\nPlatform-as-a-Service (PaaS) provides a managed hosting environment where developers can build, deploy, and manage applications without worrying about the underlying infrastructure. Azure handles the infrastructure, maintenance, and scalability, allowing developers to focus on writing code. Examples of Azure PaaS offerings include Azure App Services, Azure Functions, and Azure SQL Database.\n\nInfrastructure-as-a-Service (IaaS) provides virtualized computing resources over the internet. It allows users to rent virtual machines, storage, and networking resources on-demand, without the need for physical hardware. Azure Virtual Machines is a prime example of an IaaS offering, where users can deploy and manage VMs inside an Azure virtual network.\n\nOn-premises hosting involves deploying and managing applications on local servers within an organization's own data center. This option provides the highest level of control over the infrastructure but requires significant investment in hardware, maintenance, and IT staff. Azure also supports hybrid cloud solutions, allowing integration between on-premises infrastructure and Azure services.\n\n\nTip\nYou can extend your study of Azure AI-ready compute solutions with the Describe Azure compute and networking services module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/2-choose-compute-service",
      "title": "Choose an Azure compute service - Training | Microsoft Learn",
      "description": "Choose an Azure compute service",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/2-choose-compute-service",
      "load_timestamp": "2025-06-01T22:19:29.919671",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Virtual Machines solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Virtual Machines solutions\n\n\nCompleted\n\n\n9 minutes\n\n\nAzure Virtual Machines is the basis of the Azure Infrastructure-as-a-Service (IaaS) model. Virtual Machines can be used for developing, testing, and deploying applications in the cloud, or to extend your datacenter. Virtual Machines offers a fast, scalable, flexible way to add more compute power to your enterprise.\nThings to know about Azure Virtual Machines\nThere are two main scenarios where Azure Virtual Machines can be an ideal compute solution for an infrastructure. Virtual Machines can be used to build new workloads and migrate data by using the lift and shift pattern.\n\n\nBuild new workloads: Azure Virtual Machines is ideal when you're building new workloads and demand for your applications can fluctuate. It's economical to run your applications on a virtual machine in Azure.\n\nLift and shift migration: If you're using lift and shift (rehosting) migration to move data and applications from an on-premises location, targeting Azure Virtual Machines in the cloud is an effective strategy.\n\n\nThings to consider when using Azure Virtual Machines\nLet's walk through a checklist of things to consider when using Azure Virtual Machines as a compute solution. As you review these points, think about what configuration is needed for the Tailwind Traders requirements.\n\nStart with your network.\nName your virtual machine, and decide the location.\nDetermine the size of your virtual machine.\nReview the pricing model, and Azure Storage options.\nSelect an operating system.\n\nNetwork configuration\nThe first thing to think about isn't your virtual machines at all - it's the network. Spend some time thinking about your network configuration for Tailwind Traders. Network addresses and subnets aren't trivial to change after they're configured. If you have an on-premises network, you want to carefully consider the network topology before you create any virtual machines.\nVirtual machine name\nSome developers don't give much thought about the name for a virtual machine. However, the virtual machine name defines a manageable Azure resource, and the value isn't easy to change. Choose machine names that are meaningful and consistent, so you can easily identify what each virtual machine does.\nVirtual machine location\nAzure has datacenters all over the world filled with servers and disks. These datacenters are grouped into geographic regions like West US, North Europe, Southeast Asia, and so on. The datacenters provide redundancy and availability.\nEach virtual machine is in a region where you want the resources like CPU and storage to be allocated. The regional location lets you place your virtual machines as close as possible to your users. The location of the machine can improve performance and ensure you meet any legal, compliance, or tax requirements.\nThere are two other points to consider about the virtual machine location.\n\nThe machine location can limit your available options. Each region has different hardware available, and some configurations aren't available in all regions.\nThere are price differences between locations. To find the most cost-effective choice, check for your required configuration in different regions.\n\nVirtual machine size\nAfter you choose the virtual machine name and location, you need to decide on the size of your machine. Azure offers different memory and storage options for different virtual machine sizes.\nThe best way to determine the appropriate machine size is to consider the type of workload your machine needs to run. Based on the workload, you can choose from a subset of available virtual machine sizes. The following table shows size classifications for Azure Virtual Machines workloads and recommended usage scenarios.\n\n\nClassification\nDescription\nScenarios\n\n\nGeneral purpose\nGeneral-purpose virtual machines are designed to have a balanced CPU-to-memory ratio.\n- Testing and development  - Small to medium databases  - Low to medium traffic web servers\n\n\nCompute optimized\nCompute optimized virtual machines are designed to have a high CPU-to-memory ratio.\n- Medium traffic web servers  - Network appliances  - Batch processes  - Application servers\n\n\nMemory optimized\nMemory optimized virtual machines are designed to have a high memory-to-CPU ratio.\n- Relational database servers  - Medium to large caches  - In-memory analytics\n\n\nStorage optimized\nStorage optimized virtual machines are designed to have high disk throughput and I/O.\n- Virtual machines running databases\n\n\nGPU\nGPU virtual machines are specialized virtual machines targeted for heavy graphics rendering and video editing.\n- Model training and inferencing with deep learning\n\n\nHigh performance computes\nHigh performance compute offers the fastest and most powerful CPU virtual machines with optional high-throughput network interfaces.\n- Workloads that require fast performance  - High traffic networks\n\n\nVirtual machine pricing\nA subscription is billed two separate costs for every virtual machine: compute and storage. By separating these costs, you can scale them independently and only pay for what you need.\n\nCompute costs: Compute expenses are priced on a per-hour basis but billed on a per-minute basis. If the virtual machine is deployed for 55 minutes, you're charged for only 55 minutes of usage. You're not charged for compute capacity if you stop and deallocate the virtual machine. The hourly price varies based on the virtual machine size and operating system you select.\n\nStorage costs: You're charged separately for the Azure Storage the virtual machine uses. The status of the virtual machine has no relation to the Azure Storage charges that are incurred. You're always charged for any Azure Storage used by the disks.\n\n\nAzure Storage\nAzure Managed Disks handle Azure storage account creation and management in the background for you. You specify the disk size and the performance tier (Standard or Premium). Azure creates and manages the disk. As you add disks or scale the virtual machine up and down, you don't have to worry about the storage being used.\nOperating system\nAzure provides various operating system images that you can install into the virtual machine, including several versions of Windows and flavors of Linux. Azure bundles the cost of the operating system license into the price.\n\nIf you're looking for more than just base operating system images, you can search Azure Marketplace. There are various install images that include not only the operating system but popular software tools, such as WordPress. The image stack consists of a Linux server, Apache web server, a MySQL database, and PHP. Instead of setting up and configuring each component, you can install an Azure Marketplace image and get the entire stack all at once.\n\nIf you don't find a suitable operating system image, you can create your own disk image. Your disk image can be uploaded to Azure Storage and used to create an Azure virtual machine. Keep in mind that Azure only supports 64-bit operating systems.\n\n\nBusiness application\nTry the Azure Virtual Machines selector tool to find other sizes that best fit your workload.\n\nTip\nLearn more about virtual machines with the Introduction to Azure virtual machines module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/3-design-for-azure-virtual-machine-solutions",
      "title": "Design for Azure Virtual Machines solutions - Training | Microsoft Learn",
      "description": "Design for Azure Virtual Machines solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/3-design-for-azure-virtual-machine-solutions",
      "load_timestamp": "2025-06-01T22:19:30.121932",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Batch solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Batch solutions\n\n\nCompleted\n\n\n5 minutes\n\n\nAzure Batch runs large-scale applications efficiently in the cloud. You can schedule compute-intensive tasks and dynamically adjust resources for your solution without managing infrastructure. Azure Batch can create and manage a pool of compute nodes (virtual machines). Azure Batch can also install the application you want to run, and schedule jobs to run on the compute nodes.\nThings to know about Azure Batch\nThere are many scenarios where Azure Batch can be an ideal compute solution for your infrastructure. Azure Batch is similar to Azure Virtual Machines and can be used to build new workloads and migrate data.\n\n\nAzure Batch works well with applications that run independently (parallel workloads).\n\nAzure Batch is effective for applications that need to communicate with each other (tightly coupled workloads). You can use Batch to build a service that runs a Monte Carlo simulation for a financial services company or a service to process images.\n\nAzure Batch enables large-scale parallel and high-performance computing (HPC) batch jobs with the ability to scale to tens, hundreds, or thousands of virtual machines. When you're ready to run a job, Azure Batch:\n\nStarts a pool of compute virtual machines for you.\nInstalls applications and staging data.\nRuns jobs with as many tasks as you have.\nIdentifies failures, requeues work, and scales down the pool as work completes.\n\n\nHow Azure Batch works\nA typical real-world scenario for Azure Batch requires data and application files. The Batch workflow begins with uploading the data and application files to an Azure storage account. Based on the demand, you create a Batch pool with as many Windows or Linux virtual compute nodes as needed. If the demand increases, compute nodes can be automatically scaled.\n\n\nAs you plan for your own configuration, you can separate aspects of the scenario into two parts: your service and the Azure Batch compute.\n\nYour service uses Azure as the platform. The platform is used for completing computationally intensive work and retrieving results. You can also monitor jobs and task progress.\n\nAzure Batch operates as the compute platform behind your service. Batch uses Azure Storage to fetch applications or data needed to complete a task. Azure Batch writes output to Azure Storage. Behind the scenes, there are collections (pools) of virtual machines. Pools are the resources that jobs and tasks are executed on.\n\n\nThings to consider when using Azure Batch\nLet's look at some best practices for using Azure Batch. As you review the suggestions, think about what scenarios can be accomplished by integrating Azure Batch in the Tailwind Traders infrastructure.\n\nConsider pools. If your jobs consist of short-running tasks, don't create a new pool for each job. The overhead to create new pools diminishes the run time of the job. Also, it's best to have your jobs use pools dynamically. If your jobs use the same pool for everything, there's a higher chance of failure.\n\nConsider nodes. Individual nodes aren't guaranteed to always be available. If your Azure Batch workload requires deterministic, guaranteed progress, you should allocate pools with multiple nodes. Consider using isolated virtual machine sizes for workloads with compliance or regulatory requirements.\n\nConsider jobs. Uniquely name your jobs so you can accurately monitor and log the activity. Consider grouping your tasks into efficiently sized jobs. It's more efficient to use a single job that contains 1,000 tasks rather than creating 100 jobs that have 10 tasks each.\n\n\nTip\nContinue your learning with the Create an Azure Batch account by using the Azure portal module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/4-design-for-azure-batch-solutions",
      "title": "Design for Azure Batch solutions - Training | Microsoft Learn",
      "description": "Design for Azure Batch solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/4-design-for-azure-batch-solutions",
      "load_timestamp": "2025-06-01T22:19:30.384714",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure App Service solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure App Service solutions\n\n\nCompleted\n\n\n7 minutes\n\n\nAzure App Service is an HTTP-based service that lets you build and host web apps, background jobs, mobile backends, and RESTful APIs. You can use the programming language of your choice and build automated deployments from GitHub, Azure Pipelines, or any Git repo. App Service offers automatic scaling and high availability.\nThings to know about Azure App Service\nWith Azure App Service, all your apps share common benefits. These benefits make App Service the ideal compute solution for any hosted web application to support new workloads and migrate data.\n\n\nAzure App Service is a platform as a service (PaaS) environment. You focus on the website development and API logic. Azure handles the infrastructure to run and scale your web apps.\n\nApp Service supports development in multiple languages and frameworks, and offers integrated deployment and management with secured endpoints.\n\nApp Service offers built-in load balancing and traffic management at global scale with high availability.\n\nApp Service provides built-in authentication and authorization capabilities (sometimes referred to as Easy Auth). You can sign in users and access data by writing minimal or no code.\n\n\nContinuous deployment\nAzure App Service enables continuous deployment. Azure DevOps Services provides developer services for support teams to plan work, collaborate on code development, and build and deploy applications. Whenever possible when continuously deploying your code, use deployment slots for a new production build.\n\n\nWhen you choose a Standard App Service Plan tier or better, you can deploy your app to a staging environment, validate your changes, and do performance tests. When you're ready, you can swap your staging and production slots. The swap operation triggers the necessary worker instances to match your production scale.\nAzure App Service costs\nYou pay for the Azure compute resources your app uses while it processes requests. The cost is based on the Azure App Service plan you choose. The App Service plan determines how much hardware is devoted to your host. The plan specifies whether you're using dedicated or shared hardware and how much memory is reserved. You can have different app service plans for different apps, and your plan can be scaled up and down at any time.\nThings to consider when using Azure App Service\nLet's look at some scenarios for using Azure App Service. As you review these options, think about how you can integrate Azure App Service in the Tailwind Traders infrastructure.\n\nConsider web apps. Create web apps with App Service by using ASP.NET, ASP.NET Core, Java, Ruby, Node.js, PHP, or Python. You can choose either Windows or Linux as the host operating system.\n\nConsider API apps. Build API apps similar to REST-based web APIs with your choice of language and framework. Azure App Service offers full Swagger support, and the ability to package and publish your API in Azure Marketplace. The apps can be consumed from any HTTP or HTTPS client.\n\nConsider WebJobs. Use the App Service WebJobs feature to run a program or script. Program examples include Java, PHP, Python, or Node.js. Script examples include cmd, bat, PowerShell, or Bash. WebJobs are scheduled or run with a trigger. WebJobs are often used to run background tasks as part of your application logic.\n\nConsider Mobile apps. Exercise the Mobile Apps feature of Azure App Service to quickly build a backend for iOS and Android apps. On the mobile app side, App Service provides SDK support for native iOS and Android, Xamarin, and React native apps. With just a few steps in the Azure portal, you can:\n\nStore mobile app data in a cloud-based SQL database.\nAuthenticate customers against common social providers, such as MSA, Google, X, and Facebook.\nSend push notifications.\nExecute custom back-end logic in C# or Node.js.\n\n\nConsider continuous deployment. Choose the Standard App Service Plan tier or better to enable continuous deployment of your code. Deploy your app to a staging slot and validate your app with test runs. When the app is ready for release, swap your staging and production slots. The swap operation warms up the necessary worker instances to match your production scale, which eliminates downtime.\n\nConsider authentication and authorization. Take advantage of the built-in authentication capabilities in Azure App Service. You don't need any language, SDK, security expertise, or even any code to use the functionality in your web app or API. You can integrate with multiple sign-in providers, such as Microsoft Entra ID, Facebook, Google, and X. Azure Functions offers the same built-in authentication features that are available in App Service.\n\nConsider multiple plans to reduce costs. Configure different Azure App Service plans for different apps. Scale your plan up and down at any time. Start testing your web app in a Free App Service plan and pay nothing. When you want to add your custom DNS name to the web app, just scale your plan up to the Shared tier.\n\n\nTip\nContinue your learning with the Configure Azure App Service module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/5-design-for-azure-app-services-solutions",
      "title": "Design for Azure App Service solutions - Training | Microsoft Learn",
      "description": "Design for Azure App Service solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/5-design-for-azure-app-services-solutions",
      "load_timestamp": "2025-06-01T22:19:30.609235",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Container Instances solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Container Instances solutions\n\n\nCompleted\n\n\n11 minutes\n\n\nVirtual machines are an excellent way to reduce costs versus the investments that are necessary for physical hardware. However, each virtual machine is still limited to a single operating system. If you want to run multiple instances of an application on a single host machine, containers are an excellent choice.\nThis video highlights the differences between virtual machines and containers.\n\nThings to know about Azure Container Instances\nAzure Container Instances are a fast and simple way to run a container on Azure. Scenarios for using Azure Container Instance include simple applications, task automation, and build jobs.\nAzure Container Instances offers many benefits, including fast startup, per second billing, and persistent storage. These benefits make Azure Container Instances a great compute solution to support new workloads and migrate data by using the lift and shift pattern.\n\n\nAzure Container Instances enables fast startup. You can launch containers in seconds for immediate access to applications.\n\nAzure Container Instances implements per second billing. You incur costs only while your container is running.\n\nAzure Container Instances supports custom sizes for your containers. You can specify exact values for CPU cores and memory and avoid costs for unused resources.\n\nContainer Instances offers persistent storage. Azure Files shares can be mounted directly to a container to retrieve and persist state.\n\nContainer Instances can be used with Linux and Windows. Schedule both Windows and Linux containers using the same API.\n\n\nContainer groups\nThe top-level resource in Azure Container Instances is the container group. A container group is a collection of containers that get scheduled on the same host machine. The containers in a group share a lifecycle, resources, local network, and storage volumes.\n\n\nMulti-container groups are useful when you want to divide a single functional task into several container images. Different teams use these images. Some example scenarios include:\n\nA container serving a web application and a container pulling the latest content from source control.\nAn application container and a logging container. The logging container collects the logs and metrics output by the main application and writes them to long-term storage.\nAn application container and a monitoring container. The monitoring container periodically makes a request to the application to ensure it's running and responding correctly, and raises alerts as needed.\nA front-end container and a back-end container. The frontend might serve a web application with the backend running a service to retrieve data.\n\nThings to consider when using Azure Container Instances\nWhen you work with Azure Container Instances, there are several recommended security practices.\n\nUse a private registry. Containers are built from images that are stored in one or more repositories. These repositories can belong to a public registry or to a private registry. An example of a private registry is the Docker Trusted Registry, which can be installed on-premises or in a virtual private cloud. Another example is Azure Container Registry that can be used to build, store, and manage container images and artifacts.\n\nEnsure image integrity throughout the lifecycle. Part of managing security throughout the container lifecycle is to ensure the integrity of the container images. Images with vulnerabilities, even minor, shouldn't be allowed to run in a production environment. Keep the number of production images small to ensure they can be managed effectively.\n\nMonitor container resource activity. Monitor your resource activity, like files, network, and other resources that your containers access. Monitoring resource activity and consumption are useful both for performance monitoring and as a security measure.\n\n\nTip\nLearn more about container instances in the Configure Container Instance module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/6-design-for-azure-container-instances-solutions",
      "title": "Design for Azure Container Instances solutions - Training | Microsoft Learn",
      "description": "Design for Azure Container Instances solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/6-design-for-azure-container-instances-solutions",
      "load_timestamp": "2025-06-01T22:19:30.806913",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Kubernetes Service solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Kubernetes Service solutions\n\n\nCompleted\n\n\n6 minutes\n\n\nKubernetes is a portable, extensible open-source platform for automating deployment, scaling, and the management of containerized workloads. This orchestration platform provides the same ease of use and flexibility as with Platform as a Service (PaaS) and Infrastructure as a Service (IaaS) offerings. Kubernetes provides both container management and container orchestration.\n\n\nContainer management is the process of organizing, adding, removing, or updating a significant number of containers. Most of these tasks are manual and error prone. Container orchestration is a system that automatically deploys and manages containerized applications. The orchestrator can dynamically increase or decrease the deployed instances of the managed application. The orchestrator can also ensure all deployed container instances get updated if a new version of a service is released.\nAzure Kubernetes Service (AKS) manages your hosted Kubernetes environment and makes it simple to deploy and manage containerized applications in Azure.\nThings to know about Azure Kubernetes Service\nThe Azure Kubernetes Service environment is enabled with many features, such as automated updates, self-healing, and easy scaling. Review the following characteristics that make AKS an appealing compute option to build new workloads and support lift and shift migrations.\n\n\nAzure manages the Kubernetes cluster for free. You manage the agent nodes in the cluster and only pay for the virtual machines on which your nodes run.\n\nWhen you create the cluster, you can use Azure Resource Manager (ARM) templates to automate cluster creation. With ARM templates, you specify features like as advanced networking, Microsoft Entra ID integration, and monitoring.\n\nAKS gives you the benefits of open-source Kubernetes. You don't have the complexity or operational overhead of running your own custom Kubernetes cluster.\n\n\nThings to consider when using Azure Kubernetes Service\nThere are several factors to consider when deciding whether Azure Kubernetes Service is the right compute solution for your infrastructure. A good approach is to plan your strategy from two points of view. Consider the features from the approach of a green field new project, and also from the perspective of a lift-and-shift migration. The following features are configurable when you create a new cluster and also after you deploy.\n\n\nFeature\nConsideration\nSolution\n\n\nIdentity and security management\nDo you already use existing Azure resources and make use of Microsoft Entra ID?\nYou can configure an Azure Kubernetes Service cluster to integrate with Microsoft Entra ID and reuse existing identities and group membership.\n\n\nIntegrated logging and monitoring\nAre you using Azure Monitor?\nAzure Monitor provides performance visibility of the cluster.\n\n\nAutomatic cluster node and pod scaling\nDo you need to scale up or down a large containerization environment?\nAKS supports two auto cluster scaling options. The horizontal pod autoscaler watches the resource demand of pods and increases pods to meet demand. The cluster autoscaler component watches for pods that can't be scheduled because of node constraints. It automatically scales cluster nodes to deploy scheduled pods.\n\n\nCluster node upgrades\nDo you want to reduce the number of cluster management tasks?\nAKS manages Kubernetes software upgrades and the process of cordoning off nodes and draining them.\n\n\nStorage volume support\nDoes your application require persisted storage?\nAKS supports both static and dynamic storage volumes. Pods can attach and reattach to these storage volumes as they're created or rescheduled on different nodes.\n\n\nVirtual network support\nDo you need pod-to-pod network communication or access to on-premises networks from your AKS cluster?\nAn AKS cluster can be deployed into an existing virtual network with ease.\n\n\nIngress with HTTP application routing support\nDo you need to make your deployed applications publicly available?\nThe HTTP application routing add-on makes it easy to access AKS cluster deployed applications.\n\n\nDocker image support\nDo you already use Docker images for your containers?\nBy default, AKS supports the Docker file image format.\n\n\nPrivate container registry\nDo you need a private container registry?\nAKS integrates with Azure Container Registry (ACR). You aren't limited to ACR though, you can use other container repositories, public, or private.\n\n\nBusiness application\nTake a few minutes to read about how Mercedes-Benz R&D is using Azure Kubernetes Service.\n\nTip\nLearn more about Kubernetes in the Introduction to Kubernetes Service module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/7-design-for-azure-kubernetes-solutions",
      "title": "Design for Azure Kubernetes Service solutions - Training | Microsoft Learn",
      "description": "Design for Azure Kubernetes Service solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/7-design-for-azure-kubernetes-solutions",
      "load_timestamp": "2025-06-01T22:19:30.971316",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Functions solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Functions solutions\n\n\nCompleted\n\n\n4 minutes\n\n\nAzure Functions is a serverless application platform. Functions are used when you want to run a small piece of code in the cloud, without worrying about the infrastructure.\n\nThings to know about Azure Functions\nLet's review some benefits and scenarios of Azure Functions that make it a great compute solution for building new workloads.\n\n\nAzure Functions provides intrinsic scalability. You're charged only for the resources you use.\n\nWith Azure Functions, you can write your function code in the language of your choice.\n\nAzure Functions supports compute on demand in two significant ways:\n\nAzure Functions lets you implement your system's logic into readily available blocks of code. These code blocks (functions) can run anytime you need to respond to critical events.\n\nAs requests increase, Azure Functions meets the demand with as many resources and function instances as necessary. As requests complete, any extra resources and application instances drop off automatically.\n\n\nAzure Functions is an ideal solution for handling specific definable actions triggered by an event. A function can process an API call and store the processed data in Azure Cosmos DB. After the data transfer happens, another function can trigger a notification.\n\n\nThings to consider when using Azure Functions\nLet's look at some best practices for using Azure Functions. As you consider these suggestions, think about the advantages to using Azure Functions in the Tailwind Traders infrastructure.\n\nConsider long running functions. Avoid large, long-running functions that can cause unexpected time out issues. Whenever possible, refactor large functions into smaller function sets that work together and return responses faster. The default time out is 300 seconds for Consumption Plan functions, and 30 minutes for any other plan.\n\nConsider durable functions. Overcome time out issues in your configuration with durable functions and smaller function sets. Durable functions let you write stateful functions. Behind the scenes, the function manages the application state, checkpoints, and restarts. An example application pattern for durable functions is function chaining. Function chaining executes a sequence of functions in a specific order. The output of one function is applied to the input of another function.\n\nConsider performance and scaling. Plan how to group functions with different load profiles. Consider a scenario where you have two functions. One function processes many thousands of queued messages and has low memory requirements. The other function is called only occasionally but has high memory requirements. In this scenario, you might want to deploy separate function applications, where each function has its own set of resources. Separate resources means you can independently scale the functions.\n\nConsider defensive functions. Design your functions for exceptions. Downstream services, network outages, or memory limits can cause a function to fail. Write your functions so they can continue if a failure occurs.\n\nConsider not sharing storage accounts. Maximize performance by using a separate storage account for each function application. When you create a function app, associate it with a unique storage account. Using a unique storage account is important if your function generates a high volume of storage transactions.\n\n\nBusiness application\nTake a few minutes to read about other Azure Functions best practices.\n\nTip\nLearn more about Azure Functions in the Explore Azure Functions module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/8-design-for-azure-functions-solutions",
      "title": "Design for Azure Functions solutions - Training | Microsoft Learn",
      "description": "Design for Azure Functions solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/8-design-for-azure-functions-solutions",
      "load_timestamp": "2025-06-01T22:19:31.177863",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for Azure Logic Apps solutions - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for Azure Logic Apps solutions\n\n\nCompleted\n\n\n6 minutes\n\n\nAzure Logic Apps is another type of serverless compute solution that offers a cloud-based platform for creating and running automated workflows. Workflows are step-by-step processes that integrate your applications, data, services, and systems. With Azure Logic Apps, you can quickly develop highly scalable integration solutions for your enterprise and business-to-business (B2B) scenarios.\nThings to know about Azure Logic Apps\nLet's review some characteristics of Azure Logic Apps and scenarios for using the compute solution to build new workloads.\n\n\nAzure Logic Apps is a component of Azure Integration Services. Logic Apps simplifies the way you connect legacy, modern, and cutting-edge systems across cloud, on-premises, and hybrid environments.\n\nWith Logic Apps, you can schedule and send email notifications by using Office 365 when a specific event happens, such as a new file uploaded.\n\nUse Logic Apps to route and process customer orders across on-premises systems and cloud services.\n\nImplement Logic Apps to move uploaded files from an SFTP or FTP server to Azure Storage.\n\nMonitor tweets and analyze sentiment with Logic Apps, and create alerts or tasks for items that need review.\n\n\nCompare Azure Logic Apps and Azure Functions\nAzure Logic Apps is similar to Azure Functions as a compute service, but there are basic differences. Azure Functions is a code-first technology that uses durable functions. Azure Logic Apps is a design-first technology. Review the following flowchart and table to compare the two solutions.\n\n\nCompare\nAzure Functions\nAzure Logic Apps\n\n\nDevelopment\nCode-first\nDesign-first\n\n\nMethod\nWrite code and use the durable functions extension\nCreate orchestrations with a GUI or by editing configuration files\n\n\nConnectivity\n- Large selection of built-in binding types  - Write code for custom bindings\n- Large collection of connectors  - Enterprise Integration Pack for B2B scenarios  - Build custom connectors\n\n\nMonitoring\nAzure Application Insights\nAzure portal, Azure Monitor Logs (Log Analytics)\n\n\nThings to consider when using Azure Logic Apps\nThere are several points to consider when deciding whether Azure Logic Apps is the ideal compute solution for your infrastructure. Review the following considerations, and think about how Azure Logic Apps can enhance the compute strategy for Tailwind Traders.\n\nConsider integration. Use Logic Apps to provide the critical infrastructure component of integration with services. Logic Apps is a good option when you need to get multiple applications and systems to work together. If you're building an app with no external connections, Logic Apps is probably not the best option.\n\nConsider performance. Scale your apps automatically with the Logic Apps execution engine. Logic Apps can process large datasets in parallel to let you achieve high throughput. However, fast activation time isn't always guaranteed, nor enforcement of real-time constraints on execution time.\n\nConsider conditional expressions. Build highly complex and deeply nested conditionals into your Logic Apps. Logic Apps provides control constructs like Boolean expressions, switch statements, and loops so your apps can make decisions based on your data.\n\nConsider connectors. Investigate whether prebuilt connectors are available for all the services you need to access. You might need to create custom connectors. If a service has an existing REST or SOAP API, you can make the custom connector in a few hours without writing any code. Otherwise, you need to create the API first before making the connector.\n\nConsider mixing compute solutions. Take advantage of diverse features by mixing and matching services when you build an orchestration. You can call functions from Logic Apps, and call logic apps from an Azure function. Build each orchestration based on the service capabilities or your personal preference.\n\nConsider other options. Know when not to use Azure Logic Apps. There are cases where Logic Apps might not be the best option. Logic Apps isn't an ideal solution for real-time requirements, complex business rules, or if you're using nonstandard services.\n\n\nBusiness application\nTake a few minutes to learn how Azure Logic Apps distributes data from drones inspecting power lines.\nYou can use the following flowchart for other questions to ask as you plan for using Azure Logic Apps.\n\n\nTip\nContinue learning about Azure Functions in the Introduction to Azure Logic Apps module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/9-design-for-logic-app-solutions",
      "title": "Design for Azure Logic Apps solutions - Training | Microsoft Learn",
      "description": "Design for Azure Logic Apps solutions",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/9-design-for-logic-app-solutions",
      "load_timestamp": "2025-06-01T22:19:31.331017",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n4 minutes\n\n\nTailwind Traders has several active development projects. As the solution Architect for the company, you're responsible for selecting the right compute technology for each project. Ideally, you'd like to create compute resources and configure them to do the work and only pay for the services used.\nHere are the specific project requirements:\n\nReal-time inventory tracking. Each evening, product availability is updated on the company website. The management team wants the stock inventory updated as soon as products are ordered. To fulfill this requirement, the product database needs to be updated, and stock reorder notifications need to be sent, as needed. The current program is a Windows service written in C#.\n\nMigrate datacenter virtual machines. The datacenter virtual machines host relational database servers. These machines are used for online orders. The company needs a solution to move this capability to the cloud.\n\nHost data processing application in the cloud. The company has a small data processing application. The app ingests new product photos and writes the content to Azure Blob Storage. The app takes only a few seconds to run. The sales team needs a solution that hosts the app in the cloud and reduces costs.\n\n\nAnswer the following questions\nChoose the best response for each question.\n\n\n1.\nWhich compute option best supports the real-time inventory tracking requirement?\n\n\nAzure Logic Apps\n\n\nAzure Functions\n\n\nAzure Virtual Machines\n\n\n2.\nWhat type of virtual machine is best for the datacenter migration requirement?\n\n\nGeneral purpose VM\n\n\nCompute-optimized VM\n\n\nMemory-optimized VM\n\n\n3.\nWhat compute solution is best for hosting the company's data processing application?\n\n\nAzure Container Instances\n\n\nAzure Virtual Machines\n\n\nWeb Apps (supported by Azure App Service)\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/10-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/10-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:31.487003",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary and resources - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary and resources\n\n\nCompleted\n\n\n2 minutes\n\n\nIn this module, you learned how to choose and recommend an Azure compute solution. You reviewed scenarios for Azure Virtual Machines, Azure Logic Apps, Azure Functions, and Azure Container Instances. Azure Logic Apps and Azure Functions offer serverless compute options. You also looked at scenarios for supporting applications with Azure Batch, Azure App Service, and Azure Kubernetes Service. App Service lets you use the programming language of your choice. Azure Kubernetes Service makes it simple to deploy and manage containerized applications. You explored how to host applications in the cloud with Azure compute services to improve performance, scalability, and flexibility.\nLearn more with Copilot\nCopilot can assist you in designing Azure infrastructure solutions. Copilot can compare, recommend, explain, and research products and services where you need more information. Open a Microsoft Edge browser and choose Copilot (top right) or navigate to copilot.microsoft.com. Take a few minutes to try these prompts and extend your learning with Copilot.\n\nCompare and contrast how Azure virtual machines are different from Azure containers. Illustrate through examples when to select each product.\n\nHow can I simplify managing Azure containers? What orchestration products are available in Azure? Create a table of pros and cons.\n\nWhat Azure products are available to host a website? Rank the solutions based on ease of use.\n\nWhat is the difference between Azure Functions and Azure Logic Apps? Provide examples when to use each product.\n\n\nLearn more with Azure documentation\n\nChoose an Azure compute service.\n\nReview documentation for virtual machines in Azure.\n\nRead about Azure App Service.\n\nRead about Azure Batch.\n\nRead about Azure Container Instances.\n\nRead about Azure Functions.\n\nRead about Azure Logic Apps.\n\nDiscover what is Kubernetes.\n\nRead about the Azure Kubernetes Service.\n\nReview security considerations for Azure Container Instances.\n\n\nLearn more with self-paced training\n\nComplete the Describe Azure compute and networking services module.\n\nComplete the Introduction to Azure virtual machines module.\n\nComplete the Create an Azure Batch account by using the Azure portal module.\n\nComplete the Configure Azure App Service module.\n\nComplete the Configure Container Instance module.\n\nComplete the Introduction to Azure Kubernetes Service.\n\nComplete the Explore Azure Functions module.\n\nComplete the introduction to Azure Logic Apps module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/11-summary-resources",
      "title": "Summary and resources - Training | Microsoft Learn",
      "description": "Summary and resources",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-compute-solution/11-summary-resources",
      "load_timestamp": "2025-06-01T22:19:31.687558",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n2 minutes\n\n\nThe cloud is changing how applications are designed and secured. Instead of monoliths, applications are divided into smaller, decentralized services. These services communicate through APIs or by using asynchronous messaging or events. The services scale horizontally, adding new instances as demand requires.\nThese design changes bring new challenges. Application states are distributed, and operations are done in parallel and asynchronously. Applications must:\n\nCommunicate with each other effectively.\nBe able to be deployed rapidly.\nBe resilient when failures occur.\nBe able to integrate with other systems seamlessly.\n\nIn this module, we explore how Azure lets you create AI-ready applications composed of various components, including website front ends, back-end services, and triggered functions. We investigate how Azure includes various communication strategies to enable the components to pass data to each other.\nMeet Tailwind Traders\n\n\nTailwind Traders is a fictitious home improvement retailer. The company operates retail hardware stores across the globe and online.\nAs you work through this module, suppose you work for Tailwind Traders. You're tasked with evaluating and designing an effective application architecture for the company. The architecture should provide the best Azure solutions for exchanging messages. The strategy must help automate deployment solutions for the company applications, respond appropriately to events, and manage configurations. You're researching how Azure enables integration with APIs and provides appropriate caching.\nLearning objectives\nIn this module, you learn how to:\n\nDescribe message and event scenarios.\n\nDesign a messaging solution.\n\nDesign an Azure Event Hubs messaging solution.\n\nDesign an event-driven solution.\n\nDesign an automated app deployment solution.\n\nDesign an API integration solution.\n\nDesign an application configuration management solution.\n\nDesign a caching solution.\n\n\nSkills measured\nThe content in the module helps you prepare for Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions. The module concepts are covered in:\nDesign infrastructure solutions\n\nDesign an application architecture\n\nRecommend a messaging architecture.\n\nRecommend an event-driven architecture.\n\nRecommend a solution for API integration.\n\nRecommend a caching solution for applications.\n\nRecommend an application configuration management solution.\n\nRecommend an automated deployment solution for applications.\n\n\nPrerequisites\n\nWorking experience with developing cloud applications.\n\nConceptual knowledge of messaging, events, code deployments, configurations, API management, and app caching.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/1-introduction",
      "load_timestamp": "2025-06-01T22:19:31.893819",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Describe message and event scenarios - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDescribe message and event scenarios\n\n\nCompleted\n\n\n5 minutes\n\n\nYour first decision in your AI-ready application architecture design is to plan how the application components communicate. Defining your component strategy helps you choose the appropriate Azure service.\nSuppose you're designing the architecture for a home improvement video-sharing application for Tailwind Traders. You want your application to be as reliable and scalable as possible. You're planning to use Azure technologies to build a robust communication infrastructure. Before you can choose the appropriate Azure services, you need to design how each application component communicates with the other components. For each type of communication, you might choose a different Azure technology.\nThings to know about messages and events\nMost application components communicate by sending messages or events. Azure offers various services to support the different communication strategies.\nMessages\nLet's examine the characteristics of messages.\n\nMessages contain raw data produced by one component and consumed by another component.\n\nA message contains the data itself, not just a reference to that data.\n\n\nIn a message communication, the sending component expects the destination to process the data in a certain way. The integrity of the overall system might depend on both the sender and receiver doing a specific job.\nSuppose a user uploads a new video by using your mobile video-sharing app. Your mobile app must send the video to the web API that runs in Azure. The video file must be sent, not just an alert that indicates there's a new video. The mobile app expects that the web API stores the new video in the database and makes the video available to other users.\nEvents\nNow let's take a closer look at events.\nEvents are lighter weight than messages and are most often used for broadcast communications. An event-driven architecture consists of event producers that generate a stream of events, event consumers that listen for these events, and event channels that transfer events from producers to consumers.\n\n\nWith events, receiving components generally decide the communications in which they're interested and then subscribe to those events. An intermediary manages the subscription process. The intermediary uses services like Azure Event Grid or Azure Event Hubs. When publishers send an event, the intermediary routes that event to any interested parties. This pattern is known as a publish-subscribe architecture and is the most used.\nEvents have the following characteristics:\n\nAn event is a lightweight notification that indicates something occurred.\n\nAn event can be sent to multiple receivers or to none.\n\nAn event publisher has no expectations about actions by a receiving component.\n\nAn event is often intended to \"fan out\" or have many subscribers for each publisher.\n\nAn event is a discrete unit that's unrelated to other events, but an event might be part of a related and ordered series.\n\n\nThings to consider when choosing messages or events\nReview the following scenarios regarding when to choose message or event communication for your application architecture for Tailwind Traders.\n\nConsider messages and events. It's not uncommon for an application to implement both events and messages. An app can use events for some components and functions and messages for other components. Choose each Azure service to meet the specific needs of each component of your app.\n\nConsider sender expectations. If the sending component in your application expects the destination to process the component in a specific way, consider implementing messages. If the sender component in your application has no requirements for the destination component, you might implement events rather than messages.\n\nConsider guaranteed communication. If you're building a distributed application and want to guarantee all communication is processed, consider using messages. In a message communication, there's an expectation that both the message sender and receiver complete their tasks.\n\nConsider ephemeral communication. Ephemeral means the communication is dropped if there are no subscribing receivers. If your application doesn't require subscribers or actions from any receiver, consider using events.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/2-describe-message-event-scenarios",
      "title": "Describe message and event scenarios - Training | Microsoft Learn",
      "description": "Describe message and event scenarios",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/2-describe-message-event-scenarios",
      "load_timestamp": "2025-06-01T22:19:32.092245",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design a messaging solution - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign a messaging solution\n\n\nCompleted\n\n\n8 minutes\n\n\nAzure offers two message-based solutions, Azure Queue Storage and Azure Service Bus. Queue Storage stores large numbers of messages in Azure Storage. Service Bus is a message broker that decouples applications and services. We examine the different features and capabilities of these services and consider how to choose which service to implement.\nOne of your design tasks for Tailwind Traders is to recommend a design for their product demo application. Customers use the app to get the latest tips, reviews, and instructions for featured home improvement products. You have two requirements for the app design:\n\nEnsure all content files are uploaded to the web API reliably from the mobile app. Files include text, images, and video.\nDeliver details about new files directly to the app, such as when a customer posts a new product review or a video is added.\n\nFor these app requirements, the ideal solution is a message-based system. This video from the Developer course explains the different message queue solutions.\n\nThings to know about Azure Queue Storage\nAzure Queue Storage is a service that uses Azure Storage to store large numbers of messages. Examine the following characteristics of the service.\n\n\nQueues in Azure Queue Storage can contain millions of messages.\n\nThe number and size of queues is limited only by the capacity of the Azure storage account that owns the Queue Storage.\n\nMessages in Queue Storage can be securely accessed from anywhere in the world by using a simple REST-based interface.\n\nQueues generally provide increased reliability, guaranteed message delivery, and transactional support.\n\n\nThings to know about Azure Service Bus\nAzure Service Bus is a fully managed enterprise message broker. Service Bus is used to decouple applications and services from each other. Review the following benefits characteristics of the service.\n\nAzure Service Bus supports message queues and publish-subscribe topics.\n\nAzure Service Bus lets you load-balance work across competing workers.\n\nYou can use Service Bus to safely route and transfer data and control across service and application boundaries.\n\nService Bus helps you coordinate transactional work that requires a high degree of reliability.\n\n\nMessage queues\nAzure Service Bus message queues is a message broker system built on top of a dedicated messaging infrastructure. Like Azure queues, Service Bus holds messages until the target is ready to receive them.\n\n\nAzure Service Bus message queues are intended for enterprise applications, such as an app that uses communication protocols and different data contracts.\nBusiness scenario\nConsider the scenario where a customer is watching a video on an app. The app supports both user history and fan lists. You can support both actions by using publish-subscribe topic attributes:\n\nThe mobile app sends a message to the Watched topic.\n\nThe topic has two subscriptions. The first subscription completes the UpdateUserWatchHistory action. A second subscription completes the UpdateProductFanList action.\n\nEach subscription for the Watched topic receives its own copy of the message.\n\n\nThings to consider when choosing messaging services\nEach Azure messaging solution has a slightly different set of features and capabilities. You can choose one solution or use both to fulfill your design requirements. Review the following scenarios, and think about which messaging solutions can benefit the Tailwind Traders application architecture.\n\n\nMessaging solution\nExample scenarios\n\n\nAzure Queue Storage\nYou want a simple queue to organize messages. You need an audit trail of all messages that pass through the queue. The queue storage exceeds 80 GB. You'd like to track progress for processing a message inside of the queue.\n\n\nAzure Service Bus  message queues\nYou require an at-most-once delivery guarantee. You require at-least-once message processing (PeekLock receive mode). You require at-most-once message processing (ReceiveAndDelete receive mode). You want to group messages into transactions. You want to receive messages without polling the queue. You need to handle messages larger than 64 KB. The queue storage doesn't exceed 80 GB. You'd like to publish and consume batches of messages.\n\n\nAzure Service Bus  publish-subscribe topics\nYou need multiple receivers to handle each message. You expect multiple destinations for a single message but need queue-like behavior.\n\n\nTip\nContinue your learning with the Discover Azure message queues module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/3-design-messaging-solution",
      "title": "Design a messaging solution - Training | Microsoft Learn",
      "description": "Design a messaging solution",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/3-design-messaging-solution",
      "load_timestamp": "2025-06-01T22:19:32.276422",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design an Azure Event Hubs messaging solution - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign an Azure Event Hubs messaging solution\n\n\nCompleted\n\n\n8 minutes\n\n\nCertain applications produce a massive number of events from almost as many sources. These application scenarios are often referred to as Big Data. Big Data can require extensive infrastructure.\nSuppose you're designing the architecture for a Tailwind Traders home security monitoring application. Each security system has a dozen or more cameras sensors. Before the house can be deemed secure, the sensors and cameras are connected to a test harness and put through their paces. Additionally, cached video camera footage data is streamed when the security system is connected to the datacenter monitoring headquarters.\nFor this architecture, you might choose a messaging solution that uses event hubs. Event hubs can receive and process millions of events per second. Data sent to an event hub can be transformed in real time and stored for later analysis. This on-demand video from the Developer course provides a nice overview of Event Hubs.\n\nThings to know about Azure Event Hubs\nAzure Event Hubs is a fully managed, big data streaming platform and event ingestion service. Let's review the characteristics of the service:\n\nAzure Event Hubs supports real time data ingestion and microservices batching on the same stream.\n\nYou can send and receive events in many different languages. Messages can also be received from Azure Event Hubs by using Apache Storm.\n\nEvents received by Azure Event Hubs are added to the end of its data stream.\n\nThe data stream orders events according to the time the event is received.\nConsumers can seek along the data stream by using time offsets.\n\n\nEvent Hubs implements a pull model that differentiates it from other messaging services like Azure Service Bus queues.\n\nEvent Hubs holds each message in its cache and allows it to be read.\nMessages remain for other consumers.\n\n\nEvent Hubs doesn't have a built-in mechanism to handle messages that aren't processed as expected.\n\nAzure Event Hubs scales according to the number of purchased throughput (processing) units. Performance features vary for each pricing tier, such as Basic, Standard, or Premium.\n\n\nBusiness scenario\nLet's examine how Azure Event Hubs and other Azure services can contribute to the architecture for the home security monitoring application.\n\n\nAzure Event Hubs captures streaming video camera footage data from the camera and sensor testing equipment.\n\nAzure Blob Storage stores the video and sensor test data.\n\nAzure Stream Analytics identifies patterns in the video and sensor test data.\n\nPower BI makes decisions for monitoring alerts and improving security based on the test data patterns.\n\n\nThings to consider when using Azure Event Hubs\nAs you plan for how Azure Event Hubs can be a part of your messaging solution, consider the following points.\n\nConsider common implementations. Identify whether your application scenario is suited for event-hubs messaging. There are several common scenarios where Azure Event Hubs is a great messaging solution. Event Hubs is ideal for live dashboarding, supporting analytics pipelines like clickstreams, and detecting anomalies like fraud or outlier actions. Event hubs are also a good solution for processing transactions with real-time analysis and archiving data.\n\nConsider language and framework integration. Azure Event Hubs supports sending and receiving events in many different languages. The robust language and framework support makes it easy to integrate Event Hubs with other Azure and non-Azure services.\n\nConsider pricing tier and throughput units. Choose the pricing tier that offers the features and capabilities required by your application. Control how your Azure Event Hubs implementation scales by purchasing the necessary throughput or processing units. A single throughput unit equates to:\n\nIngress: Up to 1 MB per second or 1,000 events per second (whichever comes first)\nEgress: Up to 2 MB per second or 4,096 events per second\n\n\nConsider pull model benefits. Investigate how the pull model implemented by Event Hubs can benefit your application communication. Event Hubs holds a message in its cache and allows it to be read. When a message is read, it isn't deleted. The message remains for other consumers.\n\nConsider message failures. Remember Azure Event Hubs doesn't handle messages that aren't processed as expected. Suppose a message consumer malfunctions because of data format. Event Hubs won't detect this issue. The message remains until its time-to-live setting expires.\n\nConsider data stream access. Event Hubs adds received events to the end of its data stream, and the events are ordered according to the time they're received. Event consumers can seek along the data stream by using time offsets.\n\n\nTip\nLearn more with the Explore Azure Event Hubs module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/4-design-event-hub-messaging-solution",
      "title": "Design an Azure Event Hubs messaging solution - Training | Microsoft Learn",
      "description": "Design an Azure Event Hubs messaging solution",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/4-design-event-hub-messaging-solution",
      "load_timestamp": "2025-06-01T22:19:32.525719",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design an event-driven solution - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign an event-driven solution\n\n\nCompleted\n\n\n7 minutes\n\n\nAn event-driven architecture enables you to connect to the core application without needing to modify the existing code. When an event occurs, you can react with specific code to respond to the event. An event-driven application uses the send and forget principle. An event is sent toward the next system, which can be another service, an event hub, a stream, or a message broker.\nLet's reconsider our design for the Tailwind Traders product demo application, and examine how to use a Web API that runs in Azure. When a new product review or demo video is uploaded, we need to notify all mobile apps on user devices around the world that are interested in the products. Azure Event Grid is an ideal solution for this requirement.\n\nThe publisher of the review or video doesn't need to know about any subscribers who are interested in the affected products.\nWe want to have a one-to-many relationship where we can have multiple subscribers. Subscribers can optionally decide whether they're interested in the affected products.\n\nThings to know about Azure Event Grid\nAzure Event Grid is a fully managed event routing service that runs on Azure Service Fabric. Event Grid exists to make it easier to build event-based and serverless applications on Azure.\n\nReview the following characteristics of the service.\n\nAzure Event Grid aggregates all your events and provides routing from any source to any destination.\n\nEvent Grid distributes events from sources like Azure Blob Storage accounts.\n\nEvents are distributed to handlers like Azure functions and webhooks.\n\nThe service manages the routing and delivery of events from many sources. The management helps to minimize cost and latency by eliminating the need for polling.\n\n\nHow Azure Event Grid works\nThe following illustration shows how Azure Event Grid manages the event process from multiple event sources to multiple event handlers.\n\n\nAn event source such as Azure Blob Storage tags events with one or more topics, and sends events to Azure Event Grid.\n\nAn event handler such as Azure Functions subscribes to topics they're interested in.\n\nEvent Grid examines topic tags to decide which events to send to which handlers.\n\nEvent Grid forwards relevant events to subscribers.\n\nEvent Grid reacts when an event happens. However, the actual object that was changed (text file, video, audio, and so on) isn't part of the event data. Instead, Event Grid passes a URL or identifier to reference the changed object.\n\n\nThings to consider when using Azure Event Grid\nAzure Event Grid can be an ideal solution for an event-driven application architecture. As you review the following considerations, think about how Event Grid can benefit the Tailwind Traders application architecture.\n\nConsider multiple services. Choose one or multiple Azure services to fulfill your design requirements.\n\n\nAzure service\nPurpose\nMessage or Event\nUsage scenario\n\n\nAzure Event Grid\nReactive programming\nEvent distribution (discrete)\nReact to status changes\n\n\nAzure Event Hubs\nBig data pipeline\nEvent streaming (series)\nConduct telemetry and distributed data streaming\n\n\nAzure Service Bus\nHigh-value enterprise messaging\nMessage\nFulfill order processing and financial transactions\n\n\nConsider distinct roles for services. Investigate using Azure services side by side to fulfill distinct roles. An e-commerce site can use Azure Service Bus to process an order, Azure Event Hubs to capture site telemetry, and Azure Event Grid to respond to events like an item being shipped.\n\nConsider linking services. Link Azure services together to form an event and data pipeline stream. In this scenario, Azure Event Grid responds to events in other services. The following illustration demonstrates how several Azure services can be linked together as an event and data pipeline to stream data.\n\n\nTip\nConsider reviewing the Explore Azure Event Grid module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/5-design-event-driven-solution",
      "title": "Design an event-driven solution - Training | Microsoft Learn",
      "description": "Design an event-driven solution",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/5-design-event-driven-solution",
      "load_timestamp": "2025-06-01T22:19:32.680783",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design a caching solution - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign a caching solution\n\n\nCompleted\n\n\n7 minutes\n\n\nCaching is a common technique that aims to improve the performance and scalability of a system. Caching temporarily copies frequently accessed data to fast storage located close to the application. When the fast data storage is located closer to an application than its original data store, caching can significantly improve response times for client applications by serving data more quickly.\nCaching is most effective when a client instance repeatedly reads the same data, especially when the following conditions apply to the original data store:\n\nThe original data store remains relatively static.\nIt's subject to a high level of contention.\nIt's far away, and network latency can result in slow access to the store.\n\nSuppose Tailwind Traders is adding a new feature to the product demo application to increase customer traffic to their retail website. The event feature adds a banner to the top of the mobile app to announce special offers and limited product discounts. New offers are posted on the hour, and the remaining product availability for each offer is updated after every order is processed. The first customer to respond to a new offer receives a double discount! Customers are encouraged to check their mobile app frequently for updates to the offers and product availability. To implement this new feature, you need to design a caching solution that can support in-memory fast read and writes.\nThings to know about Azure Cache for Redis\nAzure Cache for Redis provides an in-memory data store based on the Redis software. Redis improves the performance and scalability of an application that uses back-end data stores heavily. It's able to process large volumes of application requests by keeping frequently accessed data in the server memory, which can be written to and read from quickly. Redis brings a critical low-latency and high-throughput data storage solution to modern applications.\nLet's review the characteristics of the service:\n\nAzure Cache for Redis offers two implementation options for developers:\n\nThe Redis open source (OSS Redis)\nA commercial product from Redis Labs (Redis Enterprise) as a managed service\n\n\nAzure Cache for Redis provides secure and dedicated Redis server instances and full Redis API compatibility.\n\nYou can use Azure Cache for Redis as a distributed data or content cache, session store, or message broker.\n\nDeploy Azure Cache for Redis as a standalone or with other Azure database services, such as Azure SQL or Azure Cosmos DB.\n\n\nHow Azure Cache for Redis works\nAzure Cache for Redis is hosted on Azure, and usable by any application within or outside of Azure. As indicated in the following graphic, Azure Cache for Redis can help improve performance in apps that interface with many database solutions, including Azure SQL Database, Azure Cosmos DB, and Azure Database for MySQL.\n\n\nThings to consider when using Azure Cache for Redis\nAzure Cache for Redis improves application performance by supporting common application architecture patterns. As you review the following patterns, consider patterns that might be exhibited in the Tailwind Traders application architecture. Think about how Azure Cache for Redis can supply the pattern requirements.\n\n\nPattern\nScenario\nSolution\n\n\nData cache\nDatabases are often too large to load directly into a cache.\nIt's common to use the cache-aside pattern to only load data into the cache as needed. When the system makes changes to the data, the system can also update the cache, which is then distributed to other clients. Additionally, the system can set an expiration on data, or use an eviction policy to trigger data updates into the cache.\n\n\nContent cache\nMany web pages are generated from templates that use static content such as headers, footers, banners. These static items shouldn't change often.\nUsing an in-memory cache provides quick access to static content compared to back-end datastores. This pattern reduces processing time and server load and allows web servers to be more responsive. A content cache can allow you to reduce the number of servers needed to handle loads. Azure Cache for Redis provides the Redis Output Cache Provider to support this pattern with ASP.NET.\n\n\nSession store\nA session store is commonly used with shopping carts and other user history data that a web application might associate with user cookies. Storing too much in a cookie can have a negative effect on performance as the cookie size grows and is passed and validated with every request.\nA typical solution uses the cookie as a key to query the data in a database. It's faster to use an in-memory cache like Azure Cache for Redis to associate information with a user than interacting with a full relational database.\n\n\nJob and message queuing\nSome application operations take significant time to complete, which might prevent other unrelated jobs or messages from starting.\nApplications often add tasks to a queue when the operations associated with the request take time to execute. Longer running operations are queued to be processed in sequence, often by another server. This method of deferring work is called task queuing. Azure Cache for Redis provides a distributed queue to enable this pattern in your application.\n\n\nDistributed transactions\nApplications sometimes require a series of commands against a back-end datastore to execute as a single atomic operation. All commands must succeed, or all commands must be rolled back to the initial state.\nAzure Cache for Redis supports executing a batch of commands as a single transaction.\n\n\nTip\nThere's more to learn in the Introduction to Azure Cache for Redis module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/6-design-caching-solution",
      "title": "Design a caching solution - Training | Microsoft Learn",
      "description": "Design a caching solution",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/6-design-caching-solution",
      "load_timestamp": "2025-06-01T22:19:32.954249",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design API integration - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign API integration\n\n\nCompleted\n\n\n6 minutes\n\n\nPublishing an API is a great way to increase market share, generate revenue, and foster innovation. However, maintaining even one API brings significant challenges, such as onboarding users, managing revisions, and implementing security.\nDevelopers need a way to reduce the complexity involved in supporting numerous APIs and their management. They require an API Management technology that can serve as a front door for all their APIs. They need tools to implement security, manage revisions, and perform analytics.\nSuppose Tailwind Traders started a new service for premier subscribers to request home delivery, product assembly, and in person assistance for promoted products. The service is available in the mobile app and the online retail website. Customers browse the list of promoted products, and place their order. Tailwind Traders connects with contracted specialists who deliver and assemble the product, and provide the customer with hands on instruction.\nThe backbone of the new service is a large collection of published APIs, some of which are used by the following entities:\n\nTailwind Traders mobile app and online website\nIoT devices on the delivery vehicles\nVendor product specialists\nTailwind Traders in-house development teams\nTailwind Traders employees, such as business analysts\n\nEach published API resides on a different server. Each API has its own process for onboarding users, and its own policies for security, revisions, analytics, and more. You're looking for an Azure solution that can help reduce this complexity.\nThings to know about Azure API Management\nAzure API Management is a cloud service platform that lets you publish, secure, maintain, and analyze all your APIs. The following diagram shows how Azure API Management serves as a front door for an organization's APIs, and routes to the server where the APIs are deployed.\n\n\nImportant\nAzure API Management doesn't host your actual APIs. Your APIs remain where they were originally deployed.\nAzure API Management serves as a front door for your APIs. In this way, Azure API Management is said to decouple your APIs.\nYou set API policies and other management options in Azure, while leaving your deployed back-end APIs untouched.\n\nThings to consider when using Azure API Management\nTo determine whether Azure API Management is a suitable choice for managing and publishing your organization's inventory of APIs, there are three essential criteria to consider: number of APIs, rate of API changes, and API administration load. If you have a large number of deployed APIs that you revise frequently and require significant administrative overhead, Azure API Management is a strong solution. But for scenarios that involve small, static, or simple API deployments, Azure API Management might not be the correct choice.\nReview the following criteria, and think about what APIs and API management are required to support the Tailwind Traders applications.\n\nConsider number of APIs. Identify how many APIs you need to manage. The more APIs you deploy, the greater the need for deployment standardization, and centralization of API control.\n\nConsider rate of API changes. Determine the rate at which your organization plans to implement API revisions and versions. The faster you create API revisions and publish new API versions, the greater the need for a robust, and flexible versioning control system.\n\nConsider API administration load. Define how much policy overhead you need to apply to your APIs. Policies include usage quotas, call rate limits, request transformations, and request validation. The more configurations and options your APIs require, the greater the need for standardized, and centralized policy implementations.\n\nConsider standardizing disparate APIs. Use an API management solution to standardize API specs, generate documentation, and create a consistent base URL for ease of use. Azure API Management can provide consistent analytics across multiple APIs and ensure compliance across all APIs.\n\nConsider centralized API management. Bring multiple APIs under a single administrative umbrella with Azure API Management and centralize all API operations. Without an API management service, each API is on its own in terms of administration, deployment, and developer access. A centralized model results in less duplicated effort and increases efficiency.\n\nConsider enhanced API security. Azure API Management was designed with API security in mind. Use the service to manage permissions and access, and protect your APIs from malicious usage.  Azure API Management helps to achieve all corporate and government-related compliance.\n\n\nTip\nThere's a lot more to learn in the Explore API Management module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/7-design-api-integration",
      "title": "Design API integration - Training | Microsoft Learn",
      "description": "Design API integration",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/7-design-api-integration",
      "load_timestamp": "2025-06-01T22:19:33.148036",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design an automated app deployment solution - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign an automated app deployment solution\n\n\nCompleted\n\n\n6 minutes\n\n\nWith the move to the cloud, many teams adopt agile development methods. These teams must iterate quickly and repeatedly deploy their solutions to the cloud. Teams must be assured their infrastructure is in a reliable state. Application code must be managed through a unified process.\nTo meet these challenges in your design for Tailwind Traders, you're investigating how to automate deployments by using the practice of infrastructure as code. Let's explore two Azure solutions for deployment and automation of your applications: Azure Resource Manager templates and Azure Automation.\nThings to know about Azure Resource Manager templates\nAzure Resource Manager (ARM) templates are files that define the infrastructure and configuration for your deployment. When you write an ARM template, you take a declarative approach to your resource provisioning. ARM templates describe each resource in the deployment, but they don't describe how to deploy the resources.\nThere are many benefits to using ARM templates for your resource provisioning. As you review the benefits, think about how ARM templates can be used in the Tailwind Traders architecture solution.\n\nARM templates are idempotent, which means you can repeatedly deploy the same template and get the same result.\n\nWhen an ARM template deployment is submitted to Azure Resource Manager, the resources in the ARM template are deployed in parallel. This orchestration feature process allows deployments to finish faster.\n\nThe WhatIf parameter available in PowerShell and the Azure CLI allows you to preview changes to your environment before deploying the ARM template. This parameter details any creations, modifications, and deletions made by the template.\n\nARM templates submitted to Resource Manager are validated before the deployment process. This validation alerts you to any errors in your template before resource provisioning.\n\nYou can break up your ARM templates into smaller components and link them together at deployment.\n\nYour ARM templates can be integrated into multiple CI/CD tools like Azure Pipelines and GitHub Actions.\n\nWith deployment scripts, you can run Bash or PowerShell scripts from within your ARM templates. Through extensibility, you can use a single template to deploy a complete solution.\n\n\nARM template formats\nThere are two formats available for ARM templates and Azure resource deployments, JSON and Bicep. JavaScript Object Notation (JSON) is an open-standard file format that multiple languages can use. Bicep is a domain-specific language that was recently developed for authoring templates by using an easier syntax. You can use the Bicep CLI to decompile any JSON template into a Bicep template.\nThings to know about Azure Bicep templates\nBicep is an ARM template language that's used to declaratively deploy Azure resources. Bicep is a domain-specific language designed for a specific scenario or domain. Bicep is used to create ARM templates.\n\n\nThere are many reasons to choose Bicep as the main tool set for your infrastructure as code deployments.\n\nBicep is native to the Azure ecosystem. When new Azure resources are released or updated, Bicep supports those features on day one.\n\nJSON and Bicep templates are fully integrated within the Azure platform. With Resource Manager-based deployments, you can monitor the progress of your deployment in the Azure portal.\n\nBicep is a fully supported product with Microsoft Support.\n\nAll state is stored in Azure. Users can collaborate and have confidence their updates are handled as expected.\n\nIf you're already using JSON templates as your declarative template language, it isn't difficult to transition to Bicep. You can use the Bicep CLI to decompile any template into a Bicep template.\n\n\nThings to know about Azure Automation\nAzure Automation delivers a cloud-based automation and configuration service that supports consistent management across your Azure and non-Azure environments. Azure Automation gives you complete control in three service areas: process automation, configuration management, and update management. Let's examine the details of this service, and consider how it can be implemented in the Tailwind Traders application architecture.\n\n\nService\nDescription\n\n\nProcess automation\nProcess automation enables you to automate frequent, time-consuming, and error-prone cloud management tasks. This service helps you focus on work that adds business value. By reducing errors and boosting efficiency, it also helps to lower your operational costs. The service allows you to author runbooks graphically in PowerShell or by using Python.\n\n\nConfiguration management\nConfiguration management enables access to two features, Change Tracking and Inventory and Azure Automation State Configuration. The service supports change tracking across services, daemons, software, registry, and files in your environment. The change tracking helps you diagnose unwanted changes and raise alerts.\n\n\nUpdate management\nThe update management service includes the Update Management feature for Windows and Linux systems across hybrid environments. The feature allows you to create scheduled deployments that orchestrate the installation of updates within a defined maintenance window.\n\n\nTip\nContinue your learning in the Deploy Azure infrastructure by using JSON ARM templates module.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/8-design-automated-app-deployment-solution",
      "title": "Design an automated app deployment solution - Training | Microsoft Learn",
      "description": "Design an automated app deployment solution",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/8-design-automated-app-deployment-solution",
      "load_timestamp": "2025-06-01T22:19:33.606342",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design an app configuration management solution - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign an app configuration management solution\n\n\nCompleted\n\n\n3 minutes\n\n\nTraditionally, shipping a new application feature requires a complete redeployment of the application itself. Testing or deployment of a feature often requires multiple versions of the application. Each deployment might require different configurations, credentials, changing settings or parameters for testing.\nConfiguration management is a modern software-development practice that decouples configuration from code deployment and enables quick changes to feature availability on demand. Decoupling configuration as a service enables systems to dynamically administer the deployment lifecycle.\nLet's examine an Azure solution that can help you focus on deployment issues.\nThings to know about Azure App Configuration\nAzure App Configuration provides a service to centrally manage application settings and feature flags. You can use App Configuration to store all the settings for your application and secure their accesses in one place.\nApp Configuration offers many benefits for an application architecture. As you review the features, consider how Azure App Configuration can support deployment for the Tailwind Traders applications.\n\nAzure App Configuration is a fully managed service that can be set up in minutes, and supports native integration with popular frameworks.\n\nApp Configuration offers flexible key representations and mappings, and point-in-time replay of settings.\n\nApp Configuration has a dedicated UI for feature flag management, and supports resource tagging with labels.\n\nYou can compare two sets of configurations on custom-defined dimensions.\n\nApp Configuration provides enhanced security through Microsoft Entra managed identities for Azure resources.\n\nSensitive information can be encrypted at rest and in transit.\n\nAzure App Configuration works in both development and production environments.\n\n\nDevelopment\nAn Azure App Configuration development environment consists of Visual Studio, Visual Studio Code, and the Azure CLI. These components are linked to Microsoft Entra ID, App Configuration, and Azure Key Vault.\n\n\nProduction\nAn Azure App Configuration production environment consists of Azure and Microsoft Entra managed identities for Azure resources with related Azure services. These components are linked to Microsoft Entra ID, App Configuration, and Key Vault.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/9-configuration-management-solution",
      "title": "Design an app configuration management solution - Training | Microsoft Learn",
      "description": "Design an app configuration management solution",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/9-configuration-management-solution",
      "load_timestamp": "2025-06-01T22:19:33.792106",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n4 minutes\n\n\nTailwind Traders is advancing work on its customer rewards game. With each purchase, customers are offered a chance to win cash or prizes. You're asked to provide input on engineering requirements to enable the game features.\n\nTransaction processing. When a customer makes a purchase, the order details should be grouped into a single transaction in the game.\n\nUpdate management. The developers of the game expect scheduled software deployments and installation of updates within a defined maintenance window.\n\nEvent handling. During game play, millions of events are expected per second. Customers should expect a low latency on responses. The stream should be saved to Azure Blob Storage.\n\n\nAnswer the following questions\nChoose the best response for each question.\n\n\n1.\nWhich Azure solution can supply transaction processing for the game?\n\n\nAzure Queue Storage queues\n\n\nAzure Service Bus queues\n\n\nAzure Service Bus topics\n\n\n2.\nWhat option do you recommend for the software requirements?\n\n\nAzure Resource Manager (ARM) templates\n\n\nAzure Bicep\n\n\nAzure Automation\n\n\n3.\nHow should the events be handled in the game?\n\n\nAzure Event Hubs\n\n\nAzure Event Grid\n\n\nAzure IoT Hub\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/10-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/10-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:33.993460",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary and resources - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary and resources\n\n\nCompleted\n\n\n3 minutes\n\n\nIn this module, you learned how to evaluate and design an effective AI-ready application architecture. You explored Azure solutions for exchanging messages, automating application deployment, and managing application configuration. You reviewed how to integrate Azure services with your applications by using APIs, and how to provide appropriate caching. You discovered how to use Azure services to create a scalable, efficient, event-driven solution.\nLearn more with Copilot\nCopilot can assist you in designing Azure infrastructure solutions. Copilot can compare, recommend, explain, and research products and services where you need more information. Open a Microsoft Edge browser and choose Copilot (top right) or navigate to copilot.microsoft.com. Take a few minutes to try these prompts and extend your learning with Copilot.\n\nWhat is the difference between Azure messaging architecture and an Azure event-driven messaging architecture? Provide some examples.\n\nI’m looking for a detailed comparison between Azure’s messaging and event-driven messaging architectures. Could you explain the key differences and provide two real-world examples for each, focusing on scenarios where they would be most effectively applied?\n\nTabulate the differences, pros, and cons between Azure Queues, Azure Service Bus queues, and Azure Service Bus topics.\n\nCompare and contrast the pros and cons of Azure Event Hubs and Azure Event Grid. Provide some examples.\n\n\nLearn more with Azure documentation\n\nRead about Azure Queue Storage.\n\nRead about Azure Service Bus.\n\nRead about Azure Event Grid.\n\nDiscover Azure Resource Manager (ARM) templates.\n\nDiscover Azure Automation.\n\nDiscover Azure App Configuration.\n\nDiscover Azure API Management.\n\nDiscover Azure Cache for Redis.\n\n\nLearn more with self-paced training\n\nComplete an introduction to Azure API Management.\n\nComplete an introduction to Azure Event Hubs.\n\nComplete an introduction to infrastructure as code by using Bicep.\n\nComplete an introduction to Azure Cache for Redis.\n\nExplore message queues and stream processing.\n\n\nLearn more with optional hands-on exercises\n\nPractice creating an Azure Service Bus queue and topic (sandbox).\n\nDeploy Azure infrastructure by using JSON ARM templates (sandbox).",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/11-summary-resources",
      "title": "Summary and resources - Training | Microsoft Learn",
      "description": "Summary and resources",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-application-architecture/11-summary-resources",
      "load_timestamp": "2025-06-01T22:19:34.188328",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n2 minutes\n\n\nAzure Architects need to design and recommend AI-ready network solutions. Global organizations are seeking cloud-based solutions to improve their business and foster growth into new markets. They need solutions that can offer robust and reliable user experiences in a global environment.\nMeet Tailwind Traders\n\n\nTailwind Traders is a fictitious home improvement retailer. It operates retail hardware stores across the globe and online. Tailwind Traders specializes in competitive pricing, fast shipping, and a large range of items. The company is looking at cloud technologies to improve their business operations and enable growth into new markets. The company hopes that by moving to the cloud, it can enhance its shopping experience to further differentiate itself from competitors.\nAs a member of the enterprise IT team, you're helping to define the strategy to migrate some company workloads to Azure. You need to identify the required networking components and design the network infrastructure. Tailwind Traders operations are global, so they expect to use multiple Azure regions to host its applications. Most of these applications have dependencies on infrastructure and data services, which should also reside in Azure. Internal applications migrated to Azure must remain accessible to Tailwind Traders users. Internet-facing applications migrated to Azure must remain accessible to any external customer.\nLearning objectives\nIn this module, you learn how to:\n\nRecommend an AI-ready network architecture solution based on workload requirements.\n\nDesign for on-premises connectivity to Azure Virtual Network.\n\nDesign for Azure network connectivity services.\n\nDesign for application delivery services.\n\nDesign for application protection services.\n\n\nSkills measured\nThe content in the module helps you prepare for Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions. The module concepts are covered in:\nDesign infrastructure solutions\n\nDesign network solutions\n\nRecommend a connectivity solution that connects Azure resources to the internet.\n\nRecommend a connectivity solution that connects Azure resources to on-premises networks.\n\nRecommend a solution to optimize network performance.\n\nRecommend a solution to optimize network security.\n\nRecommend a load-balancing and routing solution.\n\n\nPrerequisites\n\nWorking experience with enterprise networking.\n\nConceptual knowledge of software defined networking and hybrid connectivity.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/1-introduction",
      "load_timestamp": "2025-06-01T22:19:34.824232",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Recommend a network architecture solution based on workload requirements - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nRecommend a network architecture solution based on workload requirements\n\n\nCompleted\n\n\n9 minutes\n\n\nAzure offers several AI-ready networking services with capabilities that can be used together or separately to support workload requirements. You can connect Azure resources and on-premises resources, protect your applications, and deliver your applications.\nTailwind Traders is currently running its workloads on-premises in its datacenter. In your strategy for migrating some of the company workloads to Azure, you need to make networking design decisions to properly support the workloads and services. In order to recommend a network architecture, you need to consider how workload requirements translate to network requirements.\n\nThings to know about network requirements\nAs you plan your networking solution, there are several requirements you need to consider.\n\nNaming: Define a naming convention that you can use consistently when naming resources to make it easier to manage several network resources over time.\n\nRegions: Determine the Azure regions for your resources according to the physical locations of the consumers of your resources. A virtual network is scoped to a single region/location. However, multiple virtual networks from different regions can be connected together by using Virtual Network peering.\n\nSubscriptions: Plan out how many Azure subscriptions are required to meet your workload requirements, considering you can implement multiple virtual networks within each Azure subscription and Azure region.\n\nIP addresses: Specify a custom private IP address space by using public and private (RFC 1918) addresses. Azure assigns resources in a virtual network a private IP address from the address space that you assign.\n\nSegmentation: Segment your virtual networks with subnets based on workload and security requirements.\n\nFiltering: Define your network security and traffic filtering strategy for your workloads.\n\n\nThings to consider when defining workload requirements\nThere are many considerations to review as you plan your network according to the workload requirements for Tailwind Traders.\n\nConsider segmentation options for your virtual networks. Each subnet must have a unique address range, specified in CIDR format, within the address space of the virtual network. The address range can't overlap with other subnets in the virtual network. The following table shows how to segment a virtual network with an address space of 10.245.16.0/20 into subnets based on a three-tiered application.\n\n\nSubnet\nCIDR\nAddresses\nUsage\n\n\nDEV-FE-EUS2\n10.245.16.0/22\n1019\nFront-end or web-tier virtual machines\n\n\nDEV-APP-EUS2\n10.245.20.0/22\n1019\nApplication-tier virtual machines\n\n\nDEV-DB-EUS2\n10.245.24.0/23\n507\nDatabase virtual machines\n\n\nConsider required interfaces and IP addresses. Identify how many network interfaces and private IP addresses you require in your virtual network. There are limits to the number of network interfaces and private IP addresses that you can have within a virtual network.\n\nConsider network security groups. You can filter network traffic to and from resources in a virtual network by using network security groups and network virtual appliances. You can control how Azure routes traffic from subnets.\n\nConsider network traffic routing. Azure routes network traffic between all subnets in a virtual network, by default. You can override some of Azure's system routes with custom routes.\n\n\nBest practices\nReview the following recommended best practices for working with virtual networks and subnets.\nPlan IP addressing for virtual networks\nWhen you create virtual networks as part of your migration, plan out your virtual network IP address space. Virtual networks allow for the use of 65,536 IP addresses.\n\nAssign an address space that isn't larger than a CIDR range of /16 for each virtual network. If you assign a smaller prefix than /16, such as a /15, which has 131,072 addresses, the excess IP addresses become unusable elsewhere. It's important not to waste IP addresses, even if they're in the private ranges defined by RFC 1918.\n\nDon't overlap the virtual network address space with on-premises network ranges. Overlapping addresses can cause networks that can't be connected, and routing that doesn't work properly.\n\nIf networks overlap, you need to redesign the network. If you absolutely can't redesign the network, network address translation (NAT) can help, but should be avoided or limited as much as possible.\n\n\nImplement hub-spoke network topology\nA hub and spoke network topology isolates workloads while sharing services, such as identity and security. The hub is an Azure virtual network that acts as a central point of connectivity. The spokes are virtual networks that connect to the hub virtual network by using peering. Shared services are deployed in the hub, while individual workloads are deployed as spokes.\n\n\nImplement a hub-spoke topology in Azure to centralize common services, such as connections to on-premises networks, firewalls, and isolation between virtual networks. The hub virtual network provides a central point of connectivity to on-premises networks, and a place to host services used by workloads hosted in spoke virtual networks.\n\nUse spoke virtual networks to isolate workloads with each spoke managed separately from other spokes. Each workload can include multiple tiers, and multiple subnets that are connected with Azure load balancers.\n\nConfigure hub and spoke virtual networks in different resource groups, and even in different subscriptions. When you peer virtual networks in different subscriptions, the subscriptions can be associated to the same, or different, Microsoft Entra ID tenants. You gain decentralized management of each workload, while sharing services maintained in the hub network.\n\n\nCheck your knowledge\n\n\n1.\nWhat is a best practice for designing IP addressing for virtual networks?\n\n\nAssign an address space larger than a CIDR range of /16 for each virtual network.\n\n\nOverlap the virtual network address space with on-premises network ranges.\n\n\nUse a hub and spoke network topology to centralize common services.\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/2-recommend-network-architecture-solution-based-workload-requirements",
      "title": "Recommend a network architecture solution based on workload requirements - Training | Microsoft Learn",
      "description": "Recommend a network architecture solution based on workload requirements",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/2-recommend-network-architecture-solution-based-workload-requirements",
      "load_timestamp": "2025-06-01T22:19:35.092928",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design patterns for Azure network connectivity services - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign patterns for Azure network connectivity services\n\n\nCompleted\n\n\n7 minutes\n\n\nNow we examine three common networking patterns for organizing workloads in Azure:\n\nSingle virtual network.\nMultiple virtual networks with peering.\nMultiple virtual networks in a hub-spoke topology.\n\n\nEach pattern provides a different type of isolation and connectivity. As you review these options, consider which pattern can meet the needs of the Tailwind Traders organization.\nPattern 1: Single virtual network\nIn the first pattern, all components of your workload (or in some cases your entire IT footprint) are placed in a single virtual network. This option is possible if you're operating solely in a single region because a virtual network can't span multiple regions.\nThe entities you would most likely use to create segments in the virtual network are network security groups (NSGs). You could also use application security groups (ASGs) to simplify administration.\n\n\nHere's how you might implement a single virtual network pattern:\n\nOne subnet (Subnet 1) can contain your database workloads.\nAnother subnet (Subnet 2) can contain your web workloads.\nTo govern subnet traffic, you can implement NSGs to specify that Subnet 1 can talk only with Subnet 2, and Subnet 2 can talk to the internet.\nYou can enforce segmentation by using an NVA from Azure Marketplace or Azure Firewall.\nYou can modify the pattern to segment and support many different workloads.\n\nPattern 2: Multiple virtual networks with peering\nThe second pattern extends the single virtual network pattern to support multiple virtual networks with potential peering connections. This option lets you group applications into separate virtual networks, or implement a presence in multiple Azure regions.\n\n\nThis pattern provides built-in segmentation through virtual networks because you must explicitly peer one virtual network to another for them to communicate. (Keep in mind that Azure Virtual Network peering connectivity isn't transitive.) You can add more segmentations within a virtual network in a manner similar to pattern 1 by using NSGs in the virtual networks.\nPattern 3: Multiple virtual networks in hub-spoke topology\nThe third pattern is a more advanced virtual network organization, where you choose a virtual network in a given region as the hub for all the other virtual networks in that region.\n\n\nThe connectivity between the hub virtual network and its spoke virtual networks is achieved by using Virtual Network peering. All traffic passes through the hub virtual network, and it can act as a gateway to other hubs in different regions. You set up your security posture at the hubs, so they get to segment and govern the traffic between the virtual networks in a scalable way.\nOne benefit of this pattern is that as your network topology grows, the security posture overhead doesn't grow (except when you expand to new regions).\nCompare patterns\nThe following table compares capabilities of the three networking patterns. Review the details, and think about which patterns are applicable to the network topology for Tailwind Traders.\n\n\nCompare\nSingle virtual network\nMultiple networks with peering\nMultiple networks in hub-spoke topology\n\n\nConnectivity/Routing (how segments communicate)\nSystem routing provides default connectivity to any workload in any subnet.\nSystem routing provides default connectivity to any workload in any subnet.\nNo default connectivity between spoke virtual networks. A layer 3 router (such as Azure Firewall) in the hub virtual network is required to enable connectivity.\n\n\nNetwork-level traffic filtering\nTraffic is allowed by default. NSG can be used for filtering.\nTraffic is allowed by default. NSG can be used for filtering.\nTraffic between spoke virtual networks is denied by default. Azure Firewall configuration can enable selected traffic.\n\n\nCentralized logging\nNSG logs for the virtual network.\nAggregate NSG logs across all virtual networks.\nAzure Firewall logs to Azure Monitor all accepted/denied traffic sent via a hub.\n\n\nUnintended open public endpoints\nDevOps can accidentally open a public endpoint via incorrect NSG rules.\nDevOps can accidentally open a public endpoint via incorrect NSG rules.\nA spoke virtual network open port doesn't allow access. The return packet is dropped via stateful firewall (asymmetric routing).\n\n\nApplication level protection\nNSG provides network layer support only.\nNSG provides network layer support only.\nAzure Firewall supports FQDN filtering for HTTP/S and MSSQL for outbound traffic and across virtual networks.\n\n\nFor all patterns, the recommended Azure cloud native segmentation control is Azure Firewall. Azure Firewall works across both Azure Virtual Network and subscriptions to govern traffic flows by using layer 3 to layer 7 controls. You can define your communication rules and apply them consistently, such as network X can't talk with network Y and network Z can't access the internet. With Azure Firewall Manager, you can centrally manage policies across multiple Azure firewalls and enable DevOps teams to further customize local policies.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/3-design-patterns-for-azure-network-connectivity-services",
      "title": "Design patterns for Azure network connectivity services - Training | Microsoft Learn",
      "description": "Design patterns for Azure network connectivity services",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/3-design-patterns-for-azure-network-connectivity-services",
      "load_timestamp": "2025-06-01T22:19:35.235307",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design outbound connectivity and routing - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign outbound connectivity and routing\n\n\nCompleted\n\n\n7 minutes\n\n\nPart of the planning for your Azure AI-ready network solution includes exploring how to support outbound network connectivity and traffic communication routing.\nAround the globe, IPv4 address ranges are in short supply. Trying to purchase an IP address in the v4 range can be an expensive way to grant access to your internet resources. To address this issue, architects use Network Address Translation (NAT) to enable internal resources on a private network to share routable IPv4 addresses. The internal resources use the routable IPv4 addresses to access external resources on a public network. Instead of buying an IPv4 address for each resource that needs internet access, you can use a NAT service to map outgoing requests from your internal resources to external IP addresses. Azure provides this technology via the Azure Virtual Network NAT service.\nAzure routes communication traffic between your on-premises internal resources and external internet resources by using route tables. When you create a virtual network, Azure automatically creates a routing table for each subnet in the network. A routing table contains many different types of routes, including system, service endpoints, and subnet defaults. The table also has route entries for the Border Gateway Protocol (BGP), user-defined routes (UDRs), and routes from other virtual networks.\n\nBusiness scenarios\n\nSupport on-demand outbound-to-internet connectivity without preallocation.\nConfigure one or more static public IP addresses for scale.\nEnable configurable idle timeout.\nAllow TCP reset for unrecognized connections.\n\nThings to know about routing tables and routes\nLet's take a closer look at the characteristics of routing tables and the route types.\n\nSystem routes: When you create a virtual network for the first time without defining any subnets, Azure creates system route entries in the routing table. System routes are defined for a specific location when they're created. System routes can't be modified after they're created, but you can override these routes by configuring UDRs.\n\n\nUser-defined routes (custom): When you create one or multiple subnets inside a virtual network, Azure creates default entries in the routing table to enable communication between these subnets within a virtual network. These routes can be modified by using static routes, which are stored as UDRs in Azure. UDRs are also called custom routes. You create UDRs in Azure to override Azure's default system routes, or to add more routes to a subnet's route table.\n\n\nRoutes from other virtual networks: When you create a virtual network peering between two virtual networks, a route is added for each address range within the address space of each peered virtual network.\n\nBorder Gateway Protocol routes: If your on-premises network gateway exchanges BGP routes with an Azure Virtual Network gateway, a route is added for each route propagated from the on-premises network gateway. These routes appear in the routing table as BGP routes.\n\nService endpoint routes: The public IP addresses for certain services are added to the route table by Azure when you enable a service endpoint to the service. Service endpoints are enabled for individual subnets within a virtual network. When you enable a service endpoint, a route is only added to the route table for the subnet that belongs to this service. Azure manages the addresses in the route table automatically when the addresses change.\n\nRouting order: When you have competing entries in a routing table, Azure selects the next hop based on the longest prefix match similar to traditional routers. If there are multiple next hop entries with the same address prefix, Azure selects routes in a specific order: UDRs, then BGP routes, and then system routes.\n\n\nThings to consider when using routing tables and routes\nThere are many networking scenarios where defining and overriding routes can be an advantage. Review the following suggestions and consider the routes required to support the Tailwind Traders solution.\n\nConsider system routes. Define system routes for specific locations and scenarios that you don't expect to modify.\n\nRoute traffic between virtual machines in the same virtual network or between peered virtual networks.\nSupport communication between virtual machines by using a virtual network-to-network VPN.\nEnable site-to-site communication through Azure ExpressRoute or an Azure VPN gateway.\n\n\nConsider user defined routes. Create custom UDRs to override Azure's default system routes, or to add more routes to a subnet's route table.\n\nEnable filtering of internet traffic by using Azure Firewall or forced tunneling.\nFlow traffic between subnets through a Network Virtual Appliance (NVA).\nDefine routes to specify how packets should be routed in a virtual network.\nDefine routes that control network traffic and specify the next hop in the traffic flow.\n\n\nConsider overriding routes. Plan for route overrides to control traffic flow.\n\nFlow through NVA: Configure route tables to force traffic between subnets to flow through an NVA.\nForced tunneling: Force all internet-bound traffic through an NVA, or on-premises, through an Azure VPN gateway.\n\n\nCheck your knowledge\n\n\n1.\nWhich route would you use for a specific location where you don't expect to modify the traffic pattern?\n\n\nSystem routes.\n\n\nUser defined routes.\n\n\nCustom or overriding routes.\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/4-design-outbound-connectivity-routing",
      "title": "Design outbound connectivity and routing - Training | Microsoft Learn",
      "description": "Design outbound connectivity with NAT and define routes in routing tables.",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/4-design-outbound-connectivity-routing",
      "load_timestamp": "2025-06-01T22:19:35.407114",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for on-premises connectivity to Azure Virtual Network - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for on-premises connectivity to Azure Virtual Network\n\n\nCompleted\n\n\n7 minutes\n\n\nFor a successful migration, it's critical to connect on-premises corporate networks to Azure. This method creates an always-on connection known as a hybrid-cloud network, where services are provided from the Azure cloud to corporate users.\nLet's examine Azure services that provide connectivity from an on-premises network to Azure resources. Let's compare options for connecting an on-premises network to an Azure virtual network.\nAzure VPN Gateway\nAn Azure VPN Gateway connection is a type of virtual network gateway that sends encrypted traffic between an Azure virtual network and an on-premises location. The encrypted traffic goes over the public internet. There are different configurations available for VPN Gateway connections, such as, site-to-site, point-to-site, or virtual network-to-network.\nReference architecture: Hybrid network with Azure VPN Gateway\nAzure ExpressRoute\nAzure ExpressRoute uses a private, dedicated connection through a non-Microsoft connectivity provider. This connection is private. Traffic doesn't go over the internet. The private connection extends your on-premises network into Azure.\nReference architecture: Hybrid network with Azure ExpressRoute\nAzure ExpressRoute with VPN failover\nCombine Azure ExpressRoute and Azure VPN Gateway to create a failover to a VPN connection if there's a loss of connectivity in the ExpressRoute circuit.\nReference architecture: Hybrid network with Azure ExpressRoute and VPN failover\nAzure Virtual WAN and hub-spoke networks\nA hub-spoke network topology is a way to isolate workloads while sharing services such as identity and security. The hub is a virtual network in Azure that acts as a central point of connectivity to your on-premises network. Spokes are virtual networks that peer with the hub. Shared services are deployed in the hub, while individual workloads are deployed as spokes.\nA hub-spoke architecture can be achieved via a customer-managed hub infrastructure or a Microsoft-managed hub infrastructure. In both cases, spokes are connected to the hub by using virtual network peering. Traffic flows between the on-premises data center and the hub through an ExpressRoute or VPN gateway connection. The main differentiator of this approach is the use of Azure Virtual WAN to replace hubs as a managed service.\nAzure Virtual WAN is a networking service that provides optimized and automated branch connectivity to, and through, Azure. Azure regions serve as hubs that you can choose to connect your branches to. You can apply the Azure backbone to also connect branches and enjoy branch-to-VNet connectivity. Azure Virtual WAN brings together many Azure cloud connectivity services such as site-to-site VPN, ExpressRoute, point-to-site user VPN into a single operational interface. Connectivity to Azure VNets is established by using virtual network connections.\nReference architecture: Hub-spoke topology with Azure Virtual WAN\nThe Azure Virtual WAN architecture includes the benefits of standard hub-spoke network topology and introduces several advantages:\n\nFull meshed hub among Azure virtual networks\nBranch-to-Azure connectivity\nBranch-to-branch connectivity\nMixed use of VPN Gateway and ExpressRoute connections\nMixed use of user VPN to the site\nVirtual network-to-network connectivity\n\nCompare services\nThe following table compares the benefits and challenges of the network connectivity options. Review the scenarios, and think about which services can enhance the network solution for Tailwind Traders.\n\n\nCompare\nAzure VPN Gateway\nAzure ExpressRoute\nExpressRoute + VPN failover\nAzure Virtual WAN + hub-spoke\n\n\nBenefits\n- Simple to configure  - High bandwidth available (up to 10 Gbps depending on VPN Gateway SKU)\n- High bandwidth available (up to 10 Gbps depending on connectivity provider)  - Supports dynamic scaling of bandwidth to help reduce costs during periods of lower demand (not supported by all connectivity providers)  - Enables direct organizational access to national clouds (depends on connectivity provider)\n- High availability if ExpressRoute circuit fails (fallback connection on lower bandwidth network\n- Reduced operational overhead by replacing existing hubs with fully managed service  - Cost savings by using managed service, which removes need for NVA  - Improved security via centrally managed secured hubs with Azure Firewall and Virtual WAN  - Separates concerns between central IT (SecOps, InfraOps) and workloads (DevOps)\n\n\nChallenges\n- Requires on-premises VPN device\n- Can be complex to set up  - Requires working with non-Microsoft connectivity provider  - Provider responsible for provisioning network connection  - Requires high-bandwidth routers on-premises\n- Complex to configure  - Must set up both VPN connection and ExpressRoute circuit  - Requires redundant hardware (VPN appliances)  - Requires redundant Azure VPN Gateway connection for which you pay charges\nNote: Azure Virtual WAN is designed to reduce previously listed connectivity challenges.\n\n\nScenarios\n- Hybrid apps with light traffic between on-premises hardware and the cloud  - Able to trade slightly extended latency for flexibility and processing power of the cloud\n- Hybrid apps running large-scale, mission-critical workloads that require high degree of scalability\n- Hybrid apps that require higher bandwidth of ExpressRoute and highly available network connectivity\n- Connectivity among workloads requires central control and access to shared services  - Enterprise requires central control over security aspects like a firewall and segregated management for workloads in each spoke",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/5-design-for-premises-connectivity-to-azure-virtual-networks",
      "title": "Design for on-premises connectivity to Azure Virtual Network - Training | Microsoft Learn",
      "description": "Design for on-premises connectivity to Azure Virtual Network",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/5-design-for-premises-connectivity-to-azure-virtual-networks",
      "load_timestamp": "2025-06-01T22:19:35.607338",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Choose an application delivery service - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nChoose an application delivery service\n\n\nCompleted\n\n\n3 minutes\n\n\nAzure offers several AI-ready load-balancing services for distributing your workloads across multiple computing resources. As you review the options, there are several factors to consider in your planning.\n\nThings to know about load balancing\nWhen selecting an Azure load-balancing service, consider these configuration characteristics.\n\nTraffic type: Are you designing a web (HTTP/HTTPS) application? Is the app public facing or is it private?\n\nGlobal versus regional: Do you need to load balance virtual machines or containers within a virtual network, or load balance scale unit/deployments across regions, or both?\n\nAvailability: Does the service SLA meet your requirements?\n\nCost: What are your cost expectations? You can review the Azure pricing options. In addition to the cost of the service itself, consider the operations cost for managing a solution built on that service.\n\nFeatures and limits: What are the overall limitations of each service? You can review the service limits.\n\n\nIn the next unit, we examine features of several load-balancing services, including Azure Front Door, Traffic Manager, Load Balancer, and Application Gateway.\nThings to consider when choosing load balancing\nAs you review the descriptions of the Azure load-balancing services in the next unit, you can use the following flowchart to help you to find the ideal solution for your application. The flowchart guides you through a set of key decision criteria to reach a recommendation.\n\nTip\nTreat this flowchart as a starting point.\nEvery application has unique requirements, so use these recommendations as a starting point in your planning.\n\nAfter you identify possible solutions for your requirements, apply the options to your scenarios and do a detailed evaluation. If your application consists of multiple workloads, evaluate each workload separately. A complete solution might incorporate two or more load-balancing solutions.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/6-choose-application-delivery-service",
      "title": "Choose an application delivery service - Training | Microsoft Learn",
      "description": "Choose an application delivery service",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/6-choose-application-delivery-service",
      "load_timestamp": "2025-06-01T22:19:35.760884",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for application delivery services - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for application delivery services\n\n\nCompleted\n\n\n5 minutes\n\n\nAzure offers several AI-ready networking services to help deliver applications. In this unit, we examine Front Door, Traffic Manager, Load Balancer, and Application Gateway. Here's a high-level comparison table.\n\n\nFeature/Service\nAzure Front Door\nApplication Gateway\nTraffic Manager\nLoad Balancer\n\n\nType\nGlobal\nRegional\nGlobal\nRegional/Global\n\n\nLayer\nLayer 7 (HTTP/HTTPS)\nLayer 7 (HTTP/HTTPS)\nDNS-based\nLayer 4 (TCP/UDP)\n\n\nPrimary Use Case\nWeb traffic load balancing, application acceleration, and global routing\nWeb application firewall, TSL/SSL termination, and HTTP load balancing\nDNS-based traffic routing for high availability and performance\nInternal and external load balancing for non-HTTP(S) traffic\n\n\nKey Features\nPath-based routing, TSL/SSL offload, Web Application Firewall (WAF), URL-based routing\nPath-based routing, TSL/SSL offload, Web Application Firewall (WAF), URL-based routing\nDNS-based routing, geographic routing, priority routing, weighted routing\nHigh availability, low latency, zonal and zone-redundant endpoints\n\n\nScalability\nHigh\nHigh\nHigh\nHigh\n\n\nCost\nBased on data processed and rules applied\nBased on data processed, rules applied, and SKU\nBased on DNS queries, health checks, and data points processed\nBased on rules and data processed\n\n\nThe different load balancers can work together in your networking architecture.\n\n\nAzure Front Door\nAzure Front Door lets you define, manage, and monitor the global routing for your web traffic by optimizing for best performance and instant global failover for high availability. With Front Door, you can transform your global (multi-region) consumer and enterprise applications into robust, high-performance personalized modern applications, APIs, and content that reaches a global audience with Azure.\nBusiness scenarios\n\nLow latency: Ensure requests are sent to the lowest latency backends.\nPriority: Support primary and secondary backends.\nWeighted: Distribute traffic by using weight coefficients.\nAffinity: Ensure requests from the same end user are sent to the same backend.\nSupport WAF and CDN integration for HTTP(S) traffic.\nSupport for content delivery services.\n\n\nTip\nLearn more with the Introduction to Azure Front Door training module.\n\nAzure Traffic Manager\nAzure Traffic Manager is a DNS-based traffic load balancer that enables you to distribute traffic optimally to services across global Azure regions, while providing high availability and responsiveness. Traffic Manager provides a range of traffic-routing methods to distribute traffic such as priority, weighted, performance, geographic, multi-value, and subnet.\nBusiness scenarios\n\nIncrease application availability.\nImprove application performance.\nCombine hybrid applications.\nDistribute traffic for complex deployments.\n\n\nTip\nLearn more with the Enhance your service availability and data locality by using Azure Traffic Manager training module.\n\nAzure Load Balancer\nAzure Load Balancer provides high-performance, low-latency Layer 4 load-balancing for all UDP and TCP protocols.\nBusiness scenarios\n\nManage inbound and outbound connections.\nConfigure public and internal load-balanced endpoints.\nManage service availability by mapping inbound connections to back-end pool destinations (via TCP and HTTP health-probe rules).\n\n\nTip\nLearn more with the Introduction to Azure Load Balancer training module.\n\nAzure Application Gateway\nAzure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Application Gateway is an Application Delivery Controller (ADC) as a service, offering various layer 7 load-balancing capabilities for your applications. There are two primary methods of routing traffic: path-based routing and multiple-site routing.\nBusiness scenarios\n\nPath-based routing: Send requests with different URL paths to a different pool of back-end servers.\nMultiple-site routing: Support tenants with virtual machines or other resources that host a web application.\n\n\nTip\nLearn more with the Introduction to Azure Application Gateway training module.\n\n\nCheck your knowledge\n\n\n1.\nWhich Azure service is a DNS-based traffic load balancer that enables you to distribute traffic to services across global regions?\n\n\nAzure Application Gateway\n\n\nFront Door\n\n\nTraffic Manager\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/7-design-for-application-delivery-services",
      "title": "Design for application delivery services - Training | Microsoft Learn",
      "description": "Review considerations and a decision tree flowchart to design for application delivery services.",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/7-design-for-application-delivery-services",
      "load_timestamp": "2025-06-01T22:19:35.948657",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design for application protection services - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign for application protection services\n\n\nCompleted\n\n\n7 minutes\n\n\nAzure offers several networking services to help protect your network resources. You can protect your applications by using one service or a combination of services. In this unit, we examine Azure DDoS Protection, Azure Firewall, Private Link, Web Application Firewall, and Virtual Network security groups. This diagram illustrates network defense in depth in the private cloud.\n\n\nAzure DDoS Protection (distributed denial of service protection)\nAzure DDoS Protection provides countermeasures against the most sophisticated DDoS threats. The service provides enhanced DDoS mitigation capabilities for your application and resources deployed in your virtual networks. Additionally, customers who use Azure DDoS Protection have access to DDoS Rapid Response support to engage DDoS experts during an active attack.\nBusiness scenarios\n\nImplement always-on traffic monitoring, adaptive tuning, and mitigation scale.\nAccess multi-layered protection, including attack analytics, metrics, and alerting.\nReceive support from the DDoS rapid response team.\n\n\nTip\nLearn more with the Introduction to Azure DDoS Protection training module.\n\nAzure Private Link\nAzure Private Link enables you to access Azure PaaS services (such as Azure Storage and SQL Database) and Azure hosted customer-owned/partner services over a private endpoint in your virtual network. Traffic between your virtual network and the service travels the Microsoft backbone network. Exposing your service to the public internet is no longer necessary. You can create your own private link service in your virtual network and deliver it to your customers. Private Link is used to access PaaS services, such as Azure Storage, Azure SQL, App Services, and more.\nBusiness scenarios\n\nEnable private connectivity to services on Azure.\nIntegrate with on-premises and peered networks.\nRestrict traffic to the Microsoft network with no public internet access.\n\n\nTip\nLearn more with the Introduction to Azure Private Link training module.\n\nAzure Firewall\nAzure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It's a fully stateful firewall as a service with built-in high availability and unrestricted cloud scalability. Azure Firewall uses a static public IP address for your virtual network resources, which allows outside firewalls to identify traffic originating from your virtual network. Azure Firewall provides inbound protection for non-HTTP/S protocols (such as RDP, SSH, and FTP), outbound network-level protection for all ports and protocols, and application-level protection for outbound HTTP/S.\nBusiness scenarios\n\nImplement centralized creation, enforcement, and logging of application and network connectivity policies.\nApply connectivity policies across subscriptions and virtual networks.\nTo restrict access to your virtual machine management ports, combine Azure Firewall rules with just in time (JIT) access.\n\n\nTip\nLearn more with the Introduction to Azure Firewall training module.\n\nAzure Web Application Firewall\nAzure Web Application Firewall (WAF) provides protection to your web applications from common web exploits and vulnerabilities such as SQL injection, and cross-site scripting. Web Application Firewall provides out of box protection from OWASP top 10 vulnerabilities via managed rules. Configure customer-managed rules for extra protection based on source IP range and request attributes (headers, cookies, form data fields, query string parameters). Preventing similar attacks in your application code can be challenging. The process can require rigorous maintenance, patching, and monitoring at multiple layers of the application topology. A centralized web application firewall helps to simplify security management. A web application firewall gives application administrators better assurance of protection against threats and intrusions.\nBusiness scenarios\n\nReact faster to security threats by centrally patching known vulnerabilities instead of securing individual web apps.\nDeploy Web Application Firewall with Application Gateway, Front Door, and Content Delivery Network.\n\n\nTip\nLearn more with the Introduction to Azure Web Application Firewall training module.\n\nAzure virtual network security groups\nYou can filter network traffic to and from Azure resources in an Azure virtual network with Azure network security group (NSGs). You can use a network virtual appliance (NVA) such as Azure Firewall or firewalls from other vendors.\nAn NSG contains a list of access control list (ACL) rules that allow or deny network traffic to subnets, network interface cards (NICs), or both. NSGs can be associated with either subnets or individual NICs connected to a subnet. When an NSG is associated with a subnet, the ACL rules apply to all the virtual machines in that subnet.\nNSGs contain two sets of rules: inbound and outbound. The priority for a rule must be unique within each set. Each rule has properties of protocol, source and destination port ranges, address prefixes, direction of traffic, priority, and access type. All NSGs contain a set of default rules. The default rules can't be deleted, but because they're assigned the lowest priority, you can override them with custom rules.\nBusiness scenarios\n\nControl how Azure routes traffic from subnets.\nLimit the users in an organization who can work with resources in virtual networks.\nRestrict traffic to an individual NIC by associating an NSG directly to a NIC.\nCombine NSGs with JIT access to restrict access to your virtual machine management ports.\n\n\nCheck your knowledge\n\n\n1.\nWhich Azure product/feature would you use to filter network traffic to and from Azure resources in an Azure virtual network?\n\n\nNetwork Security Group\n\n\nPrivate Link\n\n\nWeb Application Firewall\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/8-design-for-application-protection-services",
      "title": "Design for application protection services - Training | Microsoft Learn",
      "description": "Design for application protection services",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/8-design-for-application-protection-services",
      "load_timestamp": "2025-06-01T22:19:36.116418",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n3 minutes\n\n\nTailwind Traders has several requirements to meet for their production network environment. It's important that you select the right networking solutions to meet all of the requirements.\n\nNon-internet facing web app. The company needs to load balance incoming traffic to their web application, but the app can't be internet facing.\n\nNetwork security. You need to filter HTTP(S) traffic from Azure to on-premises, and filter traffic outbound to the internet.\n\nNetwork architecture. The Network team intends to deploy resources across several Azure regions. The configuration requires global connectivity between Azure virtual networks in these Azure regions and multiple on-premises locations. The company requires centralized management of the networks and connections.\n\n\nAnswer the following questions\nChoose the best response for each question.\n\n\n1.\nWhat solution would you recommend for the load-balancing requirements?\n\n\nAzure Front Door\n\n\nAzure Application Gateway\n\n\nAzure Load Balancer\n\n\n2.\nWhich security solution is necessary to filter the traffic as indicated?\n\n\nAzure Web Application Firewall on Azure Application Gateway\n\n\nAzure Bastion\n\n\nAzure Firewall\n\n\n3.\nWhat network topology is recommended based on the architecture requirement?\n\n\nAzure Virtual WAN network topology\n\n\nTraditional network topology\n\n\nSingle isolated virtual network\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/9-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/9-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:36.302090",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary and resources - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary and resources\n\n\nCompleted\n\n\n3 minutes\n\n\nIn this module, you learned how to evaluate and design an effective AI-ready network architecture solution based on workload requirements. You reviewed features and benefits of several Azure Services, including Azure Bastion, Web Application Firewall, Front Door, and Virtual WAN. You explored how to design a solution for on-premises connectivity to Azure Virtual Networks and Azure network connectivity services. You discovered how to create a solution for application delivery services and protection services.\nLearn more with Copilot\nCopilot can assist you in designing Azure infrastructure solutions. Copilot can compare, recommend, explain, and research products and services where you need more information. Open a Microsoft Edge browser and choose Copilot (top right) or navigate to copilot.microsoft.com. Take a few minutes to try these prompts and extend your learning with Copilot.\n\nWhat Azure products are available to connect on-premises resources to the cloud? What are the advantages, disadvantages, and usage cases for each product?\n\nWhat Azure network traffic services are available for regional and global load balancing? Provide a decision tree to help me decide between the different load balancing solutions.\n\nList the top 10 best practices for optimizing network performance in Azure. How do I know if my network isn't performing well?\n\n\nLearn more with Azure documentation\n\nReview best practices to set up networking for workloads migrated to Azure.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/10-summary-resources",
      "title": "Summary and resources - Training | Microsoft Learn",
      "description": "Summary and resources",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-network-solutions/10-summary-resources",
      "load_timestamp": "2025-06-01T22:19:36.455349",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Introduction - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nIntroduction\n\n\nCompleted\n\n\n2 minutes\n\n\nAzure Architects need to design and recommend data integration solutions to support cloud migration strategies. The cloud data solution must be designed to handle the ingestion, processing, and analysis of data that's too large and complex for traditional database systems.\nMeet Tailwind Traders\n\n\nTailwind Traders is a fictitious home improvement retailer. The company specializes in hardware manufacturing, and operates retail hardware stores across the globe and online. The storage infrastructure holds various data types, including product volume stock, historical data, and production logs. The implementation also supports data streaming from real-time critical manufacturing processes and quality control statistics.\nAs you work through this lesson, suppose you're the CTO for Tailwind Traders. You're tasked with analyzing and architecting a cloud data solution to handle data that's too large and complex for traditional database systems. You're asked to determine the best approach to combine data from multiple sources. The solution must be able to reformat data into analytical models, and save the models for querying, reporting, and visualization.\nLearning objectives\nIn this module, you:\n\nDesign a data integration solution with Azure Data Factory.\n\nDesign a data integration solution with Azure Data Lake.\n\nDesign a data integration and analytics solution with Azure Databricks.\n\nDesign a data integration and analytics solution with Azure Synapse Analytics.\n\nDesign strategies for hot, warm, and cold data paths.\n\nDesign an Azure Stream Analytics solution for data analysis.\n\n\nSkills measured\nThe content in this module helps you prepare for Exam AZ-305: Designing Microsoft Azure Infrastructure Solutions. The module concepts are covered in:\nDesign data storage solutions\n\nDesign data integration\n\nRecommend a solution for data integration.\n\nRecommend a solution for data analysis.\n\n\nPrerequisites\n\nConceptual knowledge of data integration solutions.\n\nWorking experience with data integration solutions.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/1-introduction",
      "title": "Introduction - Training | Microsoft Learn",
      "description": "Introduction",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/1-introduction",
      "load_timestamp": "2025-06-01T22:19:36.649366",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design a data integration solution with Azure Data Factory - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign a data integration solution with Azure Data Factory\n\n\nCompleted\n\n\n6 minutes\n\n\nAzure Data Factory is a cloud-based data integration service that can help you create and schedule data-driven workflows. You can use Azure Data Factory to orchestrate data movement and transform data at scale. The data-driven workflows, or pipelines, ingest data from disparate data stores. Azure Data Factory is an ETL data integration process, which stands for extract, transform, and load. This integration process combines data from multiple data sources into a single data store.\nThings to know about Azure Data Factory\nThere are four major steps to create and implement a data-driven workflow in the Azure Data Factory architecture:\n\nConnect and collect. First, ingest the data to collect all the data from different sources into a centralized location.\nTransform and enrich. Next, transform the data by using a compute service like Azure Databricks and Azure HDInsight Hadoop.\nProvide continuous integration and delivery (CI/CD) and publish. Support CI/CD by using GitHub and Azure Pipelines to deliver the ETL process incrementally before publishing the data to the analytics engine.\nMonitor. Finally, use the Azure portal to monitor the pipeline for scheduled activities and for any failures.\n\nThe following diagram shows how Azure Data Factory orchestrates the ingestion of data from different data sources. Data is ingested into a Storage blob and stored in Azure Synapse Analytics. Analysis and visualization components are also connected to Azure Data Factory. Azure Data Factory provides a common management interface for all of your data integration needs.\n\n\nComponents of Azure Data Factory\nAzure Data Factory has the following components that work together to provide the platform for data movement and data integration.\n\n\nPipelines and activities: Pipelines provide a logical grouping of activities that perform a task. An activity is a single processing step in a pipeline. Azure Data Factory supports data movement, data transformation, and control activities.\nDatasets: Datasets are data structures within your data stores.\nLinked services: Linked services define the required connection information needed for Azure Data Factory to connect to external resources.\nData flows: Data flows allow data engineers to develop data transformation logic without writing code. Data flow activities can be operationalized by using existing Azure Data Factory scheduling, control, flow, and monitoring capabilities.\nIntegration runtimes: Integration runtimes are the bridge between the activity and linked Services objects. There are three types of integration runtime: Azure, self-hosted, and Azure-SSIS.\n\nBusiness scenario\nA significant challenge for a fast-growing home improvement retailer like Tailwind Traders is that it generates a high volume of data stored in relational, nonrelational, and other storage systems in both the cloud and on-premises. Management wants actionable business insights from this data as near real time as possible. Additionally, the sales team wants to set up and roll out up-selling and cross-selling solutions. How can you create a large-scale data ingestion solution in the cloud? What Azure services and solutions should you adopt to help with the movement and transformation of data between various data stores and compute resources?\nLet's review how the Azure Data Factory components are involved in a data preparation and movement scenario for Tailwind Traders. They have many different data sources to connect to and that data needs to be ingested and transformed through stored procedures that are run on the data. Finally, the data should be pushed to an analytics platform for analysis.\n\nIn this scenario, the linked service enables Tailwind Traders to ingest data from different sources and it stores connection strings to fire up compute services on demand.\nYou can execute stored procedures for data transformation that happens through the linked service in Azure-SSIS, which is the integration runtime environment for Tailwind Traders.\nThe datasets components are used by the activity object and the activity object contains the transformation logic.\nYou can trigger the pipeline, which is all the activities grouped together.\nYou can use Azure Data Factory to publish the final dataset consumed by technologies, such as Power BI or Machine Learning.\n\nThings to consider when using Azure Data Factory\nEvaluate Azure Data Factory against the following decision criteria and consider how the service can benefit your data integration solution for Tailwind Traders.\n\nConsider requirements for data integration. Azure Data Factory serves two communities: the big data community and the relational data warehousing community that uses SQL Server Integration Services (SSIS). Depending on your organization's data needs, you can set up pipelines in the cloud by using Azure Data Factory. You can access data from both cloud and on-premises data services.\nConsider coding resources. If you prefer a graphical interface to set up pipelines, then Azure Data Factory authoring and monitoring tool is the right fit for your needs. Azure Data Factory provides a low code/no code process for working with data sources.\nConsider support for multiple data sources. Azure Data Factory supports 90+ connectors to integrate with disparate data sources.\nConsider serverless infrastructure. There are advantages to using a fully managed, serverless solution for data integration. There's no need to maintain, configure, or deploy servers, and you gain the ability to scale with fluctuating workloads.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/2-solution-azure-data-factory",
      "title": "Design a data integration solution with Azure Data Factory - Training | Microsoft Learn",
      "description": "Design a data integration solution with Azure Data Factory",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/2-solution-azure-data-factory",
      "load_timestamp": "2025-06-01T22:19:36.916322",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design a data integration solution with Azure Data Lake - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign a data integration solution with Azure Data Lake\n\n\nCompleted\n\n\n7 minutes\n\n\nA data lake is a repository of data stored in its natural format, usually as blobs or files. Azure Data Lake Storage is a comprehensive, scalable, and cost-effective data lake solution for big data analytics built into Azure. Azure Data Lake Storage combines a file system with a storage platform to help you quickly identify insights into your data. The solution builds on Azure Blob Storage capabilities to provide optimizations for analytics workloads. This integration enables analytics performance, high-availability, security, and durability capabilities of Azure Storage.\n\nNote\nThe current implementation of the service is Azure Data Lake Storage Gen2.\n\nThings to know about Azure Data Lake Storage\nTo better understand Azure Data Lake Storage, let's examine the following characteristics.\n\nAzure Data Lake Storage can store any type of data by using the data's native format. With support for any data format and massive data sizes, Azure Data Lake Storage can work with structured, semi-structured, and unstructured data.\nThe solution is primarily designed to work with Hadoop and all frameworks that use the Apache Hadoop Distributed File System (HDFS) as their data access layer. Data analysis frameworks that use HDFS as their data access layer can directly access.\nAzure Data Lake Storage supports high throughput for input and output–intensive analytics and data movement.\nThe Azure Data Lake Storage access control model supports both Azure role-based access control (RBAC) and Portable Operating System Interface for UNIX (POSIX) access control lists (ACLs).\nAzure Data Lake Storage utilizes Azure Blob replication models. These models provide data redundancy in a single datacenter with locally redundant storage (LRS).\nAzure Data Lake Storage offers massive storage and accepts numerous data types for analytics.\nAzure Data Lake Storage is priced at Azure Blob Storage levels.\n\nHow Azure Data Lake Storage works\nThere are three important steps to use Azure Data Lake Storage:\n\nIngest data. Azure Data Lake Storage offers many different data ingestion methods:\n\nFor unplanned data, you can use tools like AzCopy, the Azure CLI, PowerShell, and Azure Storage Explorer.\nFor relational data, the Azure Data Factory service can be used. You can transfer data from any source, such as Azure Cosmos DB, SQL Database, Azure SQL Managed instances, and more.\nFor streaming data, you can use tools like Apache Storm on Azure HDInsight, Azure Stream Analytics, and so on.\n\nThe following diagram shows how unplanned data and streaming data are bulk ingested or unplanned ingested in Azure Data Lake Storage.\n\n\nAccess stored data. The easiest way to access your data is to use Azure Storage Explorer. Storage Explorer is a standalone application with a graphical user interface (GUI) for accessing your Azure Data Lake Storage data. You can also use PowerShell, the Azure CLI, HDFS CLI, or other programming language SDKs for accessing the data.\n\nConfigure access control. Control who can access the data stored in Azure Data Lake Storage by implementing an authorization mechanism. You can choose Azure RBAC or ACL.\n\n\nBusiness scenario\nTailwind Traders has multiple sources of data, including websites, Point of Sale (POS) systems, social media sites, and Internet of Things (IoT) devices. The company is interested in using Azure to analyze all their business data. You're tasked with providing guidance on how Azure can enhance their existing BI systems. You need to advise the team about how Azure storage capabilities can add value to the company's BI solution. To fulfill the data requirements, you plan to recommend Azure Data Lake Storage. Data Lake Storage provides a repository where you can upload and store huge amounts of unstructured data with an eye toward high-performance big data analytics.\nLet's review how Azure Data Lake Storage can be the right choice for the organization's big data requirements.\n\n\nScenario\nSolution\n\n\nProvide a data warehouse on the cloud for managing large volumes of data.\nAzure Data Lake Storage runs on virtual hardware on the Azure platform. Storage is scalable, fast, and reliable without incurring massive charges. It separates storage costs from compute costs. As your data volume grows, only your storage requirements change.\n\n\nSupport a diverse collection of data types, such as JSON files, CSV, log files, or other formats.\nAzure Data Lake Storage enables data democratization for your organization by storing all your data formats (including raw data) in a single location. By eliminating data silos, your users can use tools like Azure Data Explorer to access and work with every data item in their storage account.\n\n\nEnable real-time data ingestion and storage.\nAzure Data Lake Storage can ingest real-time data directly from an instance of Apache Storm on Azure HDInsight, Azure IoT Hub, Azure Event Hubs, or Azure Stream Analytics. It also works with semi-structured data and lets you ingest all your real-time data into your storage account.\n\n\nThings to consider when choosing Azure Blob Storage or Azure Data Lake\nThe following table compares storage solution criteria for using Azure Blob Storage versus Azure Data Lake. Review the criteria and consider which solution is optimal for Tailwind Traders.\n\n\nCompare\nAzure Data Lake\nAzure Blob Storage\n\n\nData types\nGood for storing large volumes of text data\nGood for storing unstructured nontext based data like photos, videos, and backups\n\n\nGeographic redundancy\nMust manually configure data replication\nProvides geo-redundant storage by default\n\n\nNamespaces\nSupports hierarchical namespaces\nSupports flat namespaces\n\n\nHadoop compatibility\nHadoop services can use data stored in Azure Data Lake\nBy using Azure Blob Filesystem Driver, applications and frameworks can access data in Azure Blob Storage\n\n\nSecurity\nSupports granular access\nGranular access isn't supported",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/3-solution-azure-data-lake",
      "title": "Design a data integration solution with Azure Data Lake - Training | Microsoft Learn",
      "description": "Design a data integration solution with Azure Data Lake",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/3-solution-azure-data-lake",
      "load_timestamp": "2025-06-01T22:19:37.104569",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design a data integration and analytic solution with Azure Databricks - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign a data integration and analytic solution with Azure Databricks\n\n\nCompleted\n\n\n7 minutes\n\n\nAzure Databricks is a fully managed, cloud-based Big Data and Machine Learning platform, which empowers developers to accelerate AI and innovation. Azure Databricks provides data science and engineering teams with a single platform for big data processing and Machine Learning. The Azure Databricks managed Apache Spark platform makes it simple to run large-scale Spark workloads.\nThings to know about Azure Databricks\nAzure Databricks is entirely based on Apache Spark, and it's a great tool for users who are already familiar with the open-source cluster-computing framework. Databricks is designed specifically for big data processing. Data scientists can take advantage of the built-in core API for core languages like SQL, Java, Python, R, and Scala.\nAzure Databricks has a Control plane and a Data plane:\n\nControl Plane: Hosts Databricks jobs, notebooks with query results, and the cluster manager. The Control plane also has the web application, hive metastore, and security access control lists (ACLs), and user sessions. Microsoft manages these components in collaboration with Azure Databricks.\nData Plane: Contains all the Azure Databricks runtime clusters that are hosted within the workspace. All data processing and storage exists within the client subscription. No data processing ever takes place within the Microsoft/Databricks-managed subscription.\n\nAzure Databricks offers three environments for developing data intensive applications.\n\nDatabricks SQL: Azure Databricks SQL provides an easy-to-use platform for analysts who want to run SQL queries on their data lake. You can create multiple visualization types to explore query results from different perspectives, and build and share dashboards.\nDatabricks Data Science & Engineering: Azure Databricks Data Science & Engineering is an interactive workspace that enables collaboration between data engineers, data scientists, and machine learning engineers. For a big data pipeline, the data (raw or structured) is ingested into Azure through Azure Data Factory in batches, or streamed near real-time by using Apache Kafka, Azure Event Hubs, or Azure IoT Hub. The data lands in a data lake for long term persisted storage within Azure Blob Storage or Azure Data Lake Storage. As part of your analytics workflow, use Azure Databricks to read data from multiple data sources and turn it into breakthrough insights by using Spark.\nDatabricks Machine Learning: Azure Databricks Machine Learning is an integrated end-to-end machine learning environment. It incorporates managed services for experiment tracking, model training, feature development and management, and feature and model serving.\n\nBusiness scenario\nLet's analyze a scenario for Tailwind Traders in the heavy machinery manufacturing division. Tailwind Traders is using Azure cloud services for their big data needs. They're working with both batch data and streaming data. The division employs data engineers, data scientists, and data analysts who collaborate to produce quick insightful reporting for many stakeholders. To fulfill the big data requirements, you plan to recommend Azure Databricks and implement the Data Science and Engineering environment.\nLet's review why Azure Databricks can be the right choice to meet these requirements.\n\nAzure Databricks provides an integrated Analytics workspace based on Apache Spark that allows collaboration between different users.\nBy using Spark components like Spark SQL and Dataframes, Azure Databricks can handle structured data. It integrates with real-time data ingestion tools like Kafka and Flume for processing streaming data.\nSecure data integration capabilities built on top of Spark enable you to unify your data without centralization. Data scientists can visualize data in a few steps, and use familiar tools like Matplotlib, ggplot, or d3.\nThe Azure Databricks runtime abstracts out the infrastructure complexity and the need for specialized expertise to set up and configure your data infrastructure. Users can use existing languages skills for Python, Scala, and R, and explore the data.\nAzure Databricks integrates deeply with Azure databases and stores like Azure Synapse Analytics, Azure Cosmos DB, Azure Data Lake Storage, and Azure Blob Storage. It supports diverse data store platforms, which satisfies the Tailwind Traders big data storage needs.\nIntegration with Power BI allows for quick and meaningful insights, which is a requirement for Tailwind Traders.\nAzure Databricks SQL isn't the right choice because it can't handle unstructured data.\nAzure Databricks Machine Learning is also not the right environment choice because machine learning isn't a requirement in this scenario.\n\nThings to consider when using Azure Databricks\nYou can use Azure Databricks as a solution for multiple scenarios. Consider how the service can benefit your data integration solution for Tailwind Traders.\n\nConsider data science preparation of data. Create, clone, and edit clusters of complex, unstructured data. Turn the data clusters into specific jobs. Deliver the results to data scientists and data analysts for review.\nConsider insights in the data. Implement Azure Databricks to build recommendation engines, churn analysis, and intrusion detection.\nConsider productivity across data and analytics teams. Create a collaborative environment and shared workspaces for data engineers, analysts, and scientists. Teams can work together across the data science lifecycle with shared workspaces, which helps to save valuable time and resources.\nConsider big data workloads. Exercise Azure Data Lake and the engine to get the best performance and reliability for your big data workloads. Create no-fuss multi-step data pipelines.\nConsider machine learning programs. Take advantage of the integrated end-to-end machine learning environment. It incorporates managed services for experiment tracking, model training, feature development and management, and feature and model serving.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/4-solution-azure-data-brick",
      "title": "Design a data integration and analytic solution with Azure Databricks - Training | Microsoft Learn",
      "description": "Design a data integration and analytic solution with Azure Databricks",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/4-solution-azure-data-brick",
      "load_timestamp": "2025-06-01T22:19:37.308345",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design a data integration and analytic solution with Azure Synapse Analytics - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign a data integration and analytic solution with Azure Synapse Analytics\n\n\nCompleted\n\n\n7 minutes\n\n\nAzure Synapse Analytics combines features of big data analytics, enterprise data storage, and data integration. The service lets you run queries on serverless data or data at scale. Azure Synapse supports data ingestion, exploration, transformation, and management, and supports analysis for all your BI and machine learning needs.\nThings to know about Azure Synapse Analytics\nAzure Synapse Analytics implements a massively parallel processing (MPP) architecture and has the following characteristics.\n\nThe Azure Synapse Analytics architecture includes a control node and a pool of compute nodes.\n\n\nThe control node is the brain of the architecture. It's the front end that interacts with all applications. The compute nodes provide the computational power. The data to be processed is distributed evenly across the nodes.\n\nYou submit queries in the form of Transact-SQL statements, and Azure Synapse Analytics runs them.\n\nAzure Synapse uses a technology named PolyBase that enables you to retrieve and query data from relational and nonrelational sources. You can save the data read in as SQL tables within the Azure Synapse service.\n\n\nComponents of Azure Synapse Analytics\nAzure Synapse Analytics is composed of the five elements:\n\n\nAzure Synapse SQL pool: Synapse SQL offers both serverless and dedicated resource models to work with a node-based architecture. For predictable performance and cost, you can create dedicated SQL pools. For irregular or unplanned workloads, you can use the always-available, serverless SQL endpoint.\nAzure Synapse Spark pool: This pool is a cluster of servers that run Apache Spark to process data. You write your data processing logic by using one of the four supported languages: Python, Scala, SQL, and C# (via .NET for Apache Spark). Apache Spark for Azure Synapse integrates Apache Spark (the open source big data engine used for data preparation, data engineering, ETL, and machine learning).\nAzure Synapse Pipelines: Azure Synapse Pipelines applies the capabilities of Azure Data Factory. Pipelines are the cloud-based ETL and data integration service that allows you to create data-driven workflows for orchestrating data movement and transforming data at scale. You can include activities that transform the data as it's transferred, or you can combine data from multiple sources together.\nAzure Synapse Link: This component allows you to connect to Azure Cosmos DB. You can use it to perform near real-time analytics over the operational data stored in an Azure Cosmos DB database.\nAzure Synapse Studio: This element is a web-based IDE that can be used centrally to work with all capabilities of Azure Synapse Analytics. You can use Azure Synapse Studio to create SQL and Spark pools, define and run pipelines, and configure links to external data sources.\n\nAnalytical options\nAzure Synapse Analytics supports a range of analytical scenarios. As you review the table, consider how the scenarios apply to the Tailwind Traders organization.\n\n\nAnalysis\nScenario\nDescription\n\n\nDescriptive\nWhat is happening?\nAzure Synapse applies the dedicated SQL pool capability that enables you to create a persisted data warehouse to analyze what now questions. You can make use of the serverless SQL pool to prepare data from files stored in a data lake to create a data warehouse interactively.\n\n\nDiagnostic\nWhy is it happening?\nYou can use the serverless SQL pool capability within Azure Synapse to interactively explore data within a data lake. Serverless SQL pools can quickly enable a user to search for other data that might help them to understand why questions.\n\n\nPredictive\nWhat is likely to happen?\nAzure Synapse Analytics uses its integrated Apache Spark engine and Azure Synapse Spark pools for predictive analytics. It combines this action with other services, such as Azure Machine Learning Services and Azure Databricks to help you answer what future questions.\n\n\nPrescriptive\nWhat needs to be done?\nYou can use prescriptive analytics real-time or near real-time data to help you identify solutions for your what action questions. Azure Synapse Analytics provides this capability through Apache Spark and Azure Synapse Link, and by integrating streaming technologies like Azure Stream Analytics.\n\n\nBusiness scenario\nLet's examine a scenario where the company is serving clients with stock market information. You need to provide a combination of batch and stream processing to support the Tailwind Traders infrastructure. The up-to-the-second data might be used to help monitor real time, where an instant decision is required to make informed split-second buy or sell decisions. Historical data is equally important for a view of trends in performance. What kind of data warehouse and data integration solution would you recommend to provide access to the streams of raw data, and the prepared business information derived from this data? With Azure Synapse Analytics, you can ingest data from external sources and then transform and aggregate this data into a format suitable for analytics processing.\nThings to consider when choosing Azure Data Factory or Azure Synapse Analytics\nThe following table compares storage solution criteria for using Azure Data Factory versus Azure Synapse Analytics. Review the criteria and consider which solution is optimal for Tailwind Traders.\n\n\nCompare\nAzure Data Factory\nAzure Synapse Analytics\n\n\nData sharing\nData can be shared across different data factories\nNot supported\n\n\nSolution templates\nSolution templates are provided with the Azure Data Factory template gallery\nSolution templates are provided in the Synapse Workspace Knowledge center\n\n\nIntegration runtime cross region flows\nCross region data flows are supported\nNot supported\n\n\nMonitor data\nData monitoring is integrated with Azure Monitor\nDiagnostic logs are available in Azure Monitor\n\n\nMonitor Spark Jobs for data flow\nNot supported\nSpark Jobs can be monitored for data flow by using Synapse Spark pools\n\n\nAzure Synapse Analytics is an ideal solution for many other scenarios. Consider the following options:\n\nConsider variety of data sources. When you have various data sources that use Azure Synapse Analytics for code-free ETL and data flow activities.\nConsider Machine Learning. When you need to implement Machine Learning solutions by using Apache Spark, you can use Azure Synapse Analytics for built-in support for Azure Machine Learning.\nConsider data lake integration. When you have existing data stored on a data lake and need integration with Azure Data Lake and other input sources, Azure Synapse Analytics provides seamless integration between the two components.\nConsider real-time analytics. When you require real-time analytics, you can use features like Azure Synapse Link to analyze data in real-time and offer insights.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/5-solution-azure-synapse-analytics",
      "title": "Design a data integration and analytic solution with Azure Synapse Analytics - Training | Microsoft Learn",
      "description": "Design a data integration and analytic solution with Azure Synapse Analytics",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/5-solution-azure-synapse-analytics",
      "load_timestamp": "2025-06-01T22:19:37.485310",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design strategies for hot, warm, and cold data paths - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign strategies for hot, warm, and cold data paths\n\n\nCompleted\n\n\n6 minutes\n\n\nTraditionally, data was stored on-premises. No consideration was made about how the data was to be used. In the cloud, data can be stored based on access, lifecycle, and other compliance requirements. In this unit, we examine hot, warm, and cold data paths, and consider options for storing and computing the data.\nWarm data path\nA warm data path supports analyzing data as it flows through the system. The data stream is processed in near real time. The data is saved to the warm storage, and pushed to the analytics clients.\n\nThe Azure platform provides many options for processing the events, and Azure Stream Analytics is a popular choice.\nStream Analytics can execute complex analysis at scale for tumbling, sliding, and hopping windows. The service supports running stream aggregations and joining external data sources. For complex processing, performance can be extended by cascading multiple instances of Azure Event Hubs, Stream Analytics jobs, and Azure functions.\nWarm storage can be implemented with various services on the Azure platform, such as Azure SQL Database and Azure Cosmos DB.\n\nBusiness scenario\nLet's explore a common scenario for IoT device data aggregation. The devices might send data, but not produce any results or analysis data. This situation highlights a common challenge: trying to extract insight out of IoT data. The data you're looking for isn't available in the data you receive. You need to infer utilization by combining the data you receive with other sources of data. Then, you apply rules to determine whether the machine is producing results. Also, the rules might change from company to company, when they have different expectations for analysis or results.\nCold data path\nThe warm data path is where stream processing occurs to discover patterns over time. However, you might need to calculate utilization over some time period in the past. You also might require different pivots and aggregations, and need to merge these results with the warm path results to present a unified view to the user. A cold data path can help accomplish these tasks.\n\nA cold data path consists of a batch layer and serving layers that provide a long-term view of the system.\nThe batch layer creates precalculated aggregate views to enable fast query responses over long periods. The Azure platform provides diverse technology options for this layer.\nThe cold path includes a long-term data store for the solution, and Azure Storage is a common approach. Azure Storage includes Azure Blobs (objects), Azure Data Lake Storage Gen2, Azure Files, Azure Queues, and Azure Tables.\nCold storage can be either Blobs, Data Lake Storage Gen2, Azure Tables, or a combination.\nTo store massive amounts of unstructured data, the best options are Blob Storage, Azure Files, or Azure Data Lake Storage Gen2. Cold path storage is ideal for original messages that contain unprocessed data received by IoT applications.\n\nBusiness scenario\nExamine the scenario where you need to build machine learning models for Tailwind Traders website interactions over time. You need to automate data movement and conduct data transformations. In this scenario, Azure Data Factory is a great solution for creating the batch views on the serving layer of the cold path to fulfill these requirements. It's a cloud-based managed data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation. It can process and transform the data by using services such as Azure HDInsight Hadoop, Apache Spark, and Azure Databricks. You can build machine learning models and consume them with the analytics clients.\nHot data path\nA hot data path is typically used for processing or displaying data in real time. This path is employed for real-time alerting and streaming operations. A hot path is where latency-sensitive data results need to be ready in seconds or less, and where data flows for rapid consumption by analytics clients.\nBusiness scenario\nTailwind Traders wants to implement data analysis for its customer portal. They need to collect streaming data and provide real-time alerts to administrators, customer assistants, and portal users. The hot path is ideal for this scenario. Data can be collected as it's  entered or displayed. The data can be delivered in near real time to administrators for quick analysis and follow-up action.\nCompare data paths\nThe following table compares scenarios for the three path solutions. Review the scenarios and consider which solutions are required for Tailwind Traders.\n\n\nScenario\nPath solution\n\n\nFlexible support for data requirements that change frequently. Enable processing or displaying data in real time.\nHot data path\n\n\nSupport rarely used data, such as data stored for compliance or legal reasons. Enable consumption of data for long term analytics and batch processing.\nCold data path\n\n\nStore or display a recent subset of data. Enable consumption of data for small analytical and batch processing.\nWarm data path",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/6-design-strategy-for-hot-warm-cold-data-path",
      "title": "Design strategies for hot, warm, and cold data paths - Training | Microsoft Learn",
      "description": "Design strategies for hot, warm, and cold data paths",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/6-design-strategy-for-hot-warm-cold-data-path",
      "load_timestamp": "2025-06-01T22:19:37.875427",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Design an Azure Stream Analytics solution for data analysis - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nDesign an Azure Stream Analytics solution for data analysis\n\n\nCompleted\n\n\n7 minutes\n\n\nThe process of consuming data streams, analyzing them, and deriving actionable insights is called stream processing. Azure Stream Analytics is a fully managed (PaaS offering), real-time analytics and complex event-processing engine. It offers the possibility to perform real-time analytics on multiple streams of data from sources like IoT device data, sensors, clickstreams, and social media feeds.\nThings to know about Azure Stream Analytics\nAzure Stream Analytics works on the following concepts:\n\nData streams: Data streams are continuous data generated by applications, IoT devices, or sensors. The data streams are analyzed and actionable insights are extracted. Some examples are monitoring data streams from industrial and manufacturing equipment and monitoring water pipeline data by utility providers. Data streams help us understand change over time.\nEvent processing: Event processing refers to consumption and analysis of a continuous data stream to extract actionable insights from the events happening within that stream. An example is how a car passing through a tollbooth should include temporal information like a timestamp that indicates when the event occurred.\n\n\nImportant\nAzure Stream Analytics supports processing events in three data formats: CSV, JSON, and Avro.\n\nThe following illustration shows the Stream Analytics pipeline, and how data is ingested, analyzed, and sent for presentation or action.\n\n\nKey features\nStream Analytics ingests data from Azure Event Hubs (including Azure Event Hubs from Apache Kafka), Azure IoT Hub, or Azure Blob Storage. The query, which is based on SQL query language, can be used to easily filter, sort, aggregate, and join streaming data over a period. You can also extend this SQL language with JavaScript and C# user-defined functions (UDFs).\nAn Azure Stream Analytics job consists of an input, query, and an output. You can do the following tasks with the job output:\n\nRoute data to storage systems like Azure Blob Storage, Azure SQL Database, Azure Data Lake Store, and Azure Cosmos DB.\nSend data to Power BI for real-time visualization.\nStore data in a Data Warehouse service like Azure Synapse Analytics to train a machine learning model based on historical data or perform batch analytics.\nTrigger custom downstream workflows by sending the data to services like Azure Functions, Azure Service Bus Topics, or Azure Queues.\n\nBusiness scenario\nTailwind Traders is using digital transformation for their applications and services to help with the growth of the company. They need to support accessing, storing, and analyzing sensor data from the GPS on their delivery trucks that are on the road delivering goods. You're looking for a solution to provide real time analytics on GPS streaming data from the trucks to enable administrators to make decisions in real time. On further analysis, you learn that the team would like this data present in an existing Power BI visualization dashboard. Azure Stream Analytics can help fulfill the requirements of this scenario.\nAzure Stream Analytics is an ideal solution for other common enterprise data requirements. Consider the following scenarios:\n\n\nRequirement\nDescription\n\n\nAnalyze real-time telemetry streams from IoT devices.\nGather real-time sensor data in Azure Stream Analytics by building automation systems that relay temperature, humidity, fan runtimes. You can make adjustments to maintain optimum building temperature and reduce costs.\n\n\nBuild web logs and clickstream analytics.\nA consumer goods retailer can offer real-time product suggestions to users based on e-commerce analytics.\n\n\nCreate geospatial analytics.\nPrepare analytics for geospatial data sources like sensors, social media, satellite imagery, and mobile devices. You can predict extreme weather events like wildfires and hurricanes to help airlines with routing. You can send out mobile alerts to customers for adverse weather conditions based on their geolocation.\n\n\nExecute remote monitoring and predictive maintenance of high value assets.\nMonitor high value assets such as Industrial equipment by gathering operational data in Azure Stream Analytics. You can maximize the useful life of your equipment through predictive maintenance. Data gathered from electrical power transformers is utility companies to avoid disruption of operation.\n\n\nPerform real-time analytics on point of sale data.\nDetect fraudulent credit card transactions, and identify suspicious activity at point of sale. You can spot unusually large transactions or unusual location activity based on the credit card holder's contact information. Alert triggers can be set up on data gathered in Azure Stream Analytics.\n\n\nIn the Tailwind Traders scenario, we can apply Azure Stream Analytics to visualize real-time locations of the trucks through Power BI. For management decisions on analytical workloads, data can be stored in a data warehouse like Azure Cosmos DB or Azure Data Lake for future analysis.\nThings to consider when using Azure Stream Analytics\nAzure Stream Analytics can be a valuable component in your data integration plan for Tailwind Traders. Review the following benefits of the service.\n\nConsider provisioning requirements. Azure Stream Analytics is a fully managed service. It's offered as a PaaS (Platform as a Service) offering, so there's no overhead of provisioning any hardware or infrastructure. Azure Stream Analytics fully manages your job, so you can focus on your business logic and not on the infrastructure.\nConsider costs. Stream Analytics is low cost. Billing is done by Streaming Units (SUs) consumed that represents the amount of CPU and memory resources allocated. Scaling up and down are based on business needs, which can also lower costs. No maintenance charges are involved.\nConsider implementation. You can run Azure Stream Analytics in the cloud for large-scale analytics. For ultra-low latency analytics, run Stream Analytics on IoT Edge or Azure Stack.\nConsider performance. Stream Analytics offers reliable performance guarantees. It supports higher performance by partitioning, which allows complex queries to be parallelized and executed on multiple streaming nodes. Stream Analytics can process millions of events every second. It can deliver results with ultra-low latencies.\nConsider security. Stream Analytics encrypts all incoming and outgoing communications and supports TLS 1.2. Built-in checkpoints are also encrypted. Stream Analytics doesn't store the incoming data because all processing is done in-memory.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/7-design-azure-stream-analytics-solution-for-data-analysis",
      "title": "Design an Azure Stream Analytics solution for data analysis - Training | Microsoft Learn",
      "description": "Design an Azure Stream Analytics solution for data analysis",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/7-design-azure-stream-analytics-solution-for-data-analysis",
      "load_timestamp": "2025-06-01T22:19:38.326793",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Module assessment - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nModule assessment\n\n\nCompleted\n\n\n3 minutes\n\n\nTailwind Traders has several workloads that are being migrated to Azure. It's important the data solutions are designed to handle the ingestion, processing, and analysis of data based on the following requirements:\n\nManufacturing data. The company has been storing manufacturing logs that are collected from the assembly line. You want to analyze these logs to gain insights into material behavior and quality assurance. To analyze these logs, you need to use reference data, such as material information, chemical information, and origin information that's in an on-premises data store. You want to utilize this data from the on-premises data store, combine it with other log data that it has in a cloud data store, and run stored procedures on the data to gain insights.\n\nReal-time data. Tailwind Traders needs real-time data ingestion and storage for multiple data sources like their websites, point of sale systems, and social media sites. You need a solution to analyze this data and provide useful insights to the CEO.\n\nHistorical company data. The company is required to store 5 TB of company data for legal reasons. This data is rarely used or referenced, but it must not be deleted. You need a cost effective method for storing this data.\n\n\nAnswer the following questions\nChoose the best response for each question.\n\n\n1.\nWhich Azure solution fulfills the manufacturing data requirements?\n\n\nAzure Databricks\n\n\nAzure Data Factory\n\n\nAzure Data Lake\n\n\n2.\nWhich Azure solution provides real-time data ingestion and storage of multiple data sources?\n\n\nAzure Databricks\n\n\nAzure Blob Storage\n\n\nAzure Data Lake\n\n\n3.\nBased on the historical company data requirements, what's the appropriate solution?\n\n\nCold storage\n\n\nHot storage\n\n\nWarm storage\n\n\nYou must answer all questions before checking your work.\n\n\nYou must answer all questions before checking your work.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/8-knowledge-check",
      "title": "Module assessment - Training | Microsoft Learn",
      "description": "Knowledge check",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/8-knowledge-check",
      "load_timestamp": "2025-06-01T22:19:38.611253",
      "doc_type": "html"
    }
  },
  {
    "page_content": "Summary and resources - Training | Microsoft Learn\n\n\n\t\t\tSkip to main content\n\t\t\n\n\nSummary and resources\n\n\nCompleted\n\n\n2 minutes\n\n\nIn this module, you learned about Azure data integration and data analysis solutions. You explored integration and analytics options offered by Azure Data Factory, Azure Data Lake, Azure Databricks, and Azure Synapse Analytics. You discovered how to optimize your strategy for hot, warm, and cold data paths. You reviewed how to design an Azure Stream Analytics solution for data analysis.\nLearn more with Copilot\nCopilot can assist you in designing Azure infrastructure solutions. Copilot can compare, recommend, explain, and research products and services where you need more information. Open a Microsoft Edge browser and choose Copilot (top right) or navigate to copilot.microsoft.com. Take a few minutes to try these prompts and extend your learning with Copilot.\n\nAs an Azure solution architect, I'm looking to design a data integration solution. What are my options within Azure and the use cases for them?\n\nExplain the key differences between Azure data integration services and Azure data analytics services. Provide two real-world examples for each service, focusing on scenarios where they would be most effectively applied?\n\nI'm an Azure Solution Architect tasked with designing a data integration solution that handles both batch and real-time data. We need to integrate data from SQL databases, NoSQL stores, and IoT streams into a central data warehouse and a data lake for analytics. The solution must support high-volume data transfers and complex transformations while adhering to strict compliance and security standards. What services would you recommend, and what are the typical use cases for each?\n\n\nLearn more with Azure documentation\n\nRead about Azure Data Factory.\nRead about Azure Databricks.\nRead about Azure Data Lake.\nExamine the Azure Synapse SQL architecture.\nGet an overview of Azure Synapse Link for Azure Cosmos DB.\nUnderstand Analytics end-to-end with Azure Synapse.\nUnderstand Transact-SQL features supported in Azure Synapse SQL.\nUnderstand stream processing.\nGet an overview of Azure Stream Analytics Cluster.\n\nLearn more with self-paced training\n\nUse data integration at scale with Azure Data Factory.\nDiscover Azure database and analytics services.\nReview concepts of data analytics.\nReview an introduction to Azure Data Factory.\nWork with data streams by using Azure Stream Analytics.\n\nLearn more with optional hands-on exercises\n\nExplore Azure Databricks (interactive exercise). This module requires an Azure subscription with administrative access permissions.\nExplore Azure Synapse Analytics (interactive exercise). This module requires an Azure subscription with administrative access permissions.",
    "metadata": {
      "source": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/9-summary-resources",
      "title": "Summary and resources - Training | Microsoft Learn",
      "description": "Summary and resources",
      "language": "en-us",
      "loader_type": "webbase",
      "source_url": "https://learn.microsoft.com/en-us/training/modules/design-data-integration/9-summary-resources",
      "load_timestamp": "2025-06-01T22:19:38.786823",
      "doc_type": "html"
    }
  }
]