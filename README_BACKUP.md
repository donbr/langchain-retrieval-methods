# ğŸš€ LangChain Retrieval Methods with Intelligent Ingestion Pipeline

A comprehensive tutorial and production-ready framework for comparing RAG retrieval strategies with **persistent storage**, **workflow orchestration**, and **enterprise-grade observability**.

## ğŸ¯ What's New

This project has been enhanced with **intelligent ingestion pipeline capabilities** that bridge the gap between experimentation and production deployment:

### âœ¨ **Smart Persistent Storage**
- **10x faster iterations** - no re-ingestion between experiments
- **Automatic data detection** - intelligently reuses existing data
- **Production patterns** - mimics real-world deployment scenarios

### ğŸ”„ **Prefect Workflow Orchestration**  
- **Enterprise-grade reliability** with automatic retries and error handling
- **Full observability** with comprehensive monitoring and debugging
- **Scalable deployment** from local development to cloud auto-scaling

### ğŸ“Š **Comprehensive Observability**
- **Phoenix tracing** for all LangChain components
- **Real-time monitoring** of data stores and pipeline health
- **Performance metrics** and execution insights

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Orchestration  â”‚    â”‚  Storage Layer  â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ CSV Files     â”‚â”€â”€â”€â–¶â”‚ â€¢ Prefect Flows  â”‚â”€â”€â”€â–¶â”‚ â€¢ Redis (KV)    â”‚
â”‚ â€¢ APIs          â”‚    â”‚ â€¢ Schedule Tasks â”‚    â”‚ â€¢ Qdrant (Vec)  â”‚
â”‚ â€¢ Documents     â”‚    â”‚ â€¢ Error Handling â”‚    â”‚ â€¢ PostgreSQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Retrieval Apps  â”‚
                        â”‚                  â”‚
                        â”‚ â€¢ RAG Chains     â”‚
                        â”‚ â€¢ Evaluation     â”‚
                        â”‚ â€¢ Monitoring     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. **Infrastructure Setup**

```bash
# Clone the repository
git clone <repository-url>
cd langchain-retrieval-methods

# Start all services with Docker Compose
docker compose up -d

# Verify services are running
docker compose ps
```

**Services Available:**
- **Qdrant** (Vector Database): `http://localhost:6333`
- **Redis** (Key-Value Store): `localhost:6379`  
- **PostgreSQL** (Metadata): `localhost:5432`
- **Phoenix** (Observability): `http://localhost:6006`
- **Prefect** (Orchestration): `http://localhost:4200`
- **MinIO** (Object Storage): `http://localhost:9001`

### 2. **Environment Configuration**

```bash
# Copy environment template
cp .env.example .env

# Add your API keys
OPENAI_API_KEY=your_openai_key_here
COHERE_API_KEY=your_cohere_key_here
QDRANT_API_KEY=your_qdrant_key_here  # Optional for cloud
```

### 3. **Choose Your Path**

#### **ğŸ“ Tutorial & Learning (Recommended Start)**
```bash
# Run the interactive notebook
jupyter notebook retrieval_method_comparison_arize.ipynb
```

#### **ğŸ­ Production Deployment**
```bash
# Deploy Prefect workflow
python prefect_ingestion_example.py deploy

# Run ingestion pipeline
prefect deployment run 'rag-ingestion-pipeline/production'

# Schedule automatic runs  
prefect deployment set-schedule rag-ingestion --interval 3600
```

## ğŸ“š Tutorial: Seven Retrieval Methods

The notebook demonstrates and compares these retrieval strategies:

| Method | Description | Best For |
|--------|-------------|----------|
| **ğŸ¯ Naive** | Whole-document vectors | Simple use cases, small documents |
| **ğŸ” BM25** | Keyword matching | Exact term matching, sparse retrieval |
| **ğŸ›ï¸ Contextual Compression** | LLM-based reranking | High precision requirements |
| **ğŸ”„ Multi-Query** | Query expansion | Handling ambiguous queries |
| **ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ Parent Document** | Hierarchical chunks | Large documents, context preservation |
| **ğŸ­ Ensemble** | Fusion of methods | Best overall performance |
| **ğŸ§© Semantic** | Boundary-aware chunking | Natural text boundaries |

### **Key Experiments**

Each method is evaluated across:
- **ğŸ“Š Retrieval Quality** (recall, response patterns)
- **âš¡ Latency** (ms per query)  
- **ğŸ’° Cost** (API/token usage)
- **ğŸ”§ Resource Footprint** (index size)

## ğŸ› ï¸ Implementation Options

### **Option 1: Smart Notebook (Current)**
Perfect for learning and experimentation:

```python
# Automatic data detection and reuse
if not check_data_exists():
    parent_document_retriever.add_documents(all_review_docs)
else:
    print("âœ… Using existing data!")
```

**Benefits:**
- âœ… Immediate feedback and iteration
- âœ… Smart persistence - 10x faster restarts
- âœ… Visual results and comparisons

### **Option 2: Standalone Scripts**
For controlled ingestion workflows:

```bash
# Separate ingestion from retrieval
python ingestion.py --config config/production.yaml
jupyter notebook retrieval_experiments.ipynb
```

**Benefits:**
- âœ… Clean separation of concerns
- âœ… Configurable and repeatable
- âœ… Version controlled configurations

### **Option 3: Prefect Orchestration (Production)**
Enterprise-grade workflow management:

```python
@flow(name="rag-ingestion-pipeline")
def rag_ingestion_pipeline(config: dict):
    docs = extract_documents(config["sources"])
    processed_docs = transform_documents(docs)
    
    # Parallel loading to different stores
    vector_result = load_to_vector_store(processed_docs)
    docstore_result = load_to_docstore(processed_docs)
    
    # Comprehensive validation
    validate_ingestion(config["collection_name"])
    
    return {"status": "success", "document_count": len(processed_docs)}
```

**Benefits:**
- âœ… Automatic retries and error recovery
- âœ… Comprehensive monitoring and debugging
- âœ… Scalable and production-ready
- âœ… Scheduling and automation

## ğŸ“ˆ Performance Benefits

### **Development Speed**
| Scenario | Before | After | Improvement |
|----------|--------|-------|-------------|
| **Notebook Restart** | 5-10 minutes | 30 seconds | **10-20x faster** |
| **Experiment Iteration** | Re-ingest each time | Instant reuse | **Immediate** |
| **Data Consistency** | Manual management | Automatic | **100% reliable** |

### **Production Readiness**
- **ğŸ”’ Enterprise Security**: Proper authentication and authorization
- **ğŸ“Š Full Observability**: Phoenix + Prefect monitoring stack
- **ğŸ”„ Automatic Recovery**: Built-in retry and rollback capabilities
- **âš–ï¸ Auto-scaling**: Distributed processing with work pools

## ğŸ”§ Advanced Configuration

### **Data Management Commands**

```python
# Reset all data stores (fresh start)
reset_data_stores()

# Check infrastructure status
show_data_infrastructure_summary()

# Validate retriever functionality  
validate_retriever()
```

### **Custom Data Sources**

```python
# Extend for different data sources
config = {
    "data_sources": [
        "data/documents.csv",
        "s3://bucket/files/",
        "https://api.example.com/docs"
    ],
    "collection_name": "custom_collection",
    "chunk_size": 500,
    "embedding_model": "text-embedding-3-large"
}
```

### **Production Deployment Patterns**

```yaml
# config/production.yaml
data_sources:
  - "s3://production-docs/reviews/"
  - "database://reviews_table"
collection_name: "production_reviews"
chunk_size: 200
redis_url: "redis://production-redis:6379"
qdrant_url: "https://production-qdrant.example.com"
scheduling:
  interval_hours: 24
  max_retries: 3
monitoring:
  phoenix_endpoint: "https://phoenix.example.com"
  alerts_webhook: "https://slack.example.com/webhook"
```

## ğŸ“š Documentation

### **Getting Started Guides**
- [ğŸ“– Docker Admin Guide](docs/docker-admin-guide.md) - Infrastructure setup and management
- [ğŸ” Phoenix Getting Started](docs/arize_phoenix_getting_started.md) - Observability configuration  
- [ğŸ—„ï¸ Qdrant Setup Guide](docs/qdrant_getting_started.md) - Vector database configuration
- [ğŸš€ Ingestion Pipeline Strategy](docs/ingestion_pipeline_strategy.md) - Comprehensive implementation guide

### **Example Implementations**
- [ğŸ’» Notebook Tutorial](retrieval_method_comparison_arize.py) - Interactive learning experience
- [âš™ï¸ Standalone Ingestion](ingestion.py) - Production ingestion script
- [ğŸŒŠ Prefect Workflow](prefect_ingestion_example.py) - Enterprise orchestration example
- [ğŸ”§ Retrieval Application](retrieval_application.py) - Production retrieval service

## ğŸ” Troubleshooting

### **Common Issues**

**Data not persisting between runs:**
```bash
# Check Docker volumes
docker volume ls | grep langchain-retrieval

# Verify service health
docker compose ps
```

**Ingestion pipeline fails:**
```python
# Check infrastructure status
show_data_infrastructure_summary()

# Reset and try again
reset_data_stores()
```

**Prefect deployment issues:**
```bash
# Check Prefect server status
prefect server start

# Verify deployment
prefect deployment ls
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### **Development Setup**

```bash
# Development environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements-dev.txt

# Pre-commit hooks
pre-commit install
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain** for the powerful RAG framework
- **Prefect** for workflow orchestration capabilities  
- **Arize Phoenix** for observability and tracing
- **Qdrant** for high-performance vector search
- **Redis** for reliable key-value storage

---

## ğŸ‰ Ready to Get Started?

1. **ğŸ³ Start with Docker:** `docker compose up -d`
2. **ğŸ““ Try the Tutorial:** Open `retrieval_method_comparison_arize.ipynb`
3. **ğŸš€ Deploy to Production:** Use `prefect_ingestion_example.py`
4. **ğŸ“Š Monitor Everything:** Visit Phoenix at `http://localhost:6006`

**Questions? Issues? Ideas?** Open an issue or start a discussion!

**â­ Star this repo** if it helps with your RAG projects! 