{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Environment Configuration & API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî≠ OpenTelemetry Tracing Details üî≠\n",
            "|  Phoenix Project: retrieval-method-comparison-20250601_210758\n",
            "|  Span Processor: SimpleSpanProcessor\n",
            "|  Collector Endpoint: 127.0.0.1:4317\n",
            "|  Transport: gRPC\n",
            "|  Transport Headers: {'user-agent': '****'}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "from phoenix.otel import register\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Build a dynamic project name (e.g. include timestamp)\n",
        "project_name = f\"retrieval-method-comparison-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = os.getenv('COHERE_API_KEY')\n",
        "\n",
        "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "QDRANT_API_URL = os.getenv(\"QDRANT_API_URL\")\n",
        "\n",
        "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\")\n",
        "\n",
        "# configure the Phoenix tracer\n",
        "tracer_provider = register(\n",
        "  project_name=project_name,\n",
        "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî≠ OpenTelemetry Tracing Details üî≠\n",
            "|  Phoenix Project: retrieval-method-comparison-20250601_210758\n",
            "|  Span Processor: SimpleSpanProcessor\n",
            "|  Collector Endpoint: 127.0.0.1:4317\n",
            "|  Transport: gRPC\n",
            "|  Transport Headers: {'user-agent': '****'}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from phoenix.otel import register\n",
        "\n",
        "tracer_provider = register(\n",
        "    project_name=project_name,\n",
        "    auto_instrument=True  # Automatically traces LangChain chains, retrievers, LLMs, etc.\n",
        ")\n",
        "\n",
        "# print(f\"‚úÖ Phoenix auto-instrumentation enabled for project: {project_name}\")\n",
        "# print(\"üîç Phoenix will automatically capture all LangChain component traces\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "## üìä Dataset Loading & Preprocessing for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "d794aa3a-234c-4fe7-b219-02e8b36c27ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "john_wick_1.csv already exists.\n",
            "john_wick_2.csv already exists.\n",
            "john_wick_3.csv already exists.\n",
            "john_wick_4.csv already exists.\n"
          ]
        }
      ],
      "source": [
        "# Set up a consistent data directory in the user's home directory\n",
        "from pathlib import Path\n",
        "DATA_DIR = Path.cwd() / \"data\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# URLs and filenames\n",
        "urls = [\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\", \"john_wick_1.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\", \"john_wick_2.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\", \"john_wick_3.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\", \"john_wick_4.csv\"),\n",
        "]\n",
        "\n",
        "# Download files if not already present\n",
        "for url, fname in urls:\n",
        "    file_path = DATA_DIR / fname\n",
        "    if not file_path.exists():\n",
        "        print(f\"Downloading {fname}...\")\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        file_path.write_bytes(r.content)\n",
        "    else:\n",
        "        print(f\"{fname} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "all_review_docs = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "    loader = CSVLoader(\n",
        "        file_path=(DATA_DIR / f\"john_wick_{i}.csv\"),\n",
        "        metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "    )\n",
        "\n",
        "    movie_docs = loader.load()\n",
        "    for doc in movie_docs:\n",
        "\n",
        "        # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "        doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "        # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "        doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "        # newer movies have a more recent \"last_accessed_at\" (store as ISO string)\n",
        "        doc.metadata[\"last_accessed_at\"] = (datetime.now() - timedelta(days=4-i)).isoformat()\n",
        "\n",
        "    all_review_docs.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## üèóÔ∏è Building RAG Infrastructure: Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AX5XBOIltcvD"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore  # Updated import\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from qdrant_client import QdrantClient, models\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain_community.storage import RedisStore\n",
        "from langchain.storage import create_kv_docstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Level 1: Simple Vector Storage (Baseline)\n",
        "\n",
        "- creates the vector store using the all_review_docs Document object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d6Hta5PoJlos"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1176/3908511560.py:2: UserWarning: Api key is used with an insecure connection.\n",
            "  qdrant_client = QdrantClient(\n"
          ]
        }
      ],
      "source": [
        "# Initialize Qdrant client\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_API_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    prefer_grpc=True\n",
        ")\n",
        "\n",
        "baseline_vectorstore = QdrantVectorStore(\n",
        "    embedding=embeddings,\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"azurearch_baseline\"\n",
        ")\n",
        "\n",
        "# Construct the VectorStore using cloud client\n",
        "parent_children_vectorstore = QdrantVectorStore(\n",
        "    embedding=embeddings,\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"azurearch_parent_children\",\n",
        ")\n",
        "\n",
        "semantic_vectorstore = QdrantVectorStore(\n",
        "    embedding=embeddings,\n",
        "    client=qdrant_client,\n",
        "    collection_name=\"azurearch_semantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Level 2: Hierarchical Storage (Parent-Child Architecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Documents: Redis Key-Value Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CjIJlsjsrOSi"
      },
      "outputs": [],
      "source": [
        "from langchain_community.storage import RedisStore\n",
        "from langchain_community.utilities.redis import get_client\n",
        "\n",
        "redis_client = get_client('redis://localhost:6379')\n",
        "parent_document_store = RedisStore(client=redis_client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None, None, None, None, None]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# display first 5 documents in parent_document_store\n",
        "parent_document_store.mget(list(range(5)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document Retriever definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_children_vectorstore,\n",
        "    docstore=parent_document_store,\n",
        "    child_splitter=child_splitter\n",
        ")\n",
        "\n",
        "# parent_document_retriever.add_documents(all_review_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "['azurearch_baseline', 'azurearch_parent_children', 'azurearch_semantic']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1748839136.785339    1176 chttp2_transport.cc:1201] ipv4:127.0.0.1:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {created_time:\"2025-06-01T21:38:56.785321091-07:00\", http2_error:11, grpc_status:14}\n"
          ]
        }
      ],
      "source": [
        "# display Qdrant collections\n",
        "\n",
        "existing = [c.name for c in qdrant_client.get_collections().collections]\n",
        "\n",
        "print(type(existing))\n",
        "print(existing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== baseline ===\n",
            "Exists?       True\n",
            "Point count:  count=142\n",
            "Dim / metric: 1536 / Cosine\n",
            "Shards / repl: 1 / 1\n",
            "\n",
            "=== parent ===\n",
            "Exists?       True\n",
            "Point count:  count=25840\n",
            "Dim / metric: 1536 / Cosine\n",
            "Shards / repl: 1 / 1\n",
            "\n",
            "=== semantic ===\n",
            "Exists?       True\n",
            "Point count:  count=500\n",
            "Dim / metric: 1536 / Cosine\n",
            "Shards / repl: 1 / 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display Qdrant vector store collection metadata\n",
        "\n",
        "stores = {\n",
        "    \"baseline\": baseline_vectorstore,\n",
        "    \"parent\":  parent_children_vectorstore,\n",
        "    \"semantic\": semantic_vectorstore,\n",
        "}\n",
        "\n",
        "for name, vs in stores.items():\n",
        "    client = vs.client\n",
        "    col    = vs.collection_name\n",
        "    print(f\"=== {name} ===\")\n",
        "    # 1) Existence check\n",
        "    print(\"Exists?      \", client.collection_exists(col))\n",
        "    # 2) Point count\n",
        "    print(\"Point count: \", client.count(collection_name=col))\n",
        "    # 3) Full collection info\n",
        "    desc   = client.get_collection(collection_name=col)\n",
        "    params = desc.config.params\n",
        "\n",
        "    # ‚Äî Vector dims & metric\n",
        "    vec_field = params.vectors\n",
        "    if isinstance(vec_field, dict):\n",
        "        # multi-vector mode: pick the first VectorParams\n",
        "        vp = next(iter(vec_field.values()))\n",
        "    else:\n",
        "        # single-vector mode: vectors is itself a VectorParams\n",
        "        vp = vec_field\n",
        "    print(\"Dim / metric:\", vp.size, \"/\", vp.distance)\n",
        "\n",
        "    # ‚Äî Shard count & replication factor live on params\n",
        "    print(\"Shards / repl:\", params.shard_number, \"/\", params.replication_factor)\n",
        "\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents in store: 142\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'bytes' object has no attribute 'metadata'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m all_fields = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     all_fields.update(\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m.keys())\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetadata fields present: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(all_fields)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 4) Show per-field value types and a sample value\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: 'bytes' object has no attribute 'metadata'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Assume `parent_document_store` already has docs via ParentDocumentRetriever\n",
        "#  (i.e. you already did retriever.add_documents(...) or similar)\n",
        "\n",
        "# 1) List all stored keys (document IDs)\n",
        "all_keys = list(parent_document_store.yield_keys())\n",
        "print(f\"Total documents in store: {len(all_keys)}\")\n",
        "# print(\"Document IDs:\", all_keys)\n",
        "\n",
        "# 2) Fetch all Document objects\n",
        "docs = parent_document_store.mget(all_keys)\n",
        "\n",
        "# 3) Examine metadata schema\n",
        "#    Collect all metadata field names across docs\n",
        "all_fields = set()\n",
        "for doc in docs:\n",
        "    all_fields.update(doc.metadata.keys())\n",
        "\n",
        "print(f\"Metadata fields present: {sorted(all_fields)}\")\n",
        "\n",
        "# 4) Show per-field value types and a sample value\n",
        "field_types = {field: set() for field in all_fields}\n",
        "for doc in docs:\n",
        "    for field, val in doc.metadata.items():\n",
        "        field_types[field].add(type(val).__name__)\n",
        "\n",
        "print(\"Metadata field types:\")\n",
        "for field, types in field_types.items():\n",
        "    sample = next((d.metadata[field] for d in docs if field in d.metadata), None)\n",
        "    print(f\" ‚Ä¢ {field}: types={sorted(types)}, sample={sample!r}\")\n",
        "\n",
        "# 5) (Optional) Print out first N docs‚Äô text lengths to gauge ‚Äúdimensions‚Äù\n",
        "for i, doc in enumerate(docs[:5], 1):\n",
        "    text_len = len(doc.page_content)\n",
        "    print(f\"Doc {i} (ID={all_keys[i-1]}): {text_len} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n",
            "Skipping non-Document: <class 'bytes'>\n"
          ]
        }
      ],
      "source": [
        "all_fields = set()\n",
        "for doc in docs:\n",
        "    if hasattr(doc, \"metadata\"):\n",
        "        all_fields.update(doc.metadata.keys())\n",
        "    elif isinstance(doc, dict) and \"metadata\" in doc:\n",
        "        all_fields.update(doc[\"metadata\"].keys())\n",
        "    else:\n",
        "        print(f\"Skipping non-Document: {type(doc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaIgKvJmpjFu"
      },
      "source": [
        "## üéØ Core Learning: 7 Retrieval Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "### Strategy 1: Naive Retrieval (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retriever = baseline_vectorstore.as_retriever(search_kwargs={\"k\" : 10})\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "### Strategy 2: BM25 Retrieval (Keyword-Based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(all_review_docs)\n",
        "\n",
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "### Strategy 3: Contextual Compression (AI Reranking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "### Strategy 4: Multi-Query Retrieval (Query Expansion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever,\n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "### Strategy 5: Parent Document Retrieval (Hierarchical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "### Strategy 6: Ensemble Retrieval (Combined Methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list,\n",
        "    weights=equal_weighting\n",
        ")\n",
        "\n",
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "### Strategy 7: Semantic Retrieval (Semantic Chunking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})\n",
        "\n",
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Performance Monitoring & Evaluation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup Arize Phoenix tracing\n",
        "\n",
        "tracer = tracer_provider.get_tracer(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Traceable wrappers defined\n"
          ]
        }
      ],
      "source": [
        "@tracer.chain\n",
        "def trace_naive_retrieval(question: str):\n",
        "    try:\n",
        "        result = naive_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# @traceable(name=\"bm25_retrieval\", run_type=\"chain\", metadata={\"method\":\"bm25\"})\n",
        "@tracer.chain\n",
        "def trace_bm25_retrieval(question: str):\n",
        "    try:\n",
        "        # Use the correct chain variable name here\n",
        "        res = bm25_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": res[\"response\"].content,\n",
        "            \"context_docs\": len(res[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# @traceable(name=\"contextual_compression\", run_type=\"chain\", metadata={\"method\":\"compression\"})\n",
        "@tracer.chain\n",
        "def trace_contextual_compression(question: str):\n",
        "    try:\n",
        "        result = contextual_compression_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# @traceable(name=\"multi_query_retrieval\", run_type=\"chain\", metadata={\"method\":\"multi_query\"})\n",
        "@tracer.chain\n",
        "def trace_multi_query_retrieval(question: str):\n",
        "    try:\n",
        "        result = multi_query_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# @traceable(name=\"parent_document_retrieval\", run_type=\"chain\", metadata={\"method\":\"parent_document\"})\n",
        "@tracer.chain\n",
        "def trace_parent_document_retrieval(question: str):\n",
        "    try:\n",
        "        result = parent_document_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# @traceable(name=\"ensemble_retrieval\", run_type=\"chain\", metadata={\"method\":\"ensemble\"})\n",
        "@tracer.chain\n",
        "def trace_ensemble_retrieval(question: str):\n",
        "    try:\n",
        "        result = ensemble_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# @traceable(name=\"semantic_retrieval\", run_type=\"chain\", metadata={\"method\":\"semantic\"})\n",
        "@tracer.chain\n",
        "def trace_semantic_retrieval(question: str):\n",
        "    try:\n",
        "        result = semantic_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "print(\"‚úÖ Traceable wrappers defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Execution & Real-Time Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1748837854.656639   56018 chttp2_transport.cc:1201] ipv4:127.0.0.1:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {created_time:\"2025-06-01T21:17:34.656617645-07:00\", http2_error:11, grpc_status:14}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All methods executed with tracing\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "question = \"How does Azure Site Recovery facilitate migration to Azure for SQL Server on Azure Virtual Machines?\"\n",
        "\n",
        "naive_retrieval_chain_response = trace_naive_retrieval(question)[\"response\"]\n",
        "bm25_retrieval_chain_response = trace_bm25_retrieval(question)[\"response\"]\n",
        "contextual_compression_retrieval_chain_response = trace_contextual_compression(question)[\"response\"]\n",
        "multi_query_retrieval_chain_response = trace_multi_query_retrieval(question)[\"response\"]\n",
        "semantic_retrieval_chain_response = trace_semantic_retrieval(question)[\"response\"]\n",
        "\n",
        "print(\"‚úÖ All methods executed with tracing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parent Document Retrieval traces\n",
        "\n",
        "- broke these two out due to some serialization issues after adopting Redis\n",
        "- helped with troubleshooting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1748838131.866867   57872 chttp2_transport.cc:1201] ipv4:127.0.0.1:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-06-01T21:22:11.866855483-07:00\"}\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'response'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m parent_document_retrieval_chain_response = trace_parent_document_retrieval(question)[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ensemble_retrieval_chain_response = \u001b[43mtrace_ensemble_retrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[31mKeyError\u001b[39m: 'response'"
          ]
        }
      ],
      "source": [
        "parent_document_retrieval_chain_response = trace_parent_document_retrieval(question)[\"response\"]\n",
        "ensemble_retrieval_chain_response = trace_ensemble_retrieval(question)[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Results Analysis & Performance Visualization\n",
        "\n",
        "**Arize Phoenix application URL:**  [http://localhost:6006/](http://localhost:6006/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
