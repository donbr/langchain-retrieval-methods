{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Environment Configuration & API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Build a dynamic project name (e.g. include timestamp)\n",
        "project_name = f\"retrieval-method-comparison-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = os.getenv('COHERE_API_KEY')\n",
        "\n",
        "SUPABASE_URL=os.getenv(\"SUPABASE_URL\")\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = project_name\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "## üìä Dataset Loading & Preprocessing for RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### TEMPORARY:  Qdrant and Redis Database reset\n",
        "\n",
        "- For now I'm doing a data reset before each run (ü§∑‚Äç‚ôÇÔ∏è not best practice ü§∑‚Äç‚ôÄÔ∏è)\n",
        "\n",
        "```bash\n",
        "docker compose stop qdrant redis\n",
        "docker compose rm -f qdrant redis\n",
        "docker volume rm langchain-retrieval-methods_qdrant_data langchain-retrieval-methods_redis_data\n",
        "docker compose up -d qdrant redis\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "d794aa3a-234c-4fe7-b219-02e8b36c27ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "john_wick_1.csv already exists.\n",
            "john_wick_2.csv already exists.\n",
            "john_wick_3.csv already exists.\n",
            "john_wick_4.csv already exists.\n"
          ]
        }
      ],
      "source": [
        "# Set up a consistent data directory in the user's home directory\n",
        "from pathlib import Path\n",
        "DATA_DIR = Path.cwd() / \"data\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# URLs and filenames\n",
        "urls = [\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\", \"john_wick_1.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\", \"john_wick_2.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\", \"john_wick_3.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\", \"john_wick_4.csv\"),\n",
        "]\n",
        "\n",
        "# Download files if not already present\n",
        "for url, fname in urls:\n",
        "    file_path = DATA_DIR / fname\n",
        "    if not file_path.exists():\n",
        "        print(f\"Downloading {fname}...\")\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        file_path.write_bytes(r.content)\n",
        "    else:\n",
        "        print(f\"{fname} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "all_review_docs = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "    loader = CSVLoader(\n",
        "        file_path=(DATA_DIR / f\"john_wick_{i}.csv\"),\n",
        "        metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "    )\n",
        "\n",
        "    movie_docs = loader.load()\n",
        "    for doc in movie_docs:\n",
        "\n",
        "        # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "        doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "        # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "        doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "        # newer movies have a more recent \"last_accessed_at\" (store as ISO string)\n",
        "        doc.metadata[\"last_accessed_at\"] = (datetime.now() - timedelta(days=4-i)).isoformat()\n",
        "\n",
        "    all_review_docs.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## üèóÔ∏è Building RAG Infrastructure: Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AX5XBOIltcvD"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore  # Updated import\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain_community.storage import RedisStore\n",
        "from langchain.storage import create_kv_docstore\n",
        "\n",
        "from langchain_community.vectorstores import SupabaseVectorStore\n",
        "from supabase.client import Client, create_client\n",
        "\n",
        "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
        "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
        "supabase: Client = create_client(supabase_url, supabase_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Level 1: Simple Vector Storage (Baseline)\n",
        "\n",
        "- creates the vector store using the all_review_docs Document object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_vectorstore = SupabaseVectorStore.from_documents(\n",
        "    all_review_docs,\n",
        "    embeddings,\n",
        "    client=supabase,\n",
        "    table_name=\"johnwick_baseline_documents\",\n",
        "    query_name=\"match_johnwick_baseline_documents\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Level 3: Vector Storage organized by Semantic Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RmMfpBIcF3dN"
      },
      "outputs": [],
      "source": [
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")\n",
        "\n",
        "semantic_documents = semantic_chunker.split_documents(all_review_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "semantic_vectorstore = SupabaseVectorStore.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    client=supabase,\n",
        "    table_name=\"johnwick_semantic_documents\",\n",
        "    query_name=\"match_johnwick_semantic_documents\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaIgKvJmpjFu"
      },
      "source": [
        "## üéØ Core Learning: 7 Retrieval Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strategy Setup: Tracing & Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup langsmith tracing\n",
        "\n",
        "from langsmith import Client, traceable\n",
        "\n",
        "langsmith_client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "### Strategy 1: Naive Retrieval (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retriever = baseline_vectorstore.as_retriever(search_kwargs={\"k\" : 10})\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "### Strategy 2: BM25 Retrieval (Keyword-Based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(all_review_docs)\n",
        "\n",
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "### Strategy 3: Contextual Compression (AI Reranking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "### Strategy 4: Multi-Query Retrieval (Query Expansion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever,\n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "### Strategy 6: Ensemble Retrieval (Combined Methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, compression_retriever, multi_query_retriever]\n",
        "\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list,\n",
        "    weights=equal_weighting\n",
        ")\n",
        "\n",
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "### Strategy 7: Semantic Retrieval (Semantic Chunking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})\n",
        "\n",
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Performance Monitoring & Evaluation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Traceable wrappers defined\n"
          ]
        }
      ],
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "@traceable(name=\"naive_retrieval\", run_type=\"chain\", metadata={\"method\":\"naive\"})\n",
        "def trace_naive_retrieval(question: str):\n",
        "    try:\n",
        "        result = naive_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@traceable(name=\"bm25_retrieval\", run_type=\"chain\", metadata={\"method\":\"bm25\"})\n",
        "def trace_bm25_retrieval(question: str):\n",
        "    try:\n",
        "        # Use the correct chain variable name here\n",
        "        res = bm25_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": res[\"response\"].content,\n",
        "            \"context_docs\": len(res[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@traceable(name=\"contextual_compression\", run_type=\"chain\", metadata={\"method\":\"compression\"})\n",
        "def trace_contextual_compression(question: str):\n",
        "    try:\n",
        "        result = contextual_compression_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@traceable(name=\"multi_query_retrieval\", run_type=\"chain\", metadata={\"method\":\"multi_query\"})\n",
        "def trace_multi_query_retrieval(question: str):\n",
        "    try:\n",
        "        result = multi_query_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@traceable(name=\"ensemble_retrieval\", run_type=\"chain\", metadata={\"method\":\"ensemble\"})\n",
        "def trace_ensemble_retrieval(question: str):\n",
        "    try:\n",
        "        result = ensemble_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "@traceable(name=\"semantic_retrieval\", run_type=\"chain\", metadata={\"method\":\"semantic\"})\n",
        "def trace_semantic_retrieval(question: str):\n",
        "    try:\n",
        "        result = semantic_retrieval_chain.invoke({\"question\": question})\n",
        "        return {\n",
        "            \"response\": result[\"response\"].content,\n",
        "            \"context_docs\": len(result[\"context\"])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "print(\"‚úÖ Traceable wrappers defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Execution & Real-Time Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All methods executed with tracing\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "question = \"Did people generally like John Wick?\"\n",
        "\n",
        "naive_retrieval_chain_response = trace_naive_retrieval(question)[\"response\"]\n",
        "bm25_retrieval_chain_response = trace_bm25_retrieval(question)[\"response\"]\n",
        "contextual_compression_retrieval_chain_response = trace_contextual_compression(question)[\"response\"]\n",
        "multi_query_retrieval_chain_response = trace_multi_query_retrieval(question)[\"response\"]\n",
        "semantic_retrieval_chain_response = trace_semantic_retrieval(question)[\"response\"]\n",
        "ensemble_retrieval_chain_response = trace_ensemble_retrieval(question)[\"response\"]\n",
        "\n",
        "print(\"‚úÖ All methods executed with tracing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tracers.langchain import wait_for_all_tracers\n",
        "\n",
        "# run after all your traceable calls\n",
        "wait_for_all_tracers()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
