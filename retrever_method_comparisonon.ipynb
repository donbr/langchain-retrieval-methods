{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkgFAXWVW3wm"
      },
      "outputs": [],
      "source": [
        "#!uv pip install -qU langchain langchain-community langchain-experimental langchain-openai langchain-qdrant langchain-cohere rank_bm25 qdrant-client langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"COHERE_API_KEY\"] = os.getenv('COHERE_API_KEY')\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"retrieval-method-comparison\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')\n",
        "\n",
        "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "QDRANT_API_URL = os.getenv(\"QDRANT_API_URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "d794aa3a-234c-4fe7-b219-02e8b36c27ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "john_wick_1.csv already exists.\n",
            "john_wick_2.csv already exists.\n",
            "john_wick_3.csv already exists.\n",
            "john_wick_4.csv already exists.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"### Data Preparation\"\"\"\n",
        "\n",
        "# Set up a consistent data directory in the user's home directory\n",
        "DATA_DIR = Path.home() / \"data\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# URLs and filenames\n",
        "urls = [\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\", \"john_wick_1.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\", \"john_wick_2.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\", \"john_wick_3.csv\"),\n",
        "    (\"https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\", \"john_wick_4.csv\"),\n",
        "]\n",
        "\n",
        "# Download files if not already present\n",
        "for url, fname in urls:\n",
        "    file_path = DATA_DIR / fname\n",
        "    if not file_path.exists():\n",
        "        print(f\"Downloading {fname}...\")\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        file_path.write_bytes(r.content)\n",
        "    else:\n",
        "        print(f\"{fname} already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)\n",
        "\n",
        "parent_docs = documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDFoVHtS8hrr",
        "outputId": "f26697eb-e3eb-4d74-d10f-715c0771185d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of parent_docs: 100\n",
            "Length of documents: 100\n"
          ]
        }
      ],
      "source": [
        "# display length of parent_docs and docs\n",
        "print(f\"Length of parent_docs: {len(parent_docs)}\")\n",
        "print(f\"Length of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Setup Vector Stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AX5XBOIltcvD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_qdrant import QdrantVectorStore  # Updated import\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from qdrant_client import QdrantClient, models\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d6Hta5PoJlos"
      },
      "outputs": [],
      "source": [
        "# 1. Main vectorstore using qdrant cloud\n",
        "baseline_vectorstore = QdrantVectorStore.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    url=QDRANT_API_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    prefer_grpc=True,\n",
        "    collection_name=\"johnwick_baseline\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CjIJlsjsrOSi"
      },
      "outputs": [],
      "source": [
        "# 2. Parent document setup with qdrant cloud client\n",
        "\n",
        "# Initialize cloud client\n",
        "cloud_client = QdrantClient(\n",
        "    url=QDRANT_API_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    prefer_grpc=True\n",
        ")\n",
        "\n",
        "# Check if the collection exists\n",
        "if not cloud_client.collection_exists(\"johnwick_parent\"):\n",
        "    cloud_client.create_collection(\n",
        "        collection_name=\"johnwick_parent\",\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=1536,\n",
        "            distance=models.Distance.COSINE\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# Construct the VectorStore using cloud client\n",
        "parent_vectorstore = QdrantVectorStore(\n",
        "    embedding=embeddings,\n",
        "    client=cloud_client,\n",
        "    collection_name=\"johnwick_parent\",\n",
        ")\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")\n",
        "\n",
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RmMfpBIcF3dN"
      },
      "outputs": [],
      "source": [
        "# 3. Semantic chunking using qdrant cloud\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")\n",
        "\n",
        "semantic_documents = semantic_chunker.split_documents(documents)\n",
        "\n",
        "semantic_vectorstore = QdrantVectorStore.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    url=QDRANT_API_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    prefer_grpc=True,\n",
        "    collection_name=\"johnwick_semantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaIgKvJmpjFu"
      },
      "source": [
        "## Retrievers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "### Naive Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retriever = baseline_vectorstore.as_retriever(search_kwargs={\"k\" : 10})\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LI-5ueEddku9"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain_response = naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "### BM25 Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "\n",
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oY9qzmm3SOrF"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain_response = bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "### Contextual Compression Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V3iGpokswcBb"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain_response = contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "### Multi-Query Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever,\n",
        "    llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CGgNuOb3Q3M9"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain_response = multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "### Parent Document Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TXB5i89Zly5W"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain_response = parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "### Ensemble Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list,\n",
        "    weights=equal_weighting\n",
        ")\n",
        "\n",
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0lMvqL88UQI-"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain_response = ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "### Semantic Retriever - using semantically chunked vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})\n",
        "\n",
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0lN2j-e4Z0SD"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain_response = semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYuh2V1vHY6N"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwYbdWnVHct-",
        "outputId": "11cc0bc2-434d-4512-99ed-870b64c09357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "['johnwick_baseline', 'johnwick_semantic', 'johnwick_parent', 'airbnb_pdf_rec_1000_200_images', 'mcp-anthropic-desktop', 'mcp-nsclc', 'ambrose_lake_covenant']\n"
          ]
        }
      ],
      "source": [
        "# display existing collections\n",
        "\n",
        "existing = [c.name for c in cloud_client.get_collections().collections]\n",
        "\n",
        "print(type(existing))\n",
        "print(existing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfaFQTs_MzlG",
        "outputId": "7fb85947-3663-4a83-a89f-b06fcf89d772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== baseline ===\n",
            "Exists?       True\n",
            "Point count:  count=600\n",
            "Dim / metric: 1536 / Cosine\n",
            "Shards / repl: 1 / 1\n",
            "\n",
            "=== parent ===\n",
            "Exists?       True\n",
            "Point count:  count=28902\n",
            "Dim / metric: 1536 / Cosine\n",
            "Shards / repl: 1 / 1\n",
            "\n",
            "=== semantic ===\n",
            "Exists?       True\n",
            "Point count:  count=1074\n",
            "Dim / metric: 1536 / Cosine\n",
            "Shards / repl: 1 / 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# display vector store collection metadata\n",
        "\n",
        "stores = {\n",
        "    \"baseline\": baseline_vectorstore,\n",
        "    \"parent\":  parent_vectorstore,\n",
        "    \"semantic\": semantic_vectorstore,\n",
        "}\n",
        "\n",
        "for name, vs in stores.items():\n",
        "    client = vs.client\n",
        "    col    = vs.collection_name\n",
        "    print(f\"=== {name} ===\")\n",
        "    # 1) Existence check\n",
        "    print(\"Exists?      \", client.collection_exists(col))\n",
        "    # 2) Point count\n",
        "    print(\"Point count: \", client.count(collection_name=col))\n",
        "    # 3) Full collection info\n",
        "    desc   = client.get_collection(collection_name=col)\n",
        "    params = desc.config.params\n",
        "\n",
        "    # — Vector dims & metric\n",
        "    vec_field = params.vectors\n",
        "    if isinstance(vec_field, dict):\n",
        "        # multi-vector mode: pick the first VectorParams\n",
        "        vp = next(iter(vec_field.values()))\n",
        "    else:\n",
        "        # single-vector mode: vectors is itself a VectorParams\n",
        "        vp = vec_field\n",
        "    print(\"Dim / metric:\", vp.size, \"/\", vp.distance)\n",
        "\n",
        "    # — Shard count & replication factor live on params\n",
        "    print(\"Shards / repl:\", params.shard_number, \"/\", params.replication_factor)\n",
        "\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDXX48ENcaEY",
        "outputId": "3d78a55f-2fc1-49c8-aee1-ffc3ffbf91a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents in store: 100\n",
            "Metadata fields present: ['Author', 'Movie_Title', 'Rating', 'Review_Date', 'Review_Title', 'Review_Url', 'last_accessed_at', 'row', 'source']\n",
            "Metadata field types:\n",
            " • Rating: types=['int'], sample=8\n",
            " • Movie_Title: types=['str'], sample='John Wick 1'\n",
            " • Review_Title: types=['str'], sample=' Kinetic, concise, and stylish; John Wick kicks ass.\\n'\n",
            " • Review_Date: types=['str'], sample='6 May 2015'\n",
            " • Review_Url: types=['str'], sample='/review/rw3233896/?ref_=tt_urv'\n",
            " • source: types=['str'], sample='john_wick_1.csv'\n",
            " • Author: types=['str'], sample='lnvicta'\n",
            " • row: types=['int'], sample=0\n",
            " • last_accessed_at: types=['datetime'], sample=datetime.datetime(2025, 5, 20, 19, 23, 16, 383052)\n",
            "Doc 1 (ID=0b0d1424-4259-4a60-992e-a413df1b09a7): 599 characters\n",
            "Doc 2 (ID=6e32b3e3-e149-4303-80f7-857b1d67f264): 369 characters\n",
            "Doc 3 (ID=f38a992d-0811-418b-a990-88f28909419b): 256 characters\n",
            "Doc 4 (ID=7f9a1c9d-2816-4203-bf68-b32a17766b03): 426 characters\n",
            "Doc 5 (ID=1de51d1f-5e66-4907-9a40-b5c327b0c6bd): 902 characters\n"
          ]
        }
      ],
      "source": [
        "# Assume `store` already has docs via ParentDocumentRetriever\n",
        "#  (i.e. you already did retriever.add_documents(...) or similar)\n",
        "\n",
        "# 1) List all stored keys (document IDs)\n",
        "all_keys = list(store.yield_keys())\n",
        "print(f\"Total documents in store: {len(all_keys)}\")\n",
        "# print(\"Document IDs:\", all_keys)\n",
        "\n",
        "# 2) Fetch all Document objects\n",
        "docs = store.mget(all_keys)\n",
        "\n",
        "# 3) Examine metadata schema\n",
        "#    Collect all metadata field names across docs\n",
        "all_fields = set()\n",
        "for doc in docs:\n",
        "    all_fields.update(doc.metadata.keys())\n",
        "\n",
        "print(f\"Metadata fields present: {sorted(all_fields)}\")\n",
        "\n",
        "# 4) Show per-field value types and a sample value\n",
        "field_types = {field: set() for field in all_fields}\n",
        "for doc in docs:\n",
        "    for field, val in doc.metadata.items():\n",
        "        field_types[field].add(type(val).__name__)\n",
        "\n",
        "print(\"Metadata field types:\")\n",
        "for field, types in field_types.items():\n",
        "    sample = next((d.metadata[field] for d in docs if field in d.metadata), None)\n",
        "    print(f\" • {field}: types={sorted(types)}, sample={sample!r}\")\n",
        "\n",
        "# 5) (Optional) Print out first N docs’ text lengths to gauge “dimensions”\n",
        "for i, doc in enumerate(docs[:5], 1):\n",
        "    text_len = len(doc.page_content)\n",
        "    print(f\"Doc {i} (ID={all_keys[i-1]}): {text_len} characters\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCKwiiOuHi9p"
      },
      "source": [
        "### Delete qdrant collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Pe_COpbsHcB7"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "# initialize client (cloud or on-prem)\n",
        "cloud_client = QdrantClient(\n",
        "    url=QDRANT_API_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    prefer_grpc=True,\n",
        ")\n",
        "\n",
        "# create conditional deletion flag\n",
        "delete_collection = False\n",
        "\n",
        "if delete_collection:\n",
        "    # list of collections to drop\n",
        "    collections_to_reset = [\n",
        "        \"johnwick_baseline\",\n",
        "        \"johnwick_parent\",\n",
        "        \"johnwick_semantic\",\n",
        "    ]\n",
        "\n",
        "    for col_name in collections_to_reset:\n",
        "        # guard against missing collections\n",
        "        if cloud_client.collection_exists(col_name):\n",
        "            cloud_client.delete_collection(\n",
        "                collection_name=col_name,\n",
        "                timeout=60,  # seconds\n",
        "            )\n",
        "            print(f\"Deleted collection: {col_name}\")\n",
        "        else:\n",
        "            print(f\"Collection not found (skipped): {col_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KWMPESVcFT_"
      },
      "source": [
        "## Response Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bHCFk-PajS6t",
        "outputId": "e77ca077-2d16-41d5-d5dc-fb5036f20e65"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Naive Retrieval Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Yes, people generally liked John Wick. The reviews highlight Keanu Reeves' slick performance, brilliant and meticulously choreographed action sequences, and the film's intense, fun, and violent style. Reviewers praised it as one of the best action films of the year and even the past decade, recommending it highly to action fans and those looking for something fresh. The movie was described as well-paced, stylish, and entertaining, with a strong supporting cast adding interest. Overall, the reception was very positive.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## BM25 Retrieval Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "People's opinions on John Wick vary depending on the installment:\n",
            "\n",
            "- The first John Wick movie is generally liked and praised. Reviews describe it as a special and stylish action film with smooth sequences, clear motives, and a relatable hero. It is recommended highly for action fans and seen as a must-watch. (Ratings like 8 and 10 support this.)\n",
            "\n",
            "- However, later installments receive mixed to negative reviews. For example, John Wick 3 has been criticized as boring, dull, overly violent, and lacking plot.\n",
            "\n",
            "- John Wick 4 is considered the weakest by at least one reviewer, described as nearly three hours of gunfights with meaningless dialogue and no real addition to the story.\n",
            "\n",
            "In summary, the first John Wick film was generally well-liked, but later movies in the series received more negative and mixed reactions from some viewers.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Contextual Compression Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Yes, people generally liked John Wick. The review provided gives the film a high rating of 9 out of 10 and praises Keanu Reeves' performance, the brilliant and intense action sequences, the brisk pacing, and the overall entertainment value. The reviewer describes it as \"the best action film of the year and one of the absolute best in the past decade\" and highly recommends it especially to action fans, but also suggests that anyone who enjoys a good movie will appreciate it.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Multi-Query Retrieval Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Yes, people generally liked John Wick. The reviews highlight that the original \"John Wick\" film received strong positive feedback, with ratings such as 9 and 10 out of 10. Reviewers praised Keanu Reeves' performance, the slick and brilliantly choreographed action sequences, and the overall entertainment value. It was described as one of the best action films of the year and even of the past decade. The series as a whole has remained consistent and well received, with the original films holding IMDb ratings around 7.4/10 and reviewers personally rating them about 8/10. The fourth chapter was also noted as perhaps the best installment. Overall, the \"John Wick\" series enjoys a solid fan base and critical appreciation.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Parent Document Retrieval Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "I don't know.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Ensemble Retrieval Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Yes, people generally liked the first John Wick movie. Reviews praise Keanu Reeves' performance, the slick and well-choreographed action sequences, and the stylish, fun nature of the film. It is described as one of the best action films in recent years, highly recommended especially for action fans. However, the reception of later films in the series is more mixed, with some reviewers expressing disappointment with the third and fourth installments. But overall, the original John Wick was well-received and enjoyed by many.\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Semantic Retrieval Chain Response\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Yes, based on the reviews provided, people generally liked John Wick. One review describes the first John Wick movie as \"something special,\" praising its elaborate action sequences, world-building, and smooth action scenes. It highlights Keanu Reeves' natural performance and the engaging criminal underworld setting. Additionally, multiple mentions state that \"John Wick was cool,\" indicating a positive reception. Overall, the feedback suggests that audiences appreciated the film.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Map of titles to response objects\n",
        "responses = {\n",
        "    \"Naive Retrieval Chain Response\":              naive_retrieval_chain_response,\n",
        "    \"BM25 Retrieval Chain Response\":               bm25_retrieval_chain_response,\n",
        "    \"Contextual Compression Chain Response\":       contextual_compression_retrieval_chain_response,\n",
        "    \"Multi-Query Retrieval Chain Response\":        multi_query_retrieval_chain_response,\n",
        "    \"Parent Document Retrieval Chain Response\":    parent_document_retrieval_chain_response,\n",
        "    \"Ensemble Retrieval Chain Response\":           ensemble_retrieval_chain_response,\n",
        "    \"Semantic Retrieval Chain Response\":           semantic_retrieval_chain_response,\n",
        "}\n",
        "\n",
        "for header, resp in responses.items():\n",
        "    display(Markdown(f\"## {header}\\n\"))\n",
        "    print(\"\\n\")\n",
        "    print(resp)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GGRzkNHjPUd"
      },
      "source": [
        "## Retrieval Chain visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hR9Ppe2ZaDml"
      },
      "outputs": [],
      "source": [
        "!uv pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vZOd4DL9NS3k",
        "outputId": "860d77fb-83d0-4ec7-a8d3-27cb66d8ae60"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Naive Retrieval\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## BM25 Retrieval\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+    \n",
            "          | Parallel<context,question>Input |    \n",
            "          +---------------------------------+    \n",
            "                    **           **              \n",
            "                  **               **            \n",
            "                **                   **          \n",
            "         +--------+                    **        \n",
            "         | Lambda |                     *        \n",
            "         +--------+                     *        \n",
            "              *                         *        \n",
            "              *                         *        \n",
            "              *                         *        \n",
            "      +---------------+            +--------+    \n",
            "      | BM25Retriever |            | Lambda |    \n",
            "      +---------------+            +--------+    \n",
            "                    **           **              \n",
            "                      **       **                \n",
            "                        **   **                  \n",
            "         +----------------------------------+    \n",
            "         | Parallel<context,question>Output |    \n",
            "         +----------------------------------+    \n",
            "                           *                     \n",
            "                           *                     \n",
            "                           *                     \n",
            "              +------------------------+         \n",
            "              | Parallel<context>Input |         \n",
            "              +------------------------+         \n",
            "                     **         **               \n",
            "                   **             **             \n",
            "                  *                 *            \n",
            "           +--------+          +-------------+   \n",
            "           | Lambda |          | Passthrough |   \n",
            "           +--------+          +-------------+   \n",
            "                     **         **               \n",
            "                       **     **                 \n",
            "                         *   *                   \n",
            "              +-------------------------+        \n",
            "              | Parallel<context>Output |        \n",
            "              +-------------------------+        \n",
            "                           *                     \n",
            "                           *                     \n",
            "                           *                     \n",
            "          +---------------------------------+    \n",
            "          | Parallel<response,context>Input |    \n",
            "          +---------------------------------+    \n",
            "                  ***             ***            \n",
            "                **                   ***         \n",
            "              **                        **       \n",
            "+--------------------+                    **     \n",
            "| ChatPromptTemplate |                     *     \n",
            "+--------------------+                     *     \n",
            "           *                               *     \n",
            "           *                               *     \n",
            "           *                               *     \n",
            "    +------------+                    +--------+ \n",
            "    | ChatOpenAI |                    | Lambda | \n",
            "    +------------+                   *+--------+ \n",
            "                  ***             ***            \n",
            "                     **         **               \n",
            "                       **     **                 \n",
            "         +----------------------------------+    \n",
            "         | Parallel<response,context>Output |    \n",
            "         +----------------------------------+    \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Contextual Compression\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                +---------------------------------+     \n",
            "                | Parallel<context,question>Input |     \n",
            "                +---------------------------------+     \n",
            "                        ***             ***             \n",
            "                      **                   ***          \n",
            "                    **                        **        \n",
            "            +--------+                          **      \n",
            "            | Lambda |                           *      \n",
            "            +--------+                           *      \n",
            "                 *                               *      \n",
            "                 *                               *      \n",
            "                 *                               *      \n",
            "+--------------------------------+          +--------+  \n",
            "| ContextualCompressionRetriever |          | Lambda |  \n",
            "+--------------------------------+         *+--------+  \n",
            "                        ***             ***             \n",
            "                           **         **                \n",
            "                             **     **                  \n",
            "               +----------------------------------+     \n",
            "               | Parallel<context,question>Output |     \n",
            "               +----------------------------------+     \n",
            "                                 *                      \n",
            "                                 *                      \n",
            "                                 *                      \n",
            "                    +------------------------+          \n",
            "                    | Parallel<context>Input |          \n",
            "                    +------------------------+          \n",
            "                           **         **                \n",
            "                         **             **              \n",
            "                        *                 *             \n",
            "                 +--------+          +-------------+    \n",
            "                 | Lambda |          | Passthrough |    \n",
            "                 +--------+          +-------------+    \n",
            "                           **         **                \n",
            "                             **     **                  \n",
            "                               *   *                    \n",
            "                    +-------------------------+         \n",
            "                    | Parallel<context>Output |         \n",
            "                    +-------------------------+         \n",
            "                                 *                      \n",
            "                                 *                      \n",
            "                                 *                      \n",
            "                +---------------------------------+     \n",
            "                | Parallel<response,context>Input |     \n",
            "                +---------------------------------+     \n",
            "                        **               ***            \n",
            "                     ***                    **          \n",
            "                   **                         ***       \n",
            "     +--------------------+                      **     \n",
            "     | ChatPromptTemplate |                       *     \n",
            "     +--------------------+                       *     \n",
            "                *                                 *     \n",
            "                *                                 *     \n",
            "                *                                 *     \n",
            "         +------------+                      +--------+ \n",
            "         | ChatOpenAI |                      | Lambda | \n",
            "         +------------+*                   **+--------+ \n",
            "                        **               **             \n",
            "                          ***         ***               \n",
            "                             **     **                  \n",
            "               +----------------------------------+     \n",
            "               | Parallel<response,context>Output |     \n",
            "               +----------------------------------+     \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Multi-Query Retrieval\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **           **                \n",
            "                  **               **              \n",
            "                **                   **            \n",
            "         +--------+                    **          \n",
            "         | Lambda |                     *          \n",
            "         +--------+                     *          \n",
            "              *                         *          \n",
            "              *                         *          \n",
            "              *                         *          \n",
            "  +---------------------+           +--------+     \n",
            "  | MultiQueryRetriever |           | Lambda |     \n",
            "  +---------------------+          *+--------+     \n",
            "                    **           **                \n",
            "                      **       **                  \n",
            "                        **   **                    \n",
            "         +----------------------------------+      \n",
            "         | Parallel<context,question>Output |      \n",
            "         +----------------------------------+      \n",
            "                           *                       \n",
            "                           *                       \n",
            "                           *                       \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     **         ***                \n",
            "                   **              *               \n",
            "                  *                 **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     **         ***                \n",
            "                       **      *                   \n",
            "                         *   **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                           *                       \n",
            "                           *                       \n",
            "                           *                       \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                    +--------+   \n",
            "    | ChatOpenAI |                    | Lambda |   \n",
            "    +------------+*                  *+--------+   \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "         +----------------------------------+      \n",
            "         | Parallel<response,context>Output |      \n",
            "         +----------------------------------+      \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Parent Document Retrieval\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    ***           ***              \n",
            "                  **                 **            \n",
            "                **                     **          \n",
            "         +--------+                      **        \n",
            "         | Lambda |                       *        \n",
            "         +--------+                       *        \n",
            "              *                           *        \n",
            "              *                           *        \n",
            "              *                           *        \n",
            "+-------------------------+          +--------+    \n",
            "| ParentDocumentRetriever |          | Lambda |    \n",
            "+-------------------------+          +--------+    \n",
            "                    ***           ***              \n",
            "                       **       **                 \n",
            "                         **   **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "               +------------------------+          \n",
            "               | Parallel<context>Input |          \n",
            "               +------------------------+          \n",
            "                      **         **                \n",
            "                    **             **              \n",
            "                   *                 *             \n",
            "            +--------+          +-------------+    \n",
            "            | Lambda |          | Passthrough |    \n",
            "            +--------+          +-------------+    \n",
            "                      **         **                \n",
            "                        **     **                  \n",
            "                          *   *                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **               ***            \n",
            "                ***                    **          \n",
            "              **                         ***       \n",
            "+--------------------+                      **     \n",
            "| ChatPromptTemplate |                       *     \n",
            "+--------------------+                       *     \n",
            "           *                                 *     \n",
            "           *                                 *     \n",
            "           *                                 *     \n",
            "    +------------+                      +--------+ \n",
            "    | ChatOpenAI |                      | Lambda | \n",
            "    +------------+*                   **+--------+ \n",
            "                   **               **             \n",
            "                     ***         ***               \n",
            "                        **     **                  \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Ensemble Retrieval\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+    \n",
            "          | Parallel<context,question>Input |    \n",
            "          +---------------------------------+    \n",
            "                    **           **              \n",
            "                  **               **            \n",
            "                **                   **          \n",
            "         +--------+                    **        \n",
            "         | Lambda |                     *        \n",
            "         +--------+                     *        \n",
            "              *                         *        \n",
            "              *                         *        \n",
            "              *                         *        \n",
            "    +-------------------+          +--------+    \n",
            "    | EnsembleRetriever |          | Lambda |    \n",
            "    +-------------------+          +--------+    \n",
            "                    **           **              \n",
            "                      **       **                \n",
            "                        **   **                  \n",
            "         +----------------------------------+    \n",
            "         | Parallel<context,question>Output |    \n",
            "         +----------------------------------+    \n",
            "                           *                     \n",
            "                           *                     \n",
            "                           *                     \n",
            "              +------------------------+         \n",
            "              | Parallel<context>Input |         \n",
            "              +------------------------+         \n",
            "                     **         **               \n",
            "                   **             **             \n",
            "                  *                 *            \n",
            "           +--------+          +-------------+   \n",
            "           | Lambda |          | Passthrough |   \n",
            "           +--------+          +-------------+   \n",
            "                     **         **               \n",
            "                       **     **                 \n",
            "                         *   *                   \n",
            "              +-------------------------+        \n",
            "              | Parallel<context>Output |        \n",
            "              +-------------------------+        \n",
            "                           *                     \n",
            "                           *                     \n",
            "                           *                     \n",
            "          +---------------------------------+    \n",
            "          | Parallel<response,context>Input |    \n",
            "          +---------------------------------+    \n",
            "                  ***             ***            \n",
            "                **                   ***         \n",
            "              **                        **       \n",
            "+--------------------+                    **     \n",
            "| ChatPromptTemplate |                     *     \n",
            "+--------------------+                     *     \n",
            "           *                               *     \n",
            "           *                               *     \n",
            "           *                               *     \n",
            "    +------------+                    +--------+ \n",
            "    | ChatOpenAI |                    | Lambda | \n",
            "    +------------+                   *+--------+ \n",
            "                  ***             ***            \n",
            "                     **         **               \n",
            "                       **     **                 \n",
            "         +----------------------------------+    \n",
            "         | Parallel<response,context>Output |    \n",
            "         +----------------------------------+    \n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Semantic Retrieval\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Map of titles to chains\n",
        "chains = {\n",
        "    \"Naive Retrieval\":              naive_retrieval_chain,\n",
        "    \"BM25 Retrieval\":               bm25_retrieval_chain,\n",
        "    \"Contextual Compression\":       contextual_compression_retrieval_chain,\n",
        "    \"Multi-Query Retrieval\":        multi_query_retrieval_chain,\n",
        "    \"Parent Document Retrieval\":    parent_document_retrieval_chain,\n",
        "    \"Ensemble Retrieval\":           ensemble_retrieval_chain,\n",
        "    \"Semantic Retrieval\":           semantic_retrieval_chain,\n",
        "}\n",
        "\n",
        "for title, chain in chains.items():\n",
        "    display(Markdown(f\"## {title}\\n\"))\n",
        "    print(chain.get_graph().draw_ascii())\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
